{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data.txt','\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_data = data.copy()\n",
    "# normalize X\n",
    "high = data['X'].max()\n",
    "low = data['X'].min()\n",
    "normalized_data['X'] = ((data['X'] - low) / (high - low)) * 10 # within [-0.1,0.1]\n",
    "\n",
    "# normalize Y\n",
    "high = data['Y'].max()\n",
    "low = data['Y'].min()\n",
    "normalized_data['Y'] = ((data['Y'] - low) / (high - low)) * 10 # within [-0.1,0.1]\n",
    "\n",
    "# timestamp / 40\n",
    "normalized_data['timestamp'] = (data['timestamp'] / 40).map(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_data.to_csv('data_normalized.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(16,), (16, 8, 2), (16, 4, 2)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# functions for generate a batch of sample\n",
    "'''\n",
    "    Input:\n",
    "        data: the CSV\n",
    "        num_data: size of batch\n",
    "    Output:\n",
    "        IDs: list of selected IDs\n",
    "        input_sequence: batch with shape (num_data,input_length, 2)\n",
    "        output_sequence: batch with shape (num_data, output_length, 2)\n",
    "'''\n",
    "from random import shuffle\n",
    "def get_batch(data,num_data = 16,input_length = 8, output_length = 4):\n",
    "    # evaluate the total length of series required\n",
    "    total_length = input_length + output_length\n",
    "    # filter out the series that has at least the number of `total_length` long\n",
    "    id_counts = data.groupby('ID').ID.count()\n",
    "    # get a table of candidate id, whose sequence is longer than (or eq. to) total_length\n",
    "    candidate_id_counts = id_counts[id_counts >= total_length]\n",
    "    # the number of candidates...\n",
    "    total_candidate_id_count = candidate_id_counts.count()\n",
    "    # get the random sequence...\n",
    "    random_ids_selected = [i for i in range(total_candidate_id_count)]\n",
    "    shuffle(random_ids_selected)\n",
    "    \n",
    "    selected_ids = []\n",
    "    input_batch = []\n",
    "    target_batch = []\n",
    "    # retrieve the coordinates of the sequence (from the beginning to `total_length`)\n",
    "    for i in random_ids_selected[:num_data]:\n",
    "        selected_ids.append(i)\n",
    "        # select X,Y from ID where ID == i order by timestamp...\n",
    "        sequence_of_i = data[data.ID == i].sort_values(by = \"timestamp\")[[\"X\",\"Y\"]]\n",
    "        # divide the sequence into two parts...\n",
    "        input_sequence = sequence_of_i.iloc[:input_length]\n",
    "        target_sequence = sequence_of_i.iloc[input_length:total_length]\n",
    "        # and append the new sequence to existing arrays\n",
    "        input_batch.append(np.array(input_sequence))\n",
    "        target_batch.append(np.array(target_sequence))\n",
    "    \n",
    "    # return and array of selected ids as well as the batch...\n",
    "    return np.stack(selected_ids), np.stack(input_batch), np.stack(target_batch)\n",
    "    \n",
    "\n",
    "# verify the shape is right...\n",
    "list(map(lambda a: a.shape,get_batch(normalized_data)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''\n",
    "    Visualize the traces in a batch\n",
    "    If batch size = B, sequence length = L...\n",
    "    Input:\n",
    "        batch: batch of sequence of arbitrary length, i.e. array of shape (B,L,2)\n",
    "    Output:\n",
    "        None, a graph will be drawn instead..\n",
    "'''\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "def visualize_trace(batch,target_batch):\n",
    "    # first we make sure that the shape of the batch looks like (_, _, 2)\n",
    "    def check_shape(shape):\n",
    "        if len(shape) != 3:\n",
    "            raise ValueError(\"batch should be in 3 dimension\")\n",
    "        if shape[-1] != 2:\n",
    "            raise ValueError(\"Last axis should be storing X,Y coordinates\")\n",
    "    \n",
    "    check_shape(batch.shape)\n",
    "    check_shape(target_batch.shape)\n",
    "    # sub-routine for draw a particular batch\n",
    "    def draw_batch(batch,linestyle = None):\n",
    "        # now extract the dimension\n",
    "        batch_size, sequence_length, _ = batch.shape\n",
    "        for batch_id in range(batch_size):\n",
    "            # pick a random color for this trace\n",
    "            line_color = np.random.rand(3)\n",
    "            for sequence_pos in range(sequence_length - 1):\n",
    "                # get the two adjacent coordinates\n",
    "                cur_coord = batch[batch_id, sequence_pos]\n",
    "                next_coord = batch[batch_id, sequence_pos + 1]\n",
    "                # and draw the line...\n",
    "                # sneaky plot function requires x-coordinates to be put in the same argument, so are y-coordinates...\n",
    "                plt.plot([cur_coord[0],next_coord[0]],\n",
    "                         [cur_coord[1],next_coord[1]],\n",
    "                         linestyle = linestyle,\n",
    "                         c = line_color)\n",
    "    \n",
    "    draw_batch(batch)\n",
    "    draw_batch(target_batch, \":\")\n",
    "    # finally show the graph\n",
    "#     plt.show()\n",
    "    \n",
    "# let's test this visualization,\n",
    "_, input_batch, target_batch = get_batch(normalized_data,16,16,8)\n",
    "visualize_trace(input_batch,target_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now lets define a vanilla LSTM model\n",
    "'''\n",
    "    According to the paper, there should be an RNN that takes a sequence and gives a sequence (like seq-to-seq)\n",
    "    except this output are hidden layers, like vectors of length 128\n",
    "    To interpret such result, a dense layer with ReLU is added to condense the output to 5 numbers,\n",
    "    namely, the mean_x, mean_y, sxx, syy, and sxy \n",
    "    of the bivariate gaussian of the probability of the agent at that given timestamp.\n",
    "    \n",
    "    The negative log likelihood between the real coordinate and this estimated distribution will be the loss.\n",
    "'''\n",
    "# first, the loss function, in Keras backend\n",
    "from keras.layers import *\n",
    "import keras.backend as K\n",
    "'''\n",
    "    The function takes a series of params of bivariate normal distribution, and a batch of observed coordinates,\n",
    "    and return the log likelikhood of them...\n",
    "    \n",
    "    probability (likelihood) of the observed point (x,y) given the 5 parameters (mx,my,sx,sy,sp):\n",
    "        det(2 * pi * [[sx,sp],[sp,sy]]) ^(-0.5) \n",
    "            * exp(-0.5 * ((x,y) - (mx,my)).T * [[sx,sp],[sp,sy]] * ((x,y) - (mx,my)))\n",
    "    \n",
    "    after taking log and add a minus (* -1)...\n",
    "        -( (-0.5 * log(4 * pi ^ 2 * sx * sy - sp * sp)) + (-0.5 * (...)))\n",
    "    \n",
    "    If batch size = B, sequence length = D...\n",
    "    Input:\n",
    "        Batch bivariate parameters (estimated): K.variable with shape (B,D,5),\n",
    "        Batch of overserved coordinates (label): K.variable with shape (B,D,2)\n",
    "    \n",
    "    Output:\n",
    "        a scaler (K.variable with shape ()), which is the sum of negative log likelihood\n",
    "'''\n",
    "\n",
    "def negative_log_likelihood_loss(batch_observed_coordinates,batch_bivariate_params):\n",
    "    # first check the dimension...\n",
    "    input_shape = K.int_shape(batch_bivariate_params)\n",
    "    target_shape = K.int_shape(batch_observed_coordinates)\n",
    "    \n",
    "    if len(input_shape) != 3 or len(target_shape) != 3:\n",
    "        raise ValueError(\"Dimension of both tensors should be 3\")\n",
    "    \n",
    "    if input_shape[0] != target_shape[0]:\n",
    "        raise ValueError(\"Batch size of both tensors should be the same\")\n",
    "    \n",
    "    if input_shape[1] != target_shape[1]:\n",
    "        raise ValueError(\"Sequence length of both tensors should be the same\")\n",
    "    \n",
    "    if input_shape[2] != 5:\n",
    "        raise ValueError(\"Number of predicted parameters should be 5. Namely, (mx,my,sx,sy,sp)\")\n",
    "    \n",
    "    if target_shape[2] != 2:\n",
    "        raise ValueError(\"Dimension of target coordinates should be 2. Namely, (x,y)\")\n",
    "    \n",
    "    # then split the tensors into (mx,my,sx,sy,sp)...\n",
    "    # all of them should be of shape (B,D)\n",
    "    batch_mx = batch_bivariate_params[:,:,0]\n",
    "    batch_my = batch_bivariate_params[:,:,1]\n",
    "    batch_sx = batch_bivariate_params[:,:,2]\n",
    "    batch_sy = batch_bivariate_params[:,:,3]\n",
    "    batch_sp = batch_bivariate_params[:,:,4]\n",
    "    \n",
    "    batch_x = batch_observed_coordinates[:,:,0]\n",
    "    batch_y = batch_observed_coordinates[:,:,1]\n",
    "    \n",
    "    dx = batch_x - batch_mx # (B,D), (x - mx)\n",
    "    dy = batch_y - batch_my # (B,D), (y - my)\n",
    "    dydx = Multiply()([dx,dy])\n",
    "    \n",
    "    batch_x_change =  K.concatenate([batch_mx[:,0:1], batch_mx[:,1:] - batch_mx[:,:-1]])\n",
    "    batch_y_change =  K.concatenate([batch_my[:,0:1], batch_my[:,1:] - batch_my[:,:-1]])\n",
    "    target_x_change =  K.concatenate([batch_x[:,0:1], batch_x[:,1:] - batch_x[:,:-1]])\n",
    "    target_y_change =  K.concatenate([batch_y[:,0:1], batch_y[:,1:] - batch_y[:,:-1]])\n",
    "    \n",
    "    xy_dot = Multiply()([batch_x_change,target_x_change]) + Multiply()([batch_y_change,target_y_change])\n",
    "    batch_change_norm = K.sqrt(K.square(batch_x_change) + K.square(batch_y_change))\n",
    "    target_change_norm = K.sqrt(K.square(target_x_change) + K.square(target_y_change))\n",
    "    \n",
    "    norm_prod = Multiply()([batch_change_norm,target_change_norm]) + 1e-6\n",
    "    norm_prod_inv = K.pow(norm_prod,-1) # for numerical stability\n",
    "    direction_loss =  - K.mean(Multiply()([xy_dot,norm_prod_inv]))\n",
    "#     det_inv = K.print_tensor(K.pow(det,-1), message=\"det_inv\") # (B,D), (sx * sy - sp^2) ^-1\n",
    "    # (B,D), (dx^2 * sy - 2 * sp * dy * dx + sx * dy^2)\n",
    "    exp = -0.5 * Multiply()([K.square(dx),batch_sy]) - 2 * Multiply()([dydx, batch_sp]) + Multiply()([K.square(dy),batch_sx])\n",
    "    # (B,D), -0.5 * (dx^2 * sy - 2 * sp * dy * dx + sx * dy^2) * det(Cov)^(-1)\n",
    "#     exp = Multiply()([det_inv,exp]) * (-0.5)\n",
    "    \n",
    "    # evaluate the final NLL\n",
    "    '''\n",
    "        A remark here: it is determined that the determininat of the covariance matrix will not be considered as a loss,\n",
    "        as the value of that generally became very large (under the log function)\n",
    "        therefore only the exponents are used as the loss\n",
    "    '''\n",
    "#     batch_nll = - (exp)\n",
    "    batch_nll = K.square(dx) + K.square(dy) - 0.1 * exp + 0.1 * direction_loss\n",
    "    batch_error_total = K.sum(batch_nll, axis = 1) # (B,) sum of NLL in a sequence...\n",
    "    return K.print_tensor(K.mean(batch_error_total, axis = 0)) # (), average of sum of NLL...\n",
    "\n",
    "# now test it...\n",
    "# a regular 0-centered, non-skewing normal\n",
    "[mx,my,sx,sy,sp] = [0,0,.1,.1,0.]\n",
    "bivariate_params = np.array([[[mx,my,sx,sy,sp]]]) # (1,1,5)\n",
    "target_point = np.array([[[-0.,0]]]) # (1,1,2)\n",
    "\n",
    "bivariate_ph = K.variable(value = bivariate_params, dtype = \"float32\")\n",
    "target_ph = K.variable(value = target_point, dtype = \"float32\")\n",
    "nll = negative_log_likelihood_loss(target_ph,bivariate_ph)\n",
    "K.eval(nll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_layer(batch_predicted_coordinates):\n",
    "    return K.cumsum(batch_predicted_coordinates,axis = 1)\n",
    "    \n",
    "def check_tensor(batch_observed_coordinates,batch_predicted_coordinates):\n",
    "    input_shape = K.int_shape(batch_predicted_coordinates)\n",
    "    target_shape = K.int_shape(batch_observed_coordinates)\n",
    "\n",
    "    if len(input_shape) != 3 or len(target_shape) != 3:\n",
    "        raise ValueError(\"Dimension of both tensors should be 3\")\n",
    "\n",
    "    if input_shape[0] != target_shape[0]:\n",
    "        raise ValueError(\"Batch size of both tensors should be the same\")\n",
    "\n",
    "    if input_shape[1] != target_shape[1]:\n",
    "        raise ValueError(\"Sequence length of both tensors should be the same\")\n",
    "\n",
    "    if input_shape[2] != 2:\n",
    "        raise ValueError(\"Number of predicted parameters should be 2. Namely, (mx,my)\")\n",
    "\n",
    "    if target_shape[2] != 2:\n",
    "        raise ValueError(\"Dimension of target coordinates should be 2. Namely, (x,y)\")\n",
    "    \n",
    "def ms_loss(input_tensor):\n",
    "    batch_observed_coordinates,batch_predicted_coordinates = input_tensor\n",
    "    # first check the dimension...\n",
    "    check_tensor(batch_observed_coordinates,batch_predicted_coordinates)\n",
    "    \n",
    "    diff = K.square(batch_predicted_coordinates - batch_observed_coordinates)\n",
    "\n",
    "    return K.sum(diff)\n",
    "\n",
    "def dir_loss(input_tensor):\n",
    "    batch_observed_coordinates,batch_predicted_coordinates = input_tensor\n",
    "    check_tensor(batch_observed_coordinates,batch_predicted_coordinates)\n",
    "    \n",
    "    predict_dir = batch_predicted_coordinates[:,1:] - batch_predicted_coordinates[:,:-1]\n",
    "    \n",
    "    target_dir = batch_observed_coordinates[:,1:] - batch_observed_coordinates[:,:-1]\n",
    "    \n",
    "#     predic_dir_norm = \n",
    "    # this is to maximize the cosine (therefore angle = 0)\n",
    "    return K.sum(K.square((predict_dir - target_dir)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# then, the Vanilla LSTM\n",
    "from keras.models import Model\n",
    "from keras.initializers import RandomNormal\n",
    "from keras import optimizers\n",
    "def vanilla_lstm_model(num_hidden, input_length, predict_length, lr = 1e-3):\n",
    "# def vanilla_lstm_model(num_hidden,input_length, predict_length, input_tensor, target_tensor):\n",
    "    total_length = input_length + predict_length\n",
    "    # the input\n",
    "    input_sequence = Input(shape = (total_length,2), name = 'input_sequence', dtype = 'float32') # (T, 2)\n",
    "    target_sequence = Input(shape = (total_length,2), name = 'target_sequence', dtype = 'float32') # (T, 2)\n",
    "    lstm = LSTM(num_hidden, return_sequences = True)(input_sequence) # (B,T,num_hidden)\n",
    "#     params = TimeDistributed(Dense(5, activation = 'elu'), name = 'params')(lstm) # (B,T,5)\n",
    "    predicted_coordinates_raw = TimeDistributed(Dense(2, activation = 'elu'), name = 'params')(lstm)\n",
    "    \n",
    "    # retrieve the prediction\n",
    "    extract_target_sequence_layer = Lambda(lambda x: x[:,input_length:,:])\n",
    "    predicted_coordinates_masked = extract_target_sequence_layer(predicted_coordinates_raw)\n",
    "    target_coordinates_masked = extract_target_sequence_layer(target_sequence)\n",
    "    # the output layer\n",
    "    predicted_output = Lambda(infer_layer, name = \"predict\")(predicted_coordinates_masked)\n",
    "    # compute the loss\n",
    "    \n",
    "    # first part: the square loss\n",
    "    sq_loss = Lambda(ms_loss, name = 'square_loss')([target_coordinates_masked, predicted_output])\n",
    "    # second part: the direction loss\n",
    "    ori_loss = Lambda(dir_loss, name = 'dir_loss')([target_coordinates_masked, predicted_output])\n",
    "    \n",
    "    loss = Lambda(lambda ts: ts[0] + ts[1],name = 'loss')([sq_loss, ori_loss])\n",
    "    \n",
    "    model = Model(\n",
    "        inputs = [input_sequence,target_sequence],\n",
    "        outputs = [predicted_output,loss])\n",
    "    \n",
    "    model.compile(optimizer=optimizers.RMSprop(lr = lr, clipvalue = 10., decay = 1e-6),\n",
    "                  # since there are two outputs of the model, the estimated params and the NLL,\n",
    "                  # their loss value should be specified.\n",
    "                  # for params there are no loss regarding its value, but I just assign a zero as loss (or the computational graph will break)\n",
    "                  # I made it loss - loss = 0.\n",
    "                  # and for the NLL, return the loss as-is.\n",
    "                  loss= {\n",
    "                        'predict': lambda _, loss: loss - loss, # meh...\n",
    "                          'loss': lambda _, loss: loss\n",
    "                    })\n",
    "    \n",
    "    return input_sequence, target_sequence, model, predicted_output,loss\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAEjNJREFUeJzt3X2QXXV9x/H3d3fZJORhCYZiyoYk2ihNqR2cFK10WqbgDNgOdKbqgGNHO4z5p1ifxg72gTr0n9rWPvxBO6VqdayVUuq0mTY1WkU7U5USxKcE0TQmEAgNIJCAMZtNvv3j3sTlZjf33N2z95x77vs1w+Tes7895zt3yGdPfr/v+W1kJpKkZhmpugBJUvkMd0lqIMNdkhrIcJekBjLcJamBDHdJaiDDXZIayHCXpAYy3CWpgcaquvD4yLJcNrqy5+/LpeOzHj+xNOb8nhOzfwuXrr2w5+tLUpXuv//+JzPzgm7jKgv3ZaMrec2aN/T8fVOXTM56/NmXLpnze46snz34d/7+u3q+viRVKSL2Fxk3UNMyZQa7JDXZQIW7JKkYw12SGshwl6QGanS4O98uaVg1OtwlaVgZ7pLUQIa7JDWQ4S5JDdTYcHcxVdIwG/hwP9vTqZI0rAY+3CVJZzLcJamBBibc59o0TJJ0poEJ9164mCpp2DUy3CVp2BnuktRAhrskNZDhLkkNZLhLUgMNdLjP9nSqnTKSNODhLkmaneEuSQ1kuEtSAw1EuLv1gCT1ZiDCXZLUG8NdkhqoUeFepA3y2MVTfahEkqrVqHA/m2MXT50OdgNeUtMNbLgv9NfrGfCSmmxgw70XBrmkYTMU4S5Ji2lq+ghPPrebkyePV13KaYa7JC3QkWMH+d+nPs3R49+vupTTxqouYLE5JSNpsU0sW89Pr30LS885r+pSTqt9uBd9OtXdICVVZWxkCWPjC2vyKJvTMpJUgmeO7uOp5x+quozTCoV7RFwTEQ9FxJ6IuGWWr18cEfdExAMR8Y2IeF35pfbOKRlJ/XLoyNc58MyXqy7jtK7hHhGjwO3AtcBm4MaI2Nwx7PeAuzLzMuAG4K/KLlSS6mz5+IUcmz7CyTxRdSlAsTn3y4E9mbkXICLuBK4Hds8Yk8Cq9usJ4LEyi+y00AeYJKlsE8vWMzJyDpknIUarLqdQuF8EPDLj/QHgVR1j3g98JiLeDiwHri6lOkkaECuWrGXFkrVVl3FaWQuqNwIfzcxJ4HXAxyPijHNHxNaI2BkRO6dOHi3p0rNzvl1Sv02fPMbU9JGqywCKhfujwLoZ7yfbx2a6CbgLIDO/DCwF1nSeKDPvyMwtmbllfGTZ/CqehW2Qkupg18FPsO/pL1RdBlAs3O8DNkXExogYp7Vguq1jzMPAVQAR8ZO0wv2JhRbnb2CSNEjOW/YSyHrcbHYN98ycBm4GdgAP0uqK2RURt0XEde1h7wHeFhFfBz4JvDUzc7GKlqQ6Gokxjh5/gjrEX6EnVDNzO7C949itM17vBq4otzRJGiyT513ButU/X3UZQEOfUHUxVVIVIuoxJQMDGO72uEuqq8xk75M7eOK5XVWXMnjhLkl1FRH84PhTTJ14rupS6r8rZDe2QUqqk0vXvqnqEgDv3CWpkWob7vPtcXcxVZJqHO6SpPkz3CWpRCdPnGTnB7/I/v/8bqV1GO6SVKKR0REe+9J+Hv+fh6uto9Kr98ged0mDYNX61Rze/0ylNQxUuHfqbIPstpi6YfIJNkwueD8zSTqrVrg/XekeMwMd7r2YGeoGvKTFtGrDaqaPHufoE89XVkMtw73srX4Nc0n9tHLdBABPfvNgZTXUMtz7wcCXtFiWnn8uAPs/v6eyGhof7oa4pH6b2HA+Y8vOYfSc6n5RduPDXZKqsPrlF/CDQ9VtIDYw4d6tDdJtByTVyar1qzm8r7qOmYEJ905FdoN0SkZSVSY2ns/x56c4+mQ1HTMDG+6SVGej46359n07vlPJ9WsX7mW3QUpSFV506YUAHHvmaCXXr124l8UpGUlVWnXxasYnljL9w+lKrt/YcJekKkUEE+tXc3jf9yu5fiPC3U4ZSXW0akN1HTMDEe7uBilpEI2dew5TR47x7Pf6f/c+EOHeqVsbpPPtkupgycQyoJo9ZmoV7nbKSGqS9a/dBECecFpGkhpj2ZrlnLNinGf3Pd33axvukrRIIuL0L+7ot4EPdztlJNXZ8gtX8sx3n+z7dQc+3Du5mCqpTqaPTXP8+SkOP9zfu/fah7ttkJIG2borXwrQ9w3EahPuRTtliuwGKUl1ccEr1gJw5MCzfb1ubcJdkpro3B9bwdiysb4vqhrukrSIYiRYeXFrG4J+alS4u5gqqY7yRHLoa4/19ZqFwj0iromIhyJiT0TcMseYN0bE7ojYFRH/UG6Zs7MNUtIgWDG5ipw+ydThH/btml3DPSJGgduBa4HNwI0RsbljzCbgfcAVmflTwDt7KWKuxVQ7ZSQ1wYbXvgzo76JqkTv3y4E9mbk3M6eAO4HrO8a8Dbg9M58GyMxD5ZbZYqeMpEG0asP5AH3dhmCswJiLgEdmvD8AvKpjzMsAIuK/gVHg/Zn56VIqLMFrX/zt068/+/glFVYiaRgtX7uSkfFRDu/v39a/ZS2ojgGbgCuBG4G/jYjzOgdFxNaI2BkRO6dOlvt7BedaTJ0Z7LO9l6TFNjI6wtLzlnHwS/v7d80CYx4F1s14P9k+NtMBYFtmHs/M7wHfoRX2L5CZd2TmlszcMj6ybL41S9LgCXju4OG+Xa5IuN8HbIqIjRExDtwAbOsY8y+07tqJiDW0pmn2lljnGYp0yniXLqkuNl57CSenTzJ99Hhfrtc13DNzGrgZ2AE8CNyVmbsi4raIuK49bAfwVETsBu4B3puZTxUpoIpOGUNfUr9NvOR8SDj8yDN9uV6RBVUyczuwvePYrTNeJ/Du9n+SpA4T61sdM4f3Pc35L7tg0a83ME+o9toG6d25pDpZMbmKGB3h8L7+dMwMTLifjdsOSKq7kbFRVq6b6NsGYoWmZSRJC/ez772SJRNL+3KtSsO96B7undxTRtIgWnPpi/t2rVpOyyy0U6bIfLtz8pKarJbhLklamIEIdzcMk6TeDES4n42dMpJ0psrCPZeOL8p5nUuXpBreufsLOiRp4WoX7t3YBilJ3dU+3F1MlaTe1T7cJUm9G+hw7+yUcTFVkloGOtwlSbOrVbjbKSNJ5ahVuHdjp4wkFVPrcLdTRpLmp9bh3gsXUyXpRwY23N1TRpLmVptwdzFVkspTm3CXJJVnYMLdThlJKq624W6njCTNX23DvRd2ykjSC9Ui3HtdTLVTRpLOrhbhLkkq10CEu4upktSbWoa7i6mStDC1DHdJ0sJUHu4LfTK1W6fM61d9dUHnl6RBVHm496qXTplTwW7ASxo2AxfuRRnokoZZ7cK9czHVThlJ6l3twl2StHCFwj0iromIhyJiT0TccpZxvxYRGRFbipy3n9v8Ok0jaZh0DfeIGAVuB64FNgM3RsTmWcatBN4B3Ft2kXOZq1PGIJc07IrcuV8O7MnMvZk5BdwJXD/LuD8EPgD8sMT6XsA9ZSSpmCLhfhHwyIz3B9rHTouIVwLrMvPfS6xNkjRPYws9QUSMAH8GvLXA2K3AVoAlS8474+t2ykhSOYrcuT8KrJvxfrJ97JSVwKXAFyJiH/BqYNtsi6qZeUdmbsnMLaMrV8y/6rNwvl2SioX7fcCmiNgYEePADcC2U1/MzGczc01mbsjMDcBXgOsyc+eiVCxJ6qpruGfmNHAzsAN4ELgrM3dFxG0Rcd1iFyhJ6l2hOffM3A5s7zh26xxjr1x4WWfq7JTxV+tJ0txq84Sqi6mSVJ7ahLskqTyNCnc7ZSSppVHhLklqMdwlqYFqEe7dfiG2e8pIUm9qEe6dunXK2AYpSWdXy3CXJC2M4S5JDdSYcLcNUpJ+pPJw77aYKknqXeXh3sltByRp4WoX7p1sg5Sk3tU+3DvZBilJ3Q1cuEuSuqs03F1MlaTFMTR37ncffmXVJUhS39Qq3O2UkaRy1CrcO9kpI0nzU+twlyTNT2XhfmK8qitLUvMN1J37XD3u7isjSS9Um3B3MVWSylObcJcklae24W6njCTNX23DfbF99vFLqi5BkhbNUIS7T6dKGjZDEe6SNGxqEe52ykhSuWoR7kW4j7skFVfLcLdTRpIWppbhLklaGMNdkhqo8nBf7MVU2yAlDaPKw70MBrgkvVChcI+IayLioYjYExG3zPL1d0fE7oj4RkR8LiLWl1+qJKmoruEeEaPA7cC1wGbgxojY3DHsAWBLZr4CuBv44/kWZKeMJC1ckTv3y4E9mbk3M6eAO4HrZw7IzHsy8wftt18BJsstc36crpE0rIqE+0XAIzPeH2gfm8tNwH8spKhOZT/A5KZhkppurMyTRcSbgS3AL87x9a3AVoCxidWL2injXbukYVbkzv1RYN2M95PtYy8QEVcDvwtcl5nHZjtRZt6RmVsyc8vo8uXzqXdOhrkk/UiRO/f7gE0RsZFWqN8AvGnmgIi4DPgb4JrMPFR6lQUZ8JLU0vXOPTOngZuBHcCDwF2ZuSsibouI69rD/gRYAfxTRHwtIrYtWsWSpK4Kzbln5nZge8exW2e8vrqMYmyDlKRyNOIJVUnSC1UW7jmeVV1akhrPO3dJaqChC3cfYJI0DIYu3CVpGBjuktRAtQl32yAlqTy1CXdJUnkMd0lqoNqHe9nb/UrSMKh9uJfJNkhJw2Kowl2ShoXhLkkNZLhLUgPVItztcZekctUi3CVJ5RqacLdTRtIwqX24G8qS1Lvah7skqXeGuyQ10FCEu1M7kobNUIT7bPYduKDqEiRp0VQe7va4S1L5Kg/3xeaUjKRh1Phwl6RhNBDh7t23JPVmIMJ9vvyhIGlYDUy4G9SSVNzAhHuv/GEgaZgNVLgXDexu4+xxl9R0AxXu0D24vWOXJBiruoD5MMAl6ewG7s5dktSd4S5JDWS4S1IDFQr3iLgmIh6KiD0RccssX18SEf/Y/vq9EbGhyHndNEySFkfXcI+IUeB24FpgM3BjRGzuGHYT8HRm/gTw58AHyi60LLZBShoGRe7cLwf2ZObezJwC7gSu7xhzPfCx9uu7gasiIsorU5LUiyLhfhHwyIz3B9rHZh2TmdPAs8CLyihQktS7vva5R8RWYGv77bEvXv3Bb/Xz+mfU89Yzlg/6bQ3wZNVF1ICfg5/BKX4O3T+D9UVOUiTcHwXWzXg/2T4225gDETEGTABPdZ4oM+8A7gCIiJ2ZuaVIkU3lZ9Di5+BncIqfQ3mfQZFpmfuATRGxMSLGgRuAbR1jtgFvab9+PfD5zMyFFidJmp+ud+6ZOR0RNwM7gFHgI5m5KyJuA3Zm5jbgw8DHI2IP8H1aPwAkSRUpNOeemduB7R3Hbp3x+ofAG3q89h09jm8iP4MWPwc/g1P8HEr6DMLZE0lqHrcfkKQGqiTcu21n0HQRsS4i7omI3RGxKyLeUXVNVYmI0Yh4ICL+repaqhIR50XE3RHx7Yh4MCJ+ruqa+i0i3tX+u/CtiPhkRCytuqZ+iIiPRMShiPjWjGPnR8RnI+K77T9Xz+fcfQ/3gtsZNN008J7M3Ay8GvjNIfwMTnkH8GDVRVTsL4FPZ+YlwM8wZJ9HRFwE/BawJTMvpdW4MSxNGR8Fruk4dgvwuczcBHyu/b5nVdy5F9nOoNEy82BmfrX9+gitv8ydT/02XkRMAr8MfKjqWqoSERPAL9DqOCMzpzLzmWqrqsQYsKz9nMy5wGMV19MXmflftDoMZ5q5ncvHgF+dz7mrCPci2xkMjfYOmpcB91ZbSSX+Avht4GTVhVRoI/AE8Hft6akPRcTyqovqp8x8FPhT4GHgIPBsZn6m2qoqdWFmHmy/fhy4cD4ncUG1QhGxAvhn4J2ZebjqevopIn4FOJSZ91ddS8XGgFcCf52ZlwHPM89/hg+q9pzy9bR+0P04sDwi3lxtVfXQfhh0Xi2NVYR7ke0MGi8izqEV7J/IzE9VXU8FrgCui4h9tKbmfiki/r7akipxADiQmaf+5XY3rbAfJlcD38vMJzLzOPAp4DUV11Sl/4uItQDtPw/N5yRVhHuR7Qwarb0d8oeBBzPzz6qupwqZ+b7MnMzMDbT+H/h8Zg7d3VpmPg48EhEvbx+6CthdYUlVeBh4dUSc2/67cRVDtqjcYeZ2Lm8B/nU+J+nrrpAw93YG/a6jYlcAvw58MyK+1j72O+0ngTV83g58on2zsxf4jYrr6avMvDci7ga+SquT7AGG5EnViPgkcCWwJiIOAH8A/BFwV0TcBOwH3jivc/uEqiQ1jwuqktRAhrskNZDhLkkNZLhLUgMZ7pLUQIa7JDWQ4S5JDWS4S1ID/T9jpr0M3Cp5pgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# helper function for monitoring training progress\n",
    "from scipy.stats import multivariate_normal\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "'''\n",
    "    Given a set of parameter (array of 5), visualize the heatmap of the bivariate normal distribution\n",
    "'''\n",
    "def draw_heatmap(params):\n",
    "    resolution = 100\n",
    "    interval = 1. / resolution\n",
    "    \n",
    "    x,y = np.mgrid[0:1:interval,0:1:interval]\n",
    "    pos = np.empty(x.shape + (2,))\n",
    "    pos[:,:,0] = x\n",
    "    pos[:,:,1] = y\n",
    "    result_distribution = None\n",
    "    for param in params:\n",
    "        mx,my,sx,sy,sp = param\n",
    "        F = multivariate_normal([mx,my],[[sx,sp],[sp,sy]])\n",
    "        result_distribution = F.pdf(pos) if result_distribution is None else result_distribution + F.pdf(pos)\n",
    "    plt.contourf(x,y,result_distribution)\n",
    "\n",
    "# try it out\n",
    "# fig = plt.figure()\n",
    "draw_heatmap(np.array([[0.3,0.1,0.4,0.2,.2],[0.9,0.2,0.1,0.9,0.]]))\n",
    "visualize_trace(input_batch,target_batch)\n",
    "# draw_heatmap(0.3,0.2,0.01,0.2,0.,fig)\n",
    "plt.show()\n",
    "\n",
    "'''\n",
    "    Draw the mean of the predicted params of all timestamps\n",
    "'''\n",
    "import matplotlib.cm as cm\n",
    "def draw_mean(params):\n",
    "    plt.xlim(0,10)\n",
    "    plt.ylim(0,10)\n",
    "    \n",
    "    \n",
    "    for batch in range(params.shape[0]):\n",
    "        line_color = np.random.rand(3) # choose a color to tell that these scatter points belong to the same prediction\n",
    "        xy_series = params[batch,:,:] # (D,2)\n",
    "        # prgressively change the color to indicate the direction\n",
    "        colors = cm.rainbow(np.linspace(0, 1, params.shape[1]))\n",
    "        plt.scatter(xy_series[:,0],xy_series[:,1], c = colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ReduceLROnPlateau, CSVLogger, LambdaCallback\n",
    "\n",
    "def get_callbacks(input_batch_padded,target_batch_padded,finetune = False):\n",
    "    # prepare callbacks\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='loss', \n",
    "                                  factor=0.1,\n",
    "                                  patience=50, \n",
    "                                  min_lr=1e-6)\n",
    "    csv_logger = CSVLogger(\"log.csv\")\n",
    "\n",
    "    def visualize_prediction(epoch, logs):\n",
    "        params, loss = model.predict([input_batch_padded,target_batch_padded])\n",
    "        # visualize the trace, as well as the distributions generated by the params...\n",
    "        # first clear the previous drawing...\n",
    "    #     try:\n",
    "        plt.gcf().clear()\n",
    "        visualize_trace(input_batch,target_batch)\n",
    "    #     params = params[:,INPUT_LENGTH + 1, :] # (B,5), and it should be the params immediately after the input\n",
    "    #     draw_heatmap(params)\n",
    "        draw_mean(params)\n",
    "        filename = '{}.png' if not finetune else '{}-finetune.png'\n",
    "        plt.savefig(filename.format(epoch))\n",
    "\n",
    "    plot_callback = LambdaCallback(on_epoch_begin = visualize_prediction)\n",
    "    \n",
    "    return [reduce_lr, csv_logger, plot_callback]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_sequence (InputLayer)     (None, 12, 2)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_9 (LSTM)                   (None, 12, 128)      67072       input_sequence[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "params (TimeDistributed)        (None, 12, 2)        258         lstm_9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_9 (Lambda)               (None, 4, 2)         0           params[0][0]                     \n",
      "                                                                 target_sequence[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "target_sequence (InputLayer)    (None, 12, 2)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "predict (Lambda)                (None, 4, 2)         0           lambda_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "square_loss (Lambda)            ()                   0           lambda_9[1][0]                   \n",
      "                                                                 predict[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dir_loss (Lambda)               ()                   0           lambda_9[1][0]                   \n",
      "                                                                 predict[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "loss (Lambda)                   ()                   0           square_loss[0][0]                \n",
      "                                                                 dir_loss[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 67,330\n",
      "Trainable params: 67,330\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/500\n",
      "16/16 [==============================] - 4s 225ms/step - loss: 5504.5400 - predict_loss: 0.0000e+00 - loss_loss: 5504.5400\n",
      "Epoch 2/500\n",
      "16/16 [==============================] - 1s 48ms/step - loss: 2531.3342 - predict_loss: 0.0000e+00 - loss_loss: 2531.3342\n",
      "Epoch 3/500\n",
      "16/16 [==============================] - 1s 59ms/step - loss: 1577.5123 - predict_loss: 0.0000e+00 - loss_loss: 1577.5123\n",
      "Epoch 4/500\n",
      "16/16 [==============================] - 1s 53ms/step - loss: 1378.8760 - predict_loss: 0.0000e+00 - loss_loss: 1378.8760\n",
      "Epoch 5/500\n",
      "16/16 [==============================] - 1s 52ms/step - loss: 1299.7690 - predict_loss: 0.0000e+00 - loss_loss: 1299.7690\n",
      "Epoch 6/500\n",
      "16/16 [==============================] - 1s 50ms/step - loss: 1236.3950 - predict_loss: 0.0000e+00 - loss_loss: 1236.3950\n",
      "Epoch 7/500\n",
      "16/16 [==============================] - 1s 53ms/step - loss: 1208.9680 - predict_loss: 0.0000e+00 - loss_loss: 1208.9680\n",
      "Epoch 8/500\n",
      "16/16 [==============================] - 1s 48ms/step - loss: 1165.0426 - predict_loss: 0.0000e+00 - loss_loss: 1165.0426\n",
      "Epoch 9/500\n",
      "16/16 [==============================] - 1s 59ms/step - loss: 1151.8960 - predict_loss: 0.0000e+00 - loss_loss: 1151.8960\n",
      "Epoch 10/500\n",
      "16/16 [==============================] - 1s 54ms/step - loss: 1126.2897 - predict_loss: 0.0000e+00 - loss_loss: 1126.2897\n",
      "Epoch 11/500\n",
      "16/16 [==============================] - 1s 45ms/step - loss: 1109.5127 - predict_loss: 0.0000e+00 - loss_loss: 1109.5127\n",
      "Epoch 12/500\n",
      "16/16 [==============================] - 1s 54ms/step - loss: 1065.9368 - predict_loss: 0.0000e+00 - loss_loss: 1065.9368\n",
      "Epoch 13/500\n",
      "16/16 [==============================] - 1s 46ms/step - loss: 1034.3486 - predict_loss: 0.0000e+00 - loss_loss: 1034.3486\n",
      "Epoch 14/500\n",
      "16/16 [==============================] - 1s 49ms/step - loss: 996.8464 - predict_loss: 0.0000e+00 - loss_loss: 996.8464\n",
      "Epoch 15/500\n",
      "16/16 [==============================] - 1s 57ms/step - loss: 969.7012 - predict_loss: 0.0000e+00 - loss_loss: 969.7012\n",
      "Epoch 16/500\n",
      "16/16 [==============================] - 1s 48ms/step - loss: 936.4682 - predict_loss: 0.0000e+00 - loss_loss: 936.4682\n",
      "Epoch 17/500\n",
      "16/16 [==============================] - 1s 52ms/step - loss: 912.3648 - predict_loss: 0.0000e+00 - loss_loss: 912.3648\n",
      "Epoch 18/500\n",
      "16/16 [==============================] - 1s 52ms/step - loss: 884.7297 - predict_loss: 0.0000e+00 - loss_loss: 884.7297\n",
      "Epoch 19/500\n",
      "16/16 [==============================] - 1s 52ms/step - loss: 869.7283 - predict_loss: 0.0000e+00 - loss_loss: 869.7283\n",
      "Epoch 20/500\n",
      "16/16 [==============================] - 1s 50ms/step - loss: 858.3765 - predict_loss: 0.0000e+00 - loss_loss: 858.3765\n",
      "Epoch 21/500\n",
      "16/16 [==============================] - 1s 50ms/step - loss: 870.5552 - predict_loss: 0.0000e+00 - loss_loss: 870.5552\n",
      "Epoch 22/500\n",
      "16/16 [==============================] - 1s 90ms/step - loss: 835.5745 - predict_loss: 0.0000e+00 - loss_loss: 835.5745\n",
      "Epoch 23/500\n",
      "16/16 [==============================] - 1s 54ms/step - loss: 840.3429 - predict_loss: 0.0000e+00 - loss_loss: 840.3429\n",
      "Epoch 24/500\n",
      "16/16 [==============================] - 1s 56ms/step - loss: 784.9713 - predict_loss: 0.0000e+00 - loss_loss: 784.9713\n",
      "Epoch 25/500\n",
      "16/16 [==============================] - 1s 47ms/step - loss: 795.5100 - predict_loss: 0.0000e+00 - loss_loss: 795.5100\n",
      "Epoch 26/500\n",
      "16/16 [==============================] - 1s 56ms/step - loss: 749.7186 - predict_loss: 0.0000e+00 - loss_loss: 749.7186\n",
      "Epoch 27/500\n",
      "16/16 [==============================] - 1s 55ms/step - loss: 762.8130 - predict_loss: 0.0000e+00 - loss_loss: 762.8130\n",
      "Epoch 28/500\n",
      "16/16 [==============================] - 1s 52ms/step - loss: 719.1021 - predict_loss: 0.0000e+00 - loss_loss: 719.1021\n",
      "Epoch 29/500\n",
      "16/16 [==============================] - 1s 51ms/step - loss: 738.4288 - predict_loss: 0.0000e+00 - loss_loss: 738.4288\n",
      "Epoch 30/500\n",
      "16/16 [==============================] - 1s 50ms/step - loss: 690.4890 - predict_loss: 0.0000e+00 - loss_loss: 690.4890\n",
      "Epoch 31/500\n",
      "16/16 [==============================] - 1s 50ms/step - loss: 713.8020 - predict_loss: 0.0000e+00 - loss_loss: 713.8020\n",
      "Epoch 32/500\n",
      "16/16 [==============================] - 1s 49ms/step - loss: 663.1436 - predict_loss: 0.0000e+00 - loss_loss: 663.1436\n",
      "Epoch 33/500\n",
      "16/16 [==============================] - 1s 62ms/step - loss: 690.3915 - predict_loss: 0.0000e+00 - loss_loss: 690.3915\n",
      "Epoch 34/500\n",
      "16/16 [==============================] - 1s 46ms/step - loss: 636.9376 - predict_loss: 0.0000e+00 - loss_loss: 636.9376\n",
      "Epoch 35/500\n",
      "16/16 [==============================] - 1s 48ms/step - loss: 669.3372 - predict_loss: 0.0000e+00 - loss_loss: 669.3372\n",
      "Epoch 36/500\n",
      "16/16 [==============================] - 1s 49ms/step - loss: 612.6093 - predict_loss: 0.0000e+00 - loss_loss: 612.6093\n",
      "Epoch 37/500\n",
      "16/16 [==============================] - 1s 50ms/step - loss: 650.3044 - predict_loss: 0.0000e+00 - loss_loss: 650.3044\n",
      "Epoch 38/500\n",
      "16/16 [==============================] - 1s 56ms/step - loss: 589.8066 - predict_loss: 0.0000e+00 - loss_loss: 589.8066\n",
      "Epoch 39/500\n",
      "16/16 [==============================] - 1s 63ms/step - loss: 632.2203 - predict_loss: 0.0000e+00 - loss_loss: 632.2203\n",
      "Epoch 40/500\n",
      "16/16 [==============================] - 1s 48ms/step - loss: 567.6861 - predict_loss: 0.0000e+00 - loss_loss: 567.6861\n",
      "Epoch 41/500\n",
      "16/16 [==============================] - 1s 46ms/step - loss: 614.1947 - predict_loss: 0.0000e+00 - loss_loss: 614.1947\n",
      "Epoch 42/500\n",
      "16/16 [==============================] - 1s 45ms/step - loss: 547.6965 - predict_loss: 0.0000e+00 - loss_loss: 547.6965\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/500\n",
      "16/16 [==============================] - 1s 50ms/step - loss: 599.1645 - predict_loss: 0.0000e+00 - loss_loss: 599.1645\n",
      "Epoch 44/500\n",
      "16/16 [==============================] - 1s 56ms/step - loss: 532.7977 - predict_loss: 0.0000e+00 - loss_loss: 532.7977\n",
      "Epoch 45/500\n",
      "16/16 [==============================] - 1s 66ms/step - loss: 588.0632 - predict_loss: 0.0000e+00 - loss_loss: 588.0632\n",
      "Epoch 46/500\n",
      "16/16 [==============================] - 1s 53ms/step - loss: 516.4918 - predict_loss: 0.0000e+00 - loss_loss: 516.4918\n",
      "Epoch 47/500\n",
      "16/16 [==============================] - 1s 89ms/step - loss: 567.3361 - predict_loss: 0.0000e+00 - loss_loss: 567.3361\n",
      "Epoch 48/500\n",
      "16/16 [==============================] - 1s 43ms/step - loss: 497.8781 - predict_loss: 0.0000e+00 - loss_loss: 497.8781\n",
      "Epoch 49/500\n",
      "16/16 [==============================] - 1s 51ms/step - loss: 549.9171 - predict_loss: 0.0000e+00 - loss_loss: 549.9171\n",
      "Epoch 50/500\n",
      "16/16 [==============================] - 1s 56ms/step - loss: 486.7513 - predict_loss: 0.0000e+00 - loss_loss: 486.7513\n",
      "Epoch 51/500\n",
      "16/16 [==============================] - 1s 63ms/step - loss: 547.0320 - predict_loss: 0.0000e+00 - loss_loss: 547.0320\n",
      "Epoch 52/500\n",
      "16/16 [==============================] - 1s 51ms/step - loss: 475.1230 - predict_loss: 0.0000e+00 - loss_loss: 475.1230\n",
      "Epoch 53/500\n",
      "16/16 [==============================] - 1s 58ms/step - loss: 534.3795 - predict_loss: 0.0000e+00 - loss_loss: 534.3795\n",
      "Epoch 54/500\n",
      "16/16 [==============================] - 1s 57ms/step - loss: 450.8458 - predict_loss: 0.0000e+00 - loss_loss: 450.8458\n",
      "Epoch 55/500\n",
      "16/16 [==============================] - 1s 76ms/step - loss: 511.1894 - predict_loss: 0.0000e+00 - loss_loss: 511.1894\n",
      "Epoch 56/500\n",
      "16/16 [==============================] - 2s 94ms/step - loss: 430.9623 - predict_loss: 0.0000e+00 - loss_loss: 430.9623\n",
      "Epoch 57/500\n",
      "16/16 [==============================] - 1s 66ms/step - loss: 493.3202 - predict_loss: 0.0000e+00 - loss_loss: 493.3202\n",
      "Epoch 58/500\n",
      "16/16 [==============================] - 1s 63ms/step - loss: 424.1736 - predict_loss: 0.0000e+00 - loss_loss: 424.1736\n",
      "Epoch 59/500\n",
      "16/16 [==============================] - 1s 76ms/step - loss: 502.8005 - predict_loss: 0.0000e+00 - loss_loss: 502.8005\n",
      "Epoch 60/500\n",
      "16/16 [==============================] - 1s 91ms/step - loss: 429.9502 - predict_loss: 0.0000e+00 - loss_loss: 429.9502\n",
      "Epoch 61/500\n",
      "16/16 [==============================] - 1s 84ms/step - loss: 494.3681 - predict_loss: 0.0000e+00 - loss_loss: 494.3681\n",
      "Epoch 62/500\n",
      "16/16 [==============================] - 1s 80ms/step - loss: 411.2070 - predict_loss: 0.0000e+00 - loss_loss: 411.2070\n",
      "Epoch 63/500\n",
      "16/16 [==============================] - 1s 93ms/step - loss: 469.5523 - predict_loss: 0.0000e+00 - loss_loss: 469.5523\n",
      "Epoch 64/500\n",
      "16/16 [==============================] - 2s 104ms/step - loss: 393.5376 - predict_loss: 0.0000e+00 - loss_loss: 393.5376\n",
      "Epoch 65/500\n",
      "16/16 [==============================] - 2s 119ms/step - loss: 450.8143 - predict_loss: 0.0000e+00 - loss_loss: 450.8143\n",
      "Epoch 66/500\n",
      "16/16 [==============================] - 1s 83ms/step - loss: 384.3998 - predict_loss: 0.0000e+00 - loss_loss: 384.3998\n",
      "Epoch 67/500\n",
      "16/16 [==============================] - 1s 51ms/step - loss: 449.4847 - predict_loss: 0.0000e+00 - loss_loss: 449.4847\n",
      "Epoch 68/500\n",
      "16/16 [==============================] - 1s 53ms/step - loss: 390.4022 - predict_loss: 0.0000e+00 - loss_loss: 390.4022\n",
      "Epoch 69/500\n",
      "16/16 [==============================] - 1s 63ms/step - loss: 451.0259 - predict_loss: 0.0000e+00 - loss_loss: 451.0259\n",
      "Epoch 70/500\n",
      "16/16 [==============================] - 1s 90ms/step - loss: 369.9011 - predict_loss: 0.0000e+00 - loss_loss: 369.9011\n",
      "Epoch 71/500\n",
      "16/16 [==============================] - 2s 103ms/step - loss: 434.2368 - predict_loss: 0.0000e+00 - loss_loss: 434.2368\n",
      "Epoch 72/500\n",
      "16/16 [==============================] - 2s 135ms/step - loss: 357.3493 - predict_loss: 0.0000e+00 - loss_loss: 357.3493\n",
      "Epoch 73/500\n",
      "16/16 [==============================] - 1s 91ms/step - loss: 421.5818 - predict_loss: 0.0000e+00 - loss_loss: 421.5818\n",
      "Epoch 74/500\n",
      "16/16 [==============================] - 1s 72ms/step - loss: 349.1275 - predict_loss: 0.0000e+00 - loss_loss: 349.1275\n",
      "Epoch 75/500\n",
      "16/16 [==============================] - 1s 65ms/step - loss: 412.9086 - predict_loss: 0.0000e+00 - loss_loss: 412.9086\n",
      "Epoch 76/500\n",
      "16/16 [==============================] - 1s 63ms/step - loss: 341.5815 - predict_loss: 0.0000e+00 - loss_loss: 341.5815\n",
      "Epoch 77/500\n",
      "16/16 [==============================] - 1s 70ms/step - loss: 404.9107 - predict_loss: 0.0000e+00 - loss_loss: 404.9107\n",
      "Epoch 78/500\n",
      "16/16 [==============================] - 1s 84ms/step - loss: 333.6524 - predict_loss: 0.0000e+00 - loss_loss: 333.6524\n",
      "Epoch 79/500\n",
      "16/16 [==============================] - 1s 79ms/step - loss: 394.3573 - predict_loss: 0.0000e+00 - loss_loss: 394.3573\n",
      "Epoch 80/500\n",
      "16/16 [==============================] - 1s 85ms/step - loss: 322.9888 - predict_loss: 0.0000e+00 - loss_loss: 322.9888\n",
      "Epoch 81/500\n",
      "16/16 [==============================] - 1s 56ms/step - loss: 382.6432 - predict_loss: 0.0000e+00 - loss_loss: 382.6432\n",
      "Epoch 82/500\n",
      "16/16 [==============================] - 1s 78ms/step - loss: 312.2115 - predict_loss: 0.0000e+00 - loss_loss: 312.2115\n",
      "Epoch 83/500\n",
      "16/16 [==============================] - 1s 52ms/step - loss: 370.8329 - predict_loss: 0.0000e+00 - loss_loss: 370.8329\n",
      "Epoch 84/500\n",
      "16/16 [==============================] - 1s 47ms/step - loss: 302.6350 - predict_loss: 0.0000e+00 - loss_loss: 302.6350\n",
      "Epoch 85/500\n",
      "16/16 [==============================] - 1s 49ms/step - loss: 365.0157 - predict_loss: 0.0000e+00 - loss_loss: 365.0157\n",
      "Epoch 86/500\n",
      "16/16 [==============================] - 1s 43ms/step - loss: 304.0497 - predict_loss: 0.0000e+00 - loss_loss: 304.0497\n",
      "Epoch 87/500\n",
      "16/16 [==============================] - 1s 43ms/step - loss: 375.9190 - predict_loss: 0.0000e+00 - loss_loss: 375.9190\n",
      "Epoch 88/500\n",
      "16/16 [==============================] - 2s 112ms/step - loss: 294.1281 - predict_loss: 0.0000e+00 - loss_loss: 294.1281\n",
      "Epoch 89/500\n",
      "16/16 [==============================] - 1s 90ms/step - loss: 360.1476 - predict_loss: 0.0000e+00 - loss_loss: 360.1476\n",
      "Epoch 90/500\n",
      "16/16 [==============================] - 1s 57ms/step - loss: 278.9581 - predict_loss: 0.0000e+00 - loss_loss: 278.9581\n",
      "Epoch 91/500\n",
      "16/16 [==============================] - 1s 87ms/step - loss: 335.0246 - predict_loss: 0.0000e+00 - loss_loss: 335.0246\n",
      "Epoch 92/500\n",
      "16/16 [==============================] - 1s 57ms/step - loss: 268.5411 - predict_loss: 0.0000e+00 - loss_loss: 268.5411\n",
      "Epoch 93/500\n",
      "16/16 [==============================] - 1s 90ms/step - loss: 317.4829 - predict_loss: 0.0000e+00 - loss_loss: 317.4829\n",
      "Epoch 94/500\n",
      "16/16 [==============================] - 1s 48ms/step - loss: 260.7423 - predict_loss: 0.0000e+00 - loss_loss: 260.7423\n",
      "Epoch 95/500\n",
      "16/16 [==============================] - 1s 45ms/step - loss: 307.8158 - predict_loss: 0.0000e+00 - loss_loss: 307.8158\n",
      "Epoch 96/500\n",
      "16/16 [==============================] - 2s 112ms/step - loss: 254.2026 - predict_loss: 0.0000e+00 - loss_loss: 254.2026\n",
      "Epoch 97/500\n",
      "16/16 [==============================] - 1s 49ms/step - loss: 315.8802 - predict_loss: 0.0000e+00 - loss_loss: 315.8802\n",
      "Epoch 98/500\n",
      "16/16 [==============================] - 1s 60ms/step - loss: 271.2353 - predict_loss: 0.0000e+00 - loss_loss: 271.2353\n",
      "Epoch 99/500\n",
      "16/16 [==============================] - 1s 83ms/step - loss: 346.4623 - predict_loss: 0.0000e+00 - loss_loss: 346.4623\n",
      "Epoch 100/500\n",
      "16/16 [==============================] - 1s 48ms/step - loss: 274.1647 - predict_loss: 0.0000e+00 - loss_loss: 274.1647\n",
      "Epoch 101/500\n",
      "16/16 [==============================] - 1s 58ms/step - loss: 329.3842 - predict_loss: 0.0000e+00 - loss_loss: 329.3842\n",
      "Epoch 102/500\n",
      "16/16 [==============================] - 1s 68ms/step - loss: 245.9907 - predict_loss: 0.0000e+00 - loss_loss: 245.9907\n",
      "Epoch 103/500\n",
      "16/16 [==============================] - 1s 45ms/step - loss: 307.3600 - predict_loss: 0.0000e+00 - loss_loss: 307.3600\n",
      "Epoch 104/500\n",
      "16/16 [==============================] - 1s 81ms/step - loss: 235.4580 - predict_loss: 0.0000e+00 - loss_loss: 235.4580\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 105/500\n",
      "16/16 [==============================] - 1s 58ms/step - loss: 294.1098 - predict_loss: 0.0000e+00 - loss_loss: 294.1098\n",
      "Epoch 106/500\n",
      "16/16 [==============================] - 2s 107ms/step - loss: 230.5146 - predict_loss: 0.0000e+00 - loss_loss: 230.5146\n",
      "Epoch 107/500\n",
      "16/16 [==============================] - 1s 61ms/step - loss: 286.8130 - predict_loss: 0.0000e+00 - loss_loss: 286.8130\n",
      "Epoch 108/500\n",
      "16/16 [==============================] - 1s 46ms/step - loss: 230.3714 - predict_loss: 0.0000e+00 - loss_loss: 230.3714\n",
      "Epoch 109/500\n",
      "16/16 [==============================] - 1s 54ms/step - loss: 300.6978 - predict_loss: 0.0000e+00 - loss_loss: 300.6978\n",
      "Epoch 110/500\n",
      "16/16 [==============================] - 2s 98ms/step - loss: 228.2062 - predict_loss: 0.0000e+00 - loss_loss: 228.2062\n",
      "Epoch 111/500\n",
      "16/16 [==============================] - 1s 84ms/step - loss: 292.7050 - predict_loss: 0.0000e+00 - loss_loss: 292.7050\n",
      "Epoch 112/500\n",
      "16/16 [==============================] - 1s 46ms/step - loss: 213.7111 - predict_loss: 0.0000e+00 - loss_loss: 213.7111\n",
      "Epoch 113/500\n",
      "16/16 [==============================] - 1s 43ms/step - loss: 280.3482 - predict_loss: 0.0000e+00 - loss_loss: 280.3482\n",
      "Epoch 114/500\n",
      "16/16 [==============================] - 1s 45ms/step - loss: 207.5084 - predict_loss: 0.0000e+00 - loss_loss: 207.5084\n",
      "Epoch 115/500\n",
      "16/16 [==============================] - 1s 41ms/step - loss: 272.5679 - predict_loss: 0.0000e+00 - loss_loss: 272.5679\n",
      "Epoch 116/500\n",
      "16/16 [==============================] - 1s 79ms/step - loss: 203.4906 - predict_loss: 0.0000e+00 - loss_loss: 203.4906\n",
      "Epoch 117/500\n",
      "16/16 [==============================] - 1s 56ms/step - loss: 266.7160 - predict_loss: 0.0000e+00 - loss_loss: 266.7160\n",
      "Epoch 118/500\n",
      "16/16 [==============================] - 1s 58ms/step - loss: 199.2709 - predict_loss: 0.0000e+00 - loss_loss: 199.2709\n",
      "Epoch 119/500\n",
      "16/16 [==============================] - 1s 89ms/step - loss: 260.7317 - predict_loss: 0.0000e+00 - loss_loss: 260.7317\n",
      "Epoch 120/500\n",
      "16/16 [==============================] - 1s 71ms/step - loss: 195.5871 - predict_loss: 0.0000e+00 - loss_loss: 195.5871\n",
      "Epoch 121/500\n",
      "16/16 [==============================] - 2s 102ms/step - loss: 254.9899 - predict_loss: 0.0000e+00 - loss_loss: 254.9899\n",
      "Epoch 122/500\n",
      "16/16 [==============================] - 1s 43ms/step - loss: 192.7933 - predict_loss: 0.0000e+00 - loss_loss: 192.7933\n",
      "Epoch 123/500\n",
      "16/16 [==============================] - 1s 42ms/step - loss: 251.0117 - predict_loss: 0.0000e+00 - loss_loss: 251.0117\n",
      "Epoch 124/500\n",
      "16/16 [==============================] - 1s 42ms/step - loss: 199.3087 - predict_loss: 0.0000e+00 - loss_loss: 199.3087\n",
      "Epoch 125/500\n",
      "16/16 [==============================] - 1s 47ms/step - loss: 256.4558 - predict_loss: 0.0000e+00 - loss_loss: 256.4558\n",
      "Epoch 126/500\n",
      "16/16 [==============================] - 1s 50ms/step - loss: 188.3664 - predict_loss: 0.0000e+00 - loss_loss: 188.3664\n",
      "Epoch 127/500\n",
      "16/16 [==============================] - 1s 57ms/step - loss: 240.1869 - predict_loss: 0.0000e+00 - loss_loss: 240.1869\n",
      "Epoch 128/500\n",
      "16/16 [==============================] - 2s 94ms/step - loss: 170.0690 - predict_loss: 0.0000e+00 - loss_loss: 170.0690\n",
      "Epoch 129/500\n",
      "16/16 [==============================] - 1s 66ms/step - loss: 220.5606 - predict_loss: 0.0000e+00 - loss_loss: 220.5606\n",
      "Epoch 130/500\n",
      "16/16 [==============================] - 1s 54ms/step - loss: 162.8203 - predict_loss: 0.0000e+00 - loss_loss: 162.8203\n",
      "Epoch 131/500\n",
      "16/16 [==============================] - 1s 52ms/step - loss: 213.6422 - predict_loss: 0.0000e+00 - loss_loss: 213.6422\n",
      "Epoch 132/500\n",
      "16/16 [==============================] - 1s 54ms/step - loss: 171.5945 - predict_loss: 0.0000e+00 - loss_loss: 171.5945\n",
      "Epoch 133/500\n",
      "16/16 [==============================] - 1s 49ms/step - loss: 236.9241 - predict_loss: 0.0000e+00 - loss_loss: 236.9241\n",
      "Epoch 134/500\n",
      "16/16 [==============================] - 1s 55ms/step - loss: 198.1490 - predict_loss: 0.0000e+00 - loss_loss: 198.1490\n",
      "Epoch 135/500\n",
      "16/16 [==============================] - 1s 93ms/step - loss: 241.4365 - predict_loss: 0.0000e+00 - loss_loss: 241.4365\n",
      "Epoch 136/500\n",
      "16/16 [==============================] - 1s 83ms/step - loss: 166.7612 - predict_loss: 0.0000e+00 - loss_loss: 166.7612\n",
      "Epoch 137/500\n",
      "16/16 [==============================] - 1s 70ms/step - loss: 224.4277 - predict_loss: 0.0000e+00 - loss_loss: 224.4277\n",
      "Epoch 138/500\n",
      "16/16 [==============================] - 1s 52ms/step - loss: 159.3764 - predict_loss: 0.0000e+00 - loss_loss: 159.3764\n",
      "Epoch 139/500\n",
      "16/16 [==============================] - 1s 42ms/step - loss: 215.8554 - predict_loss: 0.0000e+00 - loss_loss: 215.8554\n",
      "Epoch 140/500\n",
      "16/16 [==============================] - 1s 50ms/step - loss: 154.3080 - predict_loss: 0.0000e+00 - loss_loss: 154.3080\n",
      "Epoch 141/500\n",
      "16/16 [==============================] - 1s 91ms/step - loss: 210.4931 - predict_loss: 0.0000e+00 - loss_loss: 210.4931\n",
      "Epoch 142/500\n",
      "16/16 [==============================] - 2s 99ms/step - loss: 151.8007 - predict_loss: 0.0000e+00 - loss_loss: 151.8007\n",
      "Epoch 143/500\n",
      "16/16 [==============================] - 1s 66ms/step - loss: 205.1106 - predict_loss: 0.0000e+00 - loss_loss: 205.1106\n",
      "Epoch 144/500\n",
      "16/16 [==============================] - 1s 59ms/step - loss: 146.9349 - predict_loss: 0.0000e+00 - loss_loss: 146.9349\n",
      "Epoch 145/500\n",
      "16/16 [==============================] - 1s 71ms/step - loss: 199.5001 - predict_loss: 0.0000e+00 - loss_loss: 199.5001\n",
      "Epoch 146/500\n",
      "16/16 [==============================] - 1s 58ms/step - loss: 144.5085 - predict_loss: 0.0000e+00 - loss_loss: 144.5085\n",
      "Epoch 147/500\n",
      "16/16 [==============================] - 2s 108ms/step - loss: 194.3470 - predict_loss: 0.0000e+00 - loss_loss: 194.3470\n",
      "Epoch 148/500\n",
      "16/16 [==============================] - 1s 81ms/step - loss: 140.2388 - predict_loss: 0.0000e+00 - loss_loss: 140.2388\n",
      "Epoch 149/500\n",
      "16/16 [==============================] - 1s 86ms/step - loss: 189.0967 - predict_loss: 0.0000e+00 - loss_loss: 189.0967\n",
      "Epoch 150/500\n",
      "16/16 [==============================] - 1s 65ms/step - loss: 137.7097 - predict_loss: 0.0000e+00 - loss_loss: 137.7097\n",
      "Epoch 151/500\n",
      "16/16 [==============================] - 1s 68ms/step - loss: 184.2235 - predict_loss: 0.0000e+00 - loss_loss: 184.2235\n",
      "Epoch 152/500\n",
      "16/16 [==============================] - 1s 69ms/step - loss: 132.7037 - predict_loss: 0.0000e+00 - loss_loss: 132.7037\n",
      "Epoch 153/500\n",
      "16/16 [==============================] - 1s 77ms/step - loss: 180.7568 - predict_loss: 0.0000e+00 - loss_loss: 180.7568\n",
      "Epoch 154/500\n",
      "16/16 [==============================] - 1s 76ms/step - loss: 134.0005 - predict_loss: 0.0000e+00 - loss_loss: 134.0005\n",
      "Epoch 155/500\n",
      "16/16 [==============================] - 1s 73ms/step - loss: 183.2337 - predict_loss: 0.0000e+00 - loss_loss: 183.2337\n",
      "Epoch 156/500\n",
      "16/16 [==============================] - 1s 62ms/step - loss: 130.5645 - predict_loss: 0.0000e+00 - loss_loss: 130.5645\n",
      "Epoch 157/500\n",
      "16/16 [==============================] - 1s 67ms/step - loss: 184.9335 - predict_loss: 0.0000e+00 - loss_loss: 184.9335\n",
      "Epoch 158/500\n",
      "16/16 [==============================] - 1s 79ms/step - loss: 135.4636 - predict_loss: 0.0000e+00 - loss_loss: 135.4636\n",
      "Epoch 159/500\n",
      "16/16 [==============================] - 1s 62ms/step - loss: 173.8748 - predict_loss: 0.0000e+00 - loss_loss: 173.8748\n",
      "Epoch 160/500\n",
      "16/16 [==============================] - 1s 63ms/step - loss: 135.5394 - predict_loss: 0.0000e+00 - loss_loss: 135.5394\n",
      "Epoch 161/500\n",
      "16/16 [==============================] - 1s 67ms/step - loss: 154.7211 - predict_loss: 0.0000e+00 - loss_loss: 154.7211\n",
      "Epoch 162/500\n",
      "16/16 [==============================] - 1s 77ms/step - loss: 137.4552 - predict_loss: 0.0000e+00 - loss_loss: 137.4552\n",
      "Epoch 163/500\n",
      "16/16 [==============================] - 1s 69ms/step - loss: 129.2249 - predict_loss: 0.0000e+00 - loss_loss: 129.2249\n",
      "Epoch 164/500\n",
      "16/16 [==============================] - 1s 66ms/step - loss: 131.5298 - predict_loss: 0.0000e+00 - loss_loss: 131.5298\n",
      "Epoch 165/500\n",
      "16/16 [==============================] - 1s 59ms/step - loss: 115.3770 - predict_loss: 0.0000e+00 - loss_loss: 115.3770\n",
      "Epoch 166/500\n",
      "16/16 [==============================] - 1s 61ms/step - loss: 131.7890 - predict_loss: 0.0000e+00 - loss_loss: 131.7890\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 167/500\n",
      "16/16 [==============================] - 1s 87ms/step - loss: 114.6502 - predict_loss: 0.0000e+00 - loss_loss: 114.6502\n",
      "Epoch 168/500\n",
      "16/16 [==============================] - 1s 65ms/step - loss: 189.1960 - predict_loss: 0.0000e+00 - loss_loss: 189.1960\n",
      "Epoch 169/500\n",
      "16/16 [==============================] - 1s 65ms/step - loss: 133.8996 - predict_loss: 0.0000e+00 - loss_loss: 133.8996\n",
      "Epoch 170/500\n",
      "16/16 [==============================] - 1s 77ms/step - loss: 179.6741 - predict_loss: 0.0000e+00 - loss_loss: 179.6741\n",
      "Epoch 171/500\n",
      "16/16 [==============================] - 3s 188ms/step - loss: 119.3441 - predict_loss: 0.0000e+00 - loss_loss: 119.3441\n",
      "Epoch 172/500\n",
      "16/16 [==============================] - 1s 87ms/step - loss: 166.3622 - predict_loss: 0.0000e+00 - loss_loss: 166.3622\n",
      "Epoch 173/500\n",
      "16/16 [==============================] - 1s 78ms/step - loss: 111.0631 - predict_loss: 0.0000e+00 - loss_loss: 111.0631\n",
      "Epoch 174/500\n",
      "16/16 [==============================] - 2s 107ms/step - loss: 156.4118 - predict_loss: 0.0000e+00 - loss_loss: 156.4118\n",
      "Epoch 175/500\n",
      "16/16 [==============================] - 1s 64ms/step - loss: 107.1906 - predict_loss: 0.0000e+00 - loss_loss: 107.1906\n",
      "Epoch 176/500\n",
      "16/16 [==============================] - 1s 65ms/step - loss: 149.4111 - predict_loss: 0.0000e+00 - loss_loss: 149.4111\n",
      "Epoch 177/500\n",
      "16/16 [==============================] - 1s 58ms/step - loss: 102.8743 - predict_loss: 0.0000e+00 - loss_loss: 102.8743\n",
      "Epoch 178/500\n",
      "16/16 [==============================] - 1s 92ms/step - loss: 144.2240 - predict_loss: 0.0000e+00 - loss_loss: 144.2240\n",
      "Epoch 179/500\n",
      "16/16 [==============================] - 2s 102ms/step - loss: 101.0848 - predict_loss: 0.0000e+00 - loss_loss: 101.0848\n",
      "Epoch 180/500\n",
      "16/16 [==============================] - 1s 91ms/step - loss: 139.3584 - predict_loss: 0.0000e+00 - loss_loss: 139.3584\n",
      "Epoch 181/500\n",
      "16/16 [==============================] - 1s 82ms/step - loss: 97.6017 - predict_loss: 0.0000e+00 - loss_loss: 97.6017\n",
      "Epoch 182/500\n",
      "16/16 [==============================] - 1s 56ms/step - loss: 135.5171 - predict_loss: 0.0000e+00 - loss_loss: 135.5171\n",
      "Epoch 183/500\n",
      "16/16 [==============================] - 1s 47ms/step - loss: 96.2813 - predict_loss: 0.0000e+00 - loss_loss: 96.2813\n",
      "Epoch 184/500\n",
      "16/16 [==============================] - 1s 67ms/step - loss: 132.1774 - predict_loss: 0.0000e+00 - loss_loss: 132.1774\n",
      "Epoch 185/500\n",
      "16/16 [==============================] - 1s 83ms/step - loss: 93.0161 - predict_loss: 0.0000e+00 - loss_loss: 93.0161\n",
      "Epoch 186/500\n",
      "16/16 [==============================] - 1s 79ms/step - loss: 129.4005 - predict_loss: 0.0000e+00 - loss_loss: 129.4005\n",
      "Epoch 187/500\n",
      "16/16 [==============================] - 1s 51ms/step - loss: 96.7450 - predict_loss: 0.0000e+00 - loss_loss: 96.7450\n",
      "Epoch 188/500\n",
      "16/16 [==============================] - 1s 49ms/step - loss: 141.1158 - predict_loss: 0.0000e+00 - loss_loss: 141.1158\n",
      "Epoch 189/500\n",
      "16/16 [==============================] - 1s 46ms/step - loss: 104.0506 - predict_loss: 0.0000e+00 - loss_loss: 104.0506\n",
      "Epoch 190/500\n",
      "16/16 [==============================] - 1s 54ms/step - loss: 138.3688 - predict_loss: 0.0000e+00 - loss_loss: 138.3688\n",
      "Epoch 191/500\n",
      "16/16 [==============================] - 2s 108ms/step - loss: 82.6545 - predict_loss: 0.0000e+00 - loss_loss: 82.6545\n",
      "Epoch 192/500\n",
      "16/16 [==============================] - 1s 60ms/step - loss: 120.1854 - predict_loss: 0.0000e+00 - loss_loss: 120.1854\n",
      "Epoch 193/500\n",
      "16/16 [==============================] - 1s 78ms/step - loss: 74.7096 - predict_loss: 0.0000e+00 - loss_loss: 74.7096\n",
      "Epoch 194/500\n",
      "16/16 [==============================] - 1s 49ms/step - loss: 111.6819 - predict_loss: 0.0000e+00 - loss_loss: 111.6819\n",
      "Epoch 195/500\n",
      "16/16 [==============================] - 1s 51ms/step - loss: 77.7693 - predict_loss: 0.0000e+00 - loss_loss: 77.7693\n",
      "Epoch 196/500\n",
      "16/16 [==============================] - 2s 113ms/step - loss: 117.5799 - predict_loss: 0.0000e+00 - loss_loss: 117.5799\n",
      "Epoch 197/500\n",
      "16/16 [==============================] - 2s 104ms/step - loss: 85.9607 - predict_loss: 0.0000e+00 - loss_loss: 85.9607\n",
      "Epoch 198/500\n",
      "16/16 [==============================] - 1s 66ms/step - loss: 134.5857 - predict_loss: 0.0000e+00 - loss_loss: 134.5857\n",
      "Epoch 199/500\n",
      "16/16 [==============================] - 1s 67ms/step - loss: 94.2744 - predict_loss: 0.0000e+00 - loss_loss: 94.2744\n",
      "Epoch 200/500\n",
      "16/16 [==============================] - 1s 88ms/step - loss: 135.0693 - predict_loss: 0.0000e+00 - loss_loss: 135.0693\n",
      "Epoch 201/500\n",
      "16/16 [==============================] - 1s 89ms/step - loss: 77.1807 - predict_loss: 0.0000e+00 - loss_loss: 77.1807\n",
      "Epoch 202/500\n",
      "16/16 [==============================] - 1s 61ms/step - loss: 117.3278 - predict_loss: 0.0000e+00 - loss_loss: 117.3278\n",
      "Epoch 203/500\n",
      "16/16 [==============================] - 1s 92ms/step - loss: 74.2590 - predict_loss: 0.0000e+00 - loss_loss: 74.2590\n",
      "Epoch 204/500\n",
      "16/16 [==============================] - 1s 53ms/step - loss: 107.9564 - predict_loss: 0.0000e+00 - loss_loss: 107.9564\n",
      "Epoch 205/500\n",
      "16/16 [==============================] - 1s 52ms/step - loss: 69.6900 - predict_loss: 0.0000e+00 - loss_loss: 69.6900\n",
      "Epoch 206/500\n",
      "16/16 [==============================] - 1s 57ms/step - loss: 101.4818 - predict_loss: 0.0000e+00 - loss_loss: 101.4818\n",
      "Epoch 207/500\n",
      "16/16 [==============================] - 1s 67ms/step - loss: 74.0167 - predict_loss: 0.0000e+00 - loss_loss: 74.0167\n",
      "Epoch 208/500\n",
      "16/16 [==============================] - 2s 122ms/step - loss: 103.1786 - predict_loss: 0.0000e+00 - loss_loss: 103.1786\n",
      "Epoch 209/500\n",
      "16/16 [==============================] - 1s 70ms/step - loss: 84.0639 - predict_loss: 0.0000e+00 - loss_loss: 84.0639\n",
      "Epoch 210/500\n",
      "16/16 [==============================] - 1s 78ms/step - loss: 90.3963 - predict_loss: 0.0000e+00 - loss_loss: 90.3963\n",
      "Epoch 211/500\n",
      "16/16 [==============================] - 1s 72ms/step - loss: 85.6063 - predict_loss: 0.0000e+00 - loss_loss: 85.6063\n",
      "Epoch 212/500\n",
      "16/16 [==============================] - 1s 63ms/step - loss: 75.5434 - predict_loss: 0.0000e+00 - loss_loss: 75.5434\n",
      "Epoch 213/500\n",
      "16/16 [==============================] - 1s 50ms/step - loss: 80.1959 - predict_loss: 0.0000e+00 - loss_loss: 80.1959\n",
      "Epoch 214/500\n",
      "16/16 [==============================] - 1s 45ms/step - loss: 66.3775 - predict_loss: 0.0000e+00 - loss_loss: 66.3775\n",
      "Epoch 215/500\n",
      "16/16 [==============================] - 1s 57ms/step - loss: 78.9786 - predict_loss: 0.0000e+00 - loss_loss: 78.9786\n",
      "Epoch 216/500\n",
      "16/16 [==============================] - 1s 54ms/step - loss: 62.0707 - predict_loss: 0.0000e+00 - loss_loss: 62.0707\n",
      "Epoch 217/500\n",
      "16/16 [==============================] - 1s 63ms/step - loss: 93.4261 - predict_loss: 0.0000e+00 - loss_loss: 93.4261\n",
      "Epoch 218/500\n",
      "16/16 [==============================] - 1s 93ms/step - loss: 98.0866 - predict_loss: 0.0000e+00 - loss_loss: 98.0866\n",
      "Epoch 219/500\n",
      "16/16 [==============================] - 1s 64ms/step - loss: 120.6451 - predict_loss: 0.0000e+00 - loss_loss: 120.6451\n",
      "Epoch 220/500\n",
      "16/16 [==============================] - 1s 69ms/step - loss: 70.5276 - predict_loss: 0.0000e+00 - loss_loss: 70.5276\n",
      "Epoch 221/500\n",
      "16/16 [==============================] - 1s 87ms/step - loss: 106.8158 - predict_loss: 0.0000e+00 - loss_loss: 106.8158\n",
      "Epoch 222/500\n",
      "16/16 [==============================] - 3s 176ms/step - loss: 70.4536 - predict_loss: 0.0000e+00 - loss_loss: 70.4536\n",
      "Epoch 223/500\n",
      "16/16 [==============================] - 2s 142ms/step - loss: 104.5757 - predict_loss: 0.0000e+00 - loss_loss: 104.5757\n",
      "Epoch 224/500\n",
      "16/16 [==============================] - 1s 89ms/step - loss: 60.1658 - predict_loss: 0.0000e+00 - loss_loss: 60.1658\n",
      "Epoch 225/500\n",
      "16/16 [==============================] - 1s 81ms/step - loss: 94.6287 - predict_loss: 0.0000e+00 - loss_loss: 94.6287\n",
      "Epoch 226/500\n",
      "16/16 [==============================] - 1s 65ms/step - loss: 58.0917 - predict_loss: 0.0000e+00 - loss_loss: 58.0917\n",
      "Epoch 227/500\n",
      "16/16 [==============================] - 1s 84ms/step - loss: 90.1335 - predict_loss: 0.0000e+00 - loss_loss: 90.1335\n",
      "Epoch 228/500\n",
      "16/16 [==============================] - 1s 78ms/step - loss: 54.9536 - predict_loss: 0.0000e+00 - loss_loss: 54.9536\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 229/500\n",
      "16/16 [==============================] - 1s 74ms/step - loss: 85.5459 - predict_loss: 0.0000e+00 - loss_loss: 85.5459\n",
      "Epoch 230/500\n",
      "16/16 [==============================] - 1s 71ms/step - loss: 53.5886 - predict_loss: 0.0000e+00 - loss_loss: 53.5886\n",
      "Epoch 231/500\n",
      "16/16 [==============================] - 1s 70ms/step - loss: 82.5059 - predict_loss: 0.0000e+00 - loss_loss: 82.5059\n",
      "Epoch 232/500\n",
      "16/16 [==============================] - 1s 78ms/step - loss: 52.9602 - predict_loss: 0.0000e+00 - loss_loss: 52.9602\n",
      "Epoch 233/500\n",
      "16/16 [==============================] - 1s 75ms/step - loss: 81.0342 - predict_loss: 0.0000e+00 - loss_loss: 81.0342\n",
      "Epoch 234/500\n",
      "16/16 [==============================] - 2s 104ms/step - loss: 53.2507 - predict_loss: 0.0000e+00 - loss_loss: 53.2507\n",
      "Epoch 235/500\n",
      "16/16 [==============================] - 1s 68ms/step - loss: 84.5080 - predict_loss: 0.0000e+00 - loss_loss: 84.5080\n",
      "Epoch 236/500\n",
      "16/16 [==============================] - 1s 76ms/step - loss: 67.5526 - predict_loss: 0.0000e+00 - loss_loss: 67.5526\n",
      "Epoch 237/500\n",
      "16/16 [==============================] - 2s 121ms/step - loss: 98.6353 - predict_loss: 0.0000e+00 - loss_loss: 98.6353\n",
      "Epoch 238/500\n",
      "16/16 [==============================] - 2s 126ms/step - loss: 53.5796 - predict_loss: 0.0000e+00 - loss_loss: 53.5796\n",
      "Epoch 239/500\n",
      "16/16 [==============================] - 1s 92ms/step - loss: 77.0587 - predict_loss: 0.0000e+00 - loss_loss: 77.0587\n",
      "Epoch 240/500\n",
      "16/16 [==============================] - 1s 83ms/step - loss: 46.3682 - predict_loss: 0.0000e+00 - loss_loss: 46.3682\n",
      "Epoch 241/500\n",
      "16/16 [==============================] - 1s 75ms/step - loss: 67.5446 - predict_loss: 0.0000e+00 - loss_loss: 67.5446\n",
      "Epoch 242/500\n",
      "16/16 [==============================] - 1s 84ms/step - loss: 51.7376 - predict_loss: 0.0000e+00 - loss_loss: 51.7376\n",
      "Epoch 243/500\n",
      "16/16 [==============================] - 1s 75ms/step - loss: 78.6722 - predict_loss: 0.0000e+00 - loss_loss: 78.6722\n",
      "Epoch 244/500\n",
      "16/16 [==============================] - 1s 88ms/step - loss: 76.5291 - predict_loss: 0.0000e+00 - loss_loss: 76.5291\n",
      "Epoch 245/500\n",
      "16/16 [==============================] - 1s 87ms/step - loss: 60.3387 - predict_loss: 0.0000e+00 - loss_loss: 60.3387\n",
      "Epoch 246/500\n",
      "16/16 [==============================] - 1s 78ms/step - loss: 75.5450 - predict_loss: 0.0000e+00 - loss_loss: 75.5450\n",
      "Epoch 247/500\n",
      "16/16 [==============================] - 2s 134ms/step - loss: 46.4092 - predict_loss: 0.0000e+00 - loss_loss: 46.4092\n",
      "Epoch 248/500\n",
      "16/16 [==============================] - 1s 83ms/step - loss: 69.1673 - predict_loss: 0.0000e+00 - loss_loss: 69.1673\n",
      "Epoch 249/500\n",
      "16/16 [==============================] - 2s 111ms/step - loss: 41.6122 - predict_loss: 0.0000e+00 - loss_loss: 41.6122\n",
      "Epoch 250/500\n",
      "16/16 [==============================] - 2s 125ms/step - loss: 66.2195 - predict_loss: 0.0000e+00 - loss_loss: 66.2195\n",
      "Epoch 251/500\n",
      "16/16 [==============================] - 2s 121ms/step - loss: 40.3902 - predict_loss: 0.0000e+00 - loss_loss: 40.3902\n",
      "Epoch 252/500\n",
      "16/16 [==============================] - 2s 102ms/step - loss: 70.4988 - predict_loss: 0.0000e+00 - loss_loss: 70.4988\n",
      "Epoch 253/500\n",
      "16/16 [==============================] - 2s 97ms/step - loss: 51.3273 - predict_loss: 0.0000e+00 - loss_loss: 51.3273\n",
      "Epoch 254/500\n",
      "16/16 [==============================] - 2s 148ms/step - loss: 87.1576 - predict_loss: 0.0000e+00 - loss_loss: 87.1576\n",
      "Epoch 255/500\n",
      "16/16 [==============================] - 2s 137ms/step - loss: 49.7386 - predict_loss: 0.0000e+00 - loss_loss: 49.7386\n",
      "Epoch 256/500\n",
      "16/16 [==============================] - 1s 77ms/step - loss: 78.8104 - predict_loss: 0.0000e+00 - loss_loss: 78.8104\n",
      "Epoch 257/500\n",
      "16/16 [==============================] - 2s 109ms/step - loss: 53.5263 - predict_loss: 0.0000e+00 - loss_loss: 53.5263\n",
      "Epoch 258/500\n",
      "16/16 [==============================] - 2s 104ms/step - loss: 77.6069 - predict_loss: 0.0000e+00 - loss_loss: 77.6069\n",
      "Epoch 259/500\n",
      "16/16 [==============================] - 2s 108ms/step - loss: 44.2509 - predict_loss: 0.0000e+00 - loss_loss: 44.2509\n",
      "Epoch 260/500\n",
      "16/16 [==============================] - 1s 69ms/step - loss: 63.3288 - predict_loss: 0.0000e+00 - loss_loss: 63.3288\n",
      "Epoch 261/500\n",
      "16/16 [==============================] - 1s 71ms/step - loss: 39.9032 - predict_loss: 0.0000e+00 - loss_loss: 39.9032\n",
      "Epoch 262/500\n",
      "16/16 [==============================] - 1s 79ms/step - loss: 56.5414 - predict_loss: 0.0000e+00 - loss_loss: 56.5414\n",
      "Epoch 263/500\n",
      "16/16 [==============================] - 2s 112ms/step - loss: 37.5671 - predict_loss: 0.0000e+00 - loss_loss: 37.5671\n",
      "Epoch 264/500\n",
      "16/16 [==============================] - 2s 102ms/step - loss: 53.3102 - predict_loss: 0.0000e+00 - loss_loss: 53.3102\n",
      "Epoch 265/500\n",
      "16/16 [==============================] - 1s 72ms/step - loss: 38.2990 - predict_loss: 0.0000e+00 - loss_loss: 38.2990\n",
      "Epoch 266/500\n",
      "16/16 [==============================] - 1s 88ms/step - loss: 55.1926 - predict_loss: 0.0000e+00 - loss_loss: 55.1926\n",
      "Epoch 267/500\n",
      "16/16 [==============================] - 2s 97ms/step - loss: 42.2663 - predict_loss: 0.0000e+00 - loss_loss: 42.2663\n",
      "Epoch 268/500\n",
      "16/16 [==============================] - 1s 77ms/step - loss: 65.3066 - predict_loss: 0.0000e+00 - loss_loss: 65.3066\n",
      "Epoch 269/500\n",
      "16/16 [==============================] - 2s 134ms/step - loss: 54.7865 - predict_loss: 0.0000e+00 - loss_loss: 54.7865\n",
      "Epoch 270/500\n",
      "16/16 [==============================] - 2s 124ms/step - loss: 88.6193 - predict_loss: 0.0000e+00 - loss_loss: 88.6193\n",
      "Epoch 271/500\n",
      "16/16 [==============================] - 1s 78ms/step - loss: 37.4445 - predict_loss: 0.0000e+00 - loss_loss: 37.4445\n",
      "Epoch 272/500\n",
      "16/16 [==============================] - 1s 63ms/step - loss: 64.9104 - predict_loss: 0.0000e+00 - loss_loss: 64.9104\n",
      "Epoch 273/500\n",
      "16/16 [==============================] - 2s 110ms/step - loss: 37.6024 - predict_loss: 0.0000e+00 - loss_loss: 37.6024\n",
      "Epoch 274/500\n",
      "16/16 [==============================] - 1s 52ms/step - loss: 54.8247 - predict_loss: 0.0000e+00 - loss_loss: 54.8247\n",
      "Epoch 275/500\n",
      "16/16 [==============================] - 1s 48ms/step - loss: 38.2236 - predict_loss: 0.0000e+00 - loss_loss: 38.2236\n",
      "Epoch 276/500\n",
      "16/16 [==============================] - 1s 54ms/step - loss: 51.7792 - predict_loss: 0.0000e+00 - loss_loss: 51.7792\n",
      "Epoch 277/500\n",
      "16/16 [==============================] - 1s 57ms/step - loss: 47.5010 - predict_loss: 0.0000e+00 - loss_loss: 47.5010\n",
      "Epoch 278/500\n",
      "16/16 [==============================] - 1s 66ms/step - loss: 51.0220 - predict_loss: 0.0000e+00 - loss_loss: 51.0220\n",
      "Epoch 279/500\n",
      "16/16 [==============================] - 1s 68ms/step - loss: 51.5485 - predict_loss: 0.0000e+00 - loss_loss: 51.5485\n",
      "Epoch 280/500\n",
      "16/16 [==============================] - 1s 66ms/step - loss: 40.0079 - predict_loss: 0.0000e+00 - loss_loss: 40.0079\n",
      "Epoch 281/500\n",
      "16/16 [==============================] - 1s 77ms/step - loss: 45.6712 - predict_loss: 0.0000e+00 - loss_loss: 45.6712\n",
      "Epoch 282/500\n",
      "16/16 [==============================] - 2s 94ms/step - loss: 36.9544 - predict_loss: 0.0000e+00 - loss_loss: 36.9544\n",
      "Epoch 283/500\n",
      "16/16 [==============================] - 1s 66ms/step - loss: 44.2265 - predict_loss: 0.0000e+00 - loss_loss: 44.2265\n",
      "Epoch 284/500\n",
      "16/16 [==============================] - 1s 57ms/step - loss: 35.0001 - predict_loss: 0.0000e+00 - loss_loss: 35.0001\n",
      "Epoch 285/500\n",
      "16/16 [==============================] - 1s 65ms/step - loss: 48.5690 - predict_loss: 0.0000e+00 - loss_loss: 48.5690\n",
      "Epoch 286/500\n",
      "16/16 [==============================] - 1s 64ms/step - loss: 45.8219 - predict_loss: 0.0000e+00 - loss_loss: 45.8219\n",
      "Epoch 287/500\n",
      "16/16 [==============================] - 1s 89ms/step - loss: 70.5115 - predict_loss: 0.0000e+00 - loss_loss: 70.5115\n",
      "Epoch 288/500\n",
      "16/16 [==============================] - 1s 85ms/step - loss: 40.3902 - predict_loss: 0.0000e+00 - loss_loss: 40.3902\n",
      "Epoch 289/500\n",
      "16/16 [==============================] - 1s 54ms/step - loss: 62.0490 - predict_loss: 0.0000e+00 - loss_loss: 62.0490\n",
      "Epoch 290/500\n",
      "16/16 [==============================] - 1s 47ms/step - loss: 36.3566 - predict_loss: 0.0000e+00 - loss_loss: 36.3566\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 291/500\n",
      "16/16 [==============================] - 1s 46ms/step - loss: 60.9603 - predict_loss: 0.0000e+00 - loss_loss: 60.9603\n",
      "Epoch 292/500\n",
      "16/16 [==============================] - 1s 67ms/step - loss: 33.9682 - predict_loss: 0.0000e+00 - loss_loss: 33.9682\n",
      "Epoch 293/500\n",
      "16/16 [==============================] - 1s 66ms/step - loss: 53.8478 - predict_loss: 0.0000e+00 - loss_loss: 53.8478\n",
      "Epoch 294/500\n",
      "16/16 [==============================] - 1s 57ms/step - loss: 30.4365 - predict_loss: 0.0000e+00 - loss_loss: 30.4365\n",
      "Epoch 295/500\n",
      "16/16 [==============================] - 1s 57ms/step - loss: 46.5238 - predict_loss: 0.0000e+00 - loss_loss: 46.5238\n",
      "Epoch 296/500\n",
      "16/16 [==============================] - 1s 53ms/step - loss: 28.0604 - predict_loss: 0.0000e+00 - loss_loss: 28.0604\n",
      "Epoch 297/500\n",
      "16/16 [==============================] - 1s 65ms/step - loss: 43.2675 - predict_loss: 0.0000e+00 - loss_loss: 43.2675\n",
      "Epoch 298/500\n",
      "16/16 [==============================] - 1s 93ms/step - loss: 27.2891 - predict_loss: 0.0000e+00 - loss_loss: 27.2891\n",
      "Epoch 299/500\n",
      "16/16 [==============================] - 1s 45ms/step - loss: 41.5398 - predict_loss: 0.0000e+00 - loss_loss: 41.5398\n",
      "Epoch 300/500\n",
      "16/16 [==============================] - 1s 45ms/step - loss: 26.0034 - predict_loss: 0.0000e+00 - loss_loss: 26.0034\n",
      "Epoch 301/500\n",
      "16/16 [==============================] - 1s 44ms/step - loss: 41.3433 - predict_loss: 0.0000e+00 - loss_loss: 41.3433\n",
      "Epoch 302/500\n",
      "16/16 [==============================] - 1s 59ms/step - loss: 29.3578 - predict_loss: 0.0000e+00 - loss_loss: 29.3578\n",
      "Epoch 303/500\n",
      "16/16 [==============================] - 1s 71ms/step - loss: 52.9644 - predict_loss: 0.0000e+00 - loss_loss: 52.9644\n",
      "Epoch 304/500\n",
      "16/16 [==============================] - 1s 60ms/step - loss: 52.9308 - predict_loss: 0.0000e+00 - loss_loss: 52.9308\n",
      "Epoch 305/500\n",
      "16/16 [==============================] - 1s 60ms/step - loss: 71.6693 - predict_loss: 0.0000e+00 - loss_loss: 71.6693\n",
      "Epoch 306/500\n",
      "16/16 [==============================] - 1s 60ms/step - loss: 34.6012 - predict_loss: 0.0000e+00 - loss_loss: 34.6012\n",
      "Epoch 307/500\n",
      "16/16 [==============================] - 1s 59ms/step - loss: 45.3257 - predict_loss: 0.0000e+00 - loss_loss: 45.3257\n",
      "Epoch 308/500\n",
      "16/16 [==============================] - 1s 69ms/step - loss: 33.3093 - predict_loss: 0.0000e+00 - loss_loss: 33.3093\n",
      "Epoch 309/500\n",
      "16/16 [==============================] - 1s 54ms/step - loss: 41.6680 - predict_loss: 0.0000e+00 - loss_loss: 41.6680\n",
      "Epoch 310/500\n",
      "16/16 [==============================] - 1s 62ms/step - loss: 37.7916 - predict_loss: 0.0000e+00 - loss_loss: 37.7916\n",
      "Epoch 311/500\n",
      "16/16 [==============================] - 1s 59ms/step - loss: 37.0342 - predict_loss: 0.0000e+00 - loss_loss: 37.0342\n",
      "Epoch 312/500\n",
      "16/16 [==============================] - 1s 65ms/step - loss: 36.9133 - predict_loss: 0.0000e+00 - loss_loss: 36.9133\n",
      "Epoch 313/500\n",
      "16/16 [==============================] - 1s 60ms/step - loss: 31.7368 - predict_loss: 0.0000e+00 - loss_loss: 31.7368\n",
      "Epoch 314/500\n",
      "16/16 [==============================] - 1s 58ms/step - loss: 34.3331 - predict_loss: 0.0000e+00 - loss_loss: 34.3331\n",
      "Epoch 315/500\n",
      "16/16 [==============================] - 1s 50ms/step - loss: 27.6886 - predict_loss: 0.0000e+00 - loss_loss: 27.6886\n",
      "Epoch 316/500\n",
      "16/16 [==============================] - 1s 52ms/step - loss: 32.7732 - predict_loss: 0.0000e+00 - loss_loss: 32.7732\n",
      "Epoch 317/500\n",
      "16/16 [==============================] - 1s 65ms/step - loss: 25.0601 - predict_loss: 0.0000e+00 - loss_loss: 25.0601\n",
      "Epoch 318/500\n",
      "16/16 [==============================] - 1s 90ms/step - loss: 31.0431 - predict_loss: 0.0000e+00 - loss_loss: 31.0431\n",
      "Epoch 319/500\n",
      "16/16 [==============================] - 1s 72ms/step - loss: 24.9518 - predict_loss: 0.0000e+00 - loss_loss: 24.9518\n",
      "Epoch 320/500\n",
      "16/16 [==============================] - 1s 51ms/step - loss: 30.9235 - predict_loss: 0.0000e+00 - loss_loss: 30.9235\n",
      "Epoch 321/500\n",
      "16/16 [==============================] - 1s 59ms/step - loss: 34.0660 - predict_loss: 0.0000e+00 - loss_loss: 34.0660\n",
      "Epoch 322/500\n",
      "16/16 [==============================] - 1s 66ms/step - loss: 40.5016 - predict_loss: 0.0000e+00 - loss_loss: 40.5016\n",
      "Epoch 323/500\n",
      "16/16 [==============================] - 2s 104ms/step - loss: 58.4280 - predict_loss: 0.0000e+00 - loss_loss: 58.4280\n",
      "Epoch 324/500\n",
      "16/16 [==============================] - 1s 61ms/step - loss: 41.7736 - predict_loss: 0.0000e+00 - loss_loss: 41.7736\n",
      "Epoch 325/500\n",
      "16/16 [==============================] - 1s 62ms/step - loss: 55.1760 - predict_loss: 0.0000e+00 - loss_loss: 55.1760\n",
      "Epoch 326/500\n",
      "16/16 [==============================] - 1s 54ms/step - loss: 29.0083 - predict_loss: 0.0000e+00 - loss_loss: 29.0083\n",
      "Epoch 327/500\n",
      "16/16 [==============================] - 1s 81ms/step - loss: 41.5076 - predict_loss: 0.0000e+00 - loss_loss: 41.5076\n",
      "Epoch 328/500\n",
      "16/16 [==============================] - 1s 75ms/step - loss: 29.7950 - predict_loss: 0.0000e+00 - loss_loss: 29.7950\n",
      "Epoch 329/500\n",
      "16/16 [==============================] - 1s 48ms/step - loss: 42.3051 - predict_loss: 0.0000e+00 - loss_loss: 42.3051\n",
      "Epoch 330/500\n",
      "16/16 [==============================] - 1s 52ms/step - loss: 27.4819 - predict_loss: 0.0000e+00 - loss_loss: 27.4819\n",
      "Epoch 331/500\n",
      "16/16 [==============================] - 1s 54ms/step - loss: 41.2234 - predict_loss: 0.0000e+00 - loss_loss: 41.2234\n",
      "Epoch 332/500\n",
      "16/16 [==============================] - 1s 63ms/step - loss: 27.1643 - predict_loss: 0.0000e+00 - loss_loss: 27.1643\n",
      "Epoch 333/500\n",
      "16/16 [==============================] - 1s 45ms/step - loss: 38.0983 - predict_loss: 0.0000e+00 - loss_loss: 38.0983\n",
      "Epoch 334/500\n",
      "16/16 [==============================] - 1s 45ms/step - loss: 23.0224 - predict_loss: 0.0000e+00 - loss_loss: 23.0224\n",
      "Epoch 335/500\n",
      "16/16 [==============================] - 1s 46ms/step - loss: 32.7919 - predict_loss: 0.0000e+00 - loss_loss: 32.7919\n",
      "Epoch 336/500\n",
      "16/16 [==============================] - 1s 43ms/step - loss: 25.5509 - predict_loss: 0.0000e+00 - loss_loss: 25.5509\n",
      "Epoch 337/500\n",
      "16/16 [==============================] - 1s 43ms/step - loss: 32.0394 - predict_loss: 0.0000e+00 - loss_loss: 32.0394\n",
      "Epoch 338/500\n",
      "16/16 [==============================] - 1s 59ms/step - loss: 29.5715 - predict_loss: 0.0000e+00 - loss_loss: 29.5715\n",
      "Epoch 339/500\n",
      "16/16 [==============================] - 1s 69ms/step - loss: 31.0683 - predict_loss: 0.0000e+00 - loss_loss: 31.0683\n",
      "Epoch 340/500\n",
      "16/16 [==============================] - 1s 52ms/step - loss: 35.7870 - predict_loss: 0.0000e+00 - loss_loss: 35.7870\n",
      "Epoch 341/500\n",
      "16/16 [==============================] - 1s 48ms/step - loss: 26.9665 - predict_loss: 0.0000e+00 - loss_loss: 26.9665\n",
      "Epoch 342/500\n",
      "16/16 [==============================] - 1s 54ms/step - loss: 34.6804 - predict_loss: 0.0000e+00 - loss_loss: 34.6804\n",
      "Epoch 343/500\n",
      "16/16 [==============================] - 1s 55ms/step - loss: 25.9805 - predict_loss: 0.0000e+00 - loss_loss: 25.9805\n",
      "Epoch 344/500\n",
      "16/16 [==============================] - 1s 65ms/step - loss: 38.4457 - predict_loss: 0.0000e+00 - loss_loss: 38.4457\n",
      "Epoch 345/500\n",
      "16/16 [==============================] - 1s 79ms/step - loss: 29.5975 - predict_loss: 0.0000e+00 - loss_loss: 29.5975\n",
      "Epoch 346/500\n",
      "16/16 [==============================] - 1s 59ms/step - loss: 40.0384 - predict_loss: 0.0000e+00 - loss_loss: 40.0384\n",
      "Epoch 347/500\n",
      "16/16 [==============================] - 1s 61ms/step - loss: 24.0971 - predict_loss: 0.0000e+00 - loss_loss: 24.0971\n",
      "Epoch 348/500\n",
      "16/16 [==============================] - 1s 72ms/step - loss: 33.0350 - predict_loss: 0.0000e+00 - loss_loss: 33.0350\n",
      "Epoch 349/500\n",
      "16/16 [==============================] - 2s 112ms/step - loss: 21.6780 - predict_loss: 0.0000e+00 - loss_loss: 21.6780\n",
      "Epoch 350/500\n",
      "16/16 [==============================] - 1s 50ms/step - loss: 35.0770 - predict_loss: 0.0000e+00 - loss_loss: 35.0770\n",
      "Epoch 351/500\n",
      "16/16 [==============================] - 1s 49ms/step - loss: 28.7174 - predict_loss: 0.0000e+00 - loss_loss: 28.7174\n",
      "Epoch 352/500\n",
      "16/16 [==============================] - 1s 54ms/step - loss: 40.7235 - predict_loss: 0.0000e+00 - loss_loss: 40.7235\n",
      "Epoch 353/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 1s 54ms/step - loss: 20.6203 - predict_loss: 0.0000e+00 - loss_loss: 20.6203\n",
      "Epoch 354/500\n",
      "16/16 [==============================] - 1s 60ms/step - loss: 29.8053 - predict_loss: 0.0000e+00 - loss_loss: 29.8053\n",
      "Epoch 355/500\n",
      "16/16 [==============================] - 1s 57ms/step - loss: 23.2810 - predict_loss: 0.0000e+00 - loss_loss: 23.2810\n",
      "Epoch 356/500\n",
      "16/16 [==============================] - 1s 58ms/step - loss: 29.5911 - predict_loss: 0.0000e+00 - loss_loss: 29.5911\n",
      "Epoch 357/500\n",
      "16/16 [==============================] - 1s 57ms/step - loss: 24.4703 - predict_loss: 0.0000e+00 - loss_loss: 24.4703\n",
      "Epoch 358/500\n",
      "16/16 [==============================] - 1s 52ms/step - loss: 30.4908 - predict_loss: 0.0000e+00 - loss_loss: 30.4908\n",
      "Epoch 359/500\n",
      "16/16 [==============================] - 1s 87ms/step - loss: 32.0381 - predict_loss: 0.0000e+00 - loss_loss: 32.0381\n",
      "Epoch 360/500\n",
      "16/16 [==============================] - 1s 70ms/step - loss: 25.7007 - predict_loss: 0.0000e+00 - loss_loss: 25.7007\n",
      "Epoch 361/500\n",
      "16/16 [==============================] - 1s 69ms/step - loss: 31.2027 - predict_loss: 0.0000e+00 - loss_loss: 31.2027\n",
      "Epoch 362/500\n",
      "16/16 [==============================] - 1s 52ms/step - loss: 22.7049 - predict_loss: 0.0000e+00 - loss_loss: 22.7049\n",
      "Epoch 363/500\n",
      "16/16 [==============================] - 1s 69ms/step - loss: 29.5610 - predict_loss: 0.0000e+00 - loss_loss: 29.5610\n",
      "Epoch 364/500\n",
      "16/16 [==============================] - 1s 64ms/step - loss: 24.7434 - predict_loss: 0.0000e+00 - loss_loss: 24.7434\n",
      "Epoch 365/500\n",
      "16/16 [==============================] - 1s 44ms/step - loss: 35.9600 - predict_loss: 0.0000e+00 - loss_loss: 35.9600\n",
      "Epoch 366/500\n",
      "16/16 [==============================] - 1s 50ms/step - loss: 25.9484 - predict_loss: 0.0000e+00 - loss_loss: 25.9484\n",
      "Epoch 367/500\n",
      "16/16 [==============================] - 1s 46ms/step - loss: 33.5190 - predict_loss: 0.0000e+00 - loss_loss: 33.5190\n",
      "Epoch 368/500\n",
      "16/16 [==============================] - 1s 43ms/step - loss: 19.8429 - predict_loss: 0.0000e+00 - loss_loss: 19.8429\n",
      "Epoch 369/500\n",
      "16/16 [==============================] - 1s 59ms/step - loss: 28.5855 - predict_loss: 0.0000e+00 - loss_loss: 28.5855\n",
      "Epoch 370/500\n",
      "16/16 [==============================] - 1s 86ms/step - loss: 21.7270 - predict_loss: 0.0000e+00 - loss_loss: 21.7270\n",
      "Epoch 371/500\n",
      "16/16 [==============================] - 1s 56ms/step - loss: 34.5143 - predict_loss: 0.0000e+00 - loss_loss: 34.5143\n",
      "Epoch 372/500\n",
      "16/16 [==============================] - 1s 53ms/step - loss: 23.2616 - predict_loss: 0.0000e+00 - loss_loss: 23.2616\n",
      "Epoch 373/500\n",
      "16/16 [==============================] - 1s 55ms/step - loss: 31.7573 - predict_loss: 0.0000e+00 - loss_loss: 31.7573\n",
      "Epoch 374/500\n",
      "16/16 [==============================] - 2s 118ms/step - loss: 17.5470 - predict_loss: 0.0000e+00 - loss_loss: 17.5470\n",
      "Epoch 375/500\n",
      "16/16 [==============================] - 1s 58ms/step - loss: 24.5199 - predict_loss: 0.0000e+00 - loss_loss: 24.5199\n",
      "Epoch 376/500\n",
      "16/16 [==============================] - 1s 61ms/step - loss: 21.6663 - predict_loss: 0.0000e+00 - loss_loss: 21.6663\n",
      "Epoch 377/500\n",
      "16/16 [==============================] - 1s 54ms/step - loss: 27.5804 - predict_loss: 0.0000e+00 - loss_loss: 27.5804\n",
      "Epoch 378/500\n",
      "16/16 [==============================] - 1s 48ms/step - loss: 26.1673 - predict_loss: 0.0000e+00 - loss_loss: 26.1673\n",
      "Epoch 379/500\n",
      "16/16 [==============================] - 1s 64ms/step - loss: 25.9620 - predict_loss: 0.0000e+00 - loss_loss: 25.9620\n",
      "Epoch 380/500\n",
      "16/16 [==============================] - 1s 50ms/step - loss: 27.9794 - predict_loss: 0.0000e+00 - loss_loss: 27.9794\n",
      "Epoch 381/500\n",
      "16/16 [==============================] - 1s 52ms/step - loss: 21.2895 - predict_loss: 0.0000e+00 - loss_loss: 21.2895\n",
      "Epoch 382/500\n",
      "16/16 [==============================] - 1s 58ms/step - loss: 26.4887 - predict_loss: 0.0000e+00 - loss_loss: 26.4887\n",
      "Epoch 383/500\n",
      "16/16 [==============================] - 1s 51ms/step - loss: 20.5634 - predict_loss: 0.0000e+00 - loss_loss: 20.5634\n",
      "Epoch 384/500\n",
      "16/16 [==============================] - 1s 50ms/step - loss: 27.3987 - predict_loss: 0.0000e+00 - loss_loss: 27.3987\n",
      "Epoch 385/500\n",
      "16/16 [==============================] - 1s 76ms/step - loss: 24.6994 - predict_loss: 0.0000e+00 - loss_loss: 24.6994\n",
      "Epoch 386/500\n",
      "16/16 [==============================] - 1s 57ms/step - loss: 33.9300 - predict_loss: 0.0000e+00 - loss_loss: 33.9300\n",
      "Epoch 387/500\n",
      "16/16 [==============================] - 1s 56ms/step - loss: 21.9138 - predict_loss: 0.0000e+00 - loss_loss: 21.9138\n",
      "Epoch 388/500\n",
      "16/16 [==============================] - 1s 54ms/step - loss: 27.6706 - predict_loss: 0.0000e+00 - loss_loss: 27.6706\n",
      "Epoch 389/500\n",
      "16/16 [==============================] - 1s 58ms/step - loss: 20.7231 - predict_loss: 0.0000e+00 - loss_loss: 20.7231\n",
      "Epoch 390/500\n",
      "16/16 [==============================] - 2s 117ms/step - loss: 28.8360 - predict_loss: 0.0000e+00 - loss_loss: 28.8360\n",
      "Epoch 391/500\n",
      "16/16 [==============================] - 1s 61ms/step - loss: 20.3313 - predict_loss: 0.0000e+00 - loss_loss: 20.3313\n",
      "Epoch 392/500\n",
      "16/16 [==============================] - 1s 51ms/step - loss: 30.9003 - predict_loss: 0.0000e+00 - loss_loss: 30.9003\n",
      "Epoch 393/500\n",
      "16/16 [==============================] - 3s 164ms/step - loss: 19.5873 - predict_loss: 0.0000e+00 - loss_loss: 19.5873\n",
      "Epoch 394/500\n",
      "16/16 [==============================] - 2s 140ms/step - loss: 24.8892 - predict_loss: 0.0000e+00 - loss_loss: 24.8892\n",
      "Epoch 395/500\n",
      "16/16 [==============================] - 1s 93ms/step - loss: 15.0434 - predict_loss: 0.0000e+00 - loss_loss: 15.0434\n",
      "Epoch 396/500\n",
      "16/16 [==============================] - 2s 106ms/step - loss: 20.3533 - predict_loss: 0.0000e+00 - loss_loss: 20.3533\n",
      "Epoch 397/500\n",
      "16/16 [==============================] - 1s 93ms/step - loss: 18.5890 - predict_loss: 0.0000e+00 - loss_loss: 18.5890\n",
      "Epoch 398/500\n",
      "16/16 [==============================] - 1s 83ms/step - loss: 25.3671 - predict_loss: 0.0000e+00 - loss_loss: 25.3671\n",
      "Epoch 399/500\n",
      "16/16 [==============================] - 3s 215ms/step - loss: 26.6707 - predict_loss: 0.0000e+00 - loss_loss: 26.6707\n",
      "Epoch 400/500\n",
      "16/16 [==============================] - 1s 78ms/step - loss: 23.7600 - predict_loss: 0.0000e+00 - loss_loss: 23.7600\n",
      "Epoch 401/500\n",
      "16/16 [==============================] - 2s 111ms/step - loss: 24.7260 - predict_loss: 0.0000e+00 - loss_loss: 24.7260\n",
      "Epoch 402/500\n",
      "16/16 [==============================] - 1s 86ms/step - loss: 18.0032 - predict_loss: 0.0000e+00 - loss_loss: 18.0032\n",
      "Epoch 403/500\n",
      "16/16 [==============================] - 1s 69ms/step - loss: 22.4664 - predict_loss: 0.0000e+00 - loss_loss: 22.4664\n",
      "Epoch 404/500\n",
      "16/16 [==============================] - 1s 77ms/step - loss: 18.2439 - predict_loss: 0.0000e+00 - loss_loss: 18.2439\n",
      "Epoch 405/500\n",
      "16/16 [==============================] - 1s 66ms/step - loss: 23.4064 - predict_loss: 0.0000e+00 - loss_loss: 23.4064\n",
      "Epoch 406/500\n",
      "16/16 [==============================] - 1s 75ms/step - loss: 21.5112 - predict_loss: 0.0000e+00 - loss_loss: 21.5112\n",
      "Epoch 407/500\n",
      "16/16 [==============================] - 1s 63ms/step - loss: 31.0195 - predict_loss: 0.0000e+00 - loss_loss: 31.0195\n",
      "Epoch 408/500\n",
      "16/16 [==============================] - 1s 62ms/step - loss: 22.3256 - predict_loss: 0.0000e+00 - loss_loss: 22.3256\n",
      "Epoch 409/500\n",
      "16/16 [==============================] - 1s 77ms/step - loss: 27.2432 - predict_loss: 0.0000e+00 - loss_loss: 27.2432\n",
      "Epoch 410/500\n",
      "16/16 [==============================] - 1s 71ms/step - loss: 17.4773 - predict_loss: 0.0000e+00 - loss_loss: 17.4773\n",
      "Epoch 411/500\n",
      "16/16 [==============================] - 1s 71ms/step - loss: 23.8988 - predict_loss: 0.0000e+00 - loss_loss: 23.8988\n",
      "Epoch 412/500\n",
      "16/16 [==============================] - 2s 100ms/step - loss: 16.3717 - predict_loss: 0.0000e+00 - loss_loss: 16.3717\n",
      "Epoch 413/500\n",
      "16/16 [==============================] - 2s 107ms/step - loss: 24.9768 - predict_loss: 0.0000e+00 - loss_loss: 24.9768\n",
      "Epoch 414/500\n",
      "16/16 [==============================] - 1s 58ms/step - loss: 20.1717 - predict_loss: 0.0000e+00 - loss_loss: 20.1717\n",
      "Epoch 415/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 1s 65ms/step - loss: 25.2978 - predict_loss: 0.0000e+00 - loss_loss: 25.2978\n",
      "Epoch 416/500\n",
      "16/16 [==============================] - 2s 105ms/step - loss: 12.8163 - predict_loss: 0.0000e+00 - loss_loss: 12.8163\n",
      "Epoch 417/500\n",
      "16/16 [==============================] - 3s 164ms/step - loss: 16.9349 - predict_loss: 0.0000e+00 - loss_loss: 16.9349\n",
      "Epoch 418/500\n",
      "16/16 [==============================] - 2s 111ms/step - loss: 15.5358 - predict_loss: 0.0000e+00 - loss_loss: 15.5358\n",
      "Epoch 419/500\n",
      "16/16 [==============================] - 1s 64ms/step - loss: 21.8393 - predict_loss: 0.0000e+00 - loss_loss: 21.8393\n",
      "Epoch 420/500\n",
      "16/16 [==============================] - 2s 109ms/step - loss: 23.9980 - predict_loss: 0.0000e+00 - loss_loss: 23.9980\n",
      "Epoch 421/500\n",
      "16/16 [==============================] - 1s 69ms/step - loss: 24.2692 - predict_loss: 0.0000e+00 - loss_loss: 24.2692\n",
      "Epoch 422/500\n",
      "16/16 [==============================] - 1s 88ms/step - loss: 23.3812 - predict_loss: 0.0000e+00 - loss_loss: 23.3812\n",
      "Epoch 423/500\n",
      "16/16 [==============================] - 2s 118ms/step - loss: 16.4590 - predict_loss: 0.0000e+00 - loss_loss: 16.4590\n",
      "Epoch 424/500\n",
      "16/16 [==============================] - 3s 184ms/step - loss: 19.6692 - predict_loss: 0.0000e+00 - loss_loss: 19.6692\n",
      "Epoch 425/500\n",
      "16/16 [==============================] - 2s 108ms/step - loss: 16.2055 - predict_loss: 0.0000e+00 - loss_loss: 16.2055\n",
      "Epoch 426/500\n",
      "16/16 [==============================] - 2s 111ms/step - loss: 20.2206 - predict_loss: 0.0000e+00 - loss_loss: 20.2206\n",
      "Epoch 427/500\n",
      "16/16 [==============================] - 2s 123ms/step - loss: 18.0142 - predict_loss: 0.0000e+00 - loss_loss: 18.0142\n",
      "Epoch 428/500\n",
      "16/16 [==============================] - 2s 127ms/step - loss: 26.7505 - predict_loss: 0.0000e+00 - loss_loss: 26.7505\n",
      "Epoch 429/500\n",
      "16/16 [==============================] - 1s 84ms/step - loss: 24.0774 - predict_loss: 0.0000e+00 - loss_loss: 24.0774\n",
      "Epoch 430/500\n",
      "16/16 [==============================] - 1s 93ms/step - loss: 29.4011 - predict_loss: 0.0000e+00 - loss_loss: 29.4011\n",
      "Epoch 431/500\n",
      "16/16 [==============================] - 2s 109ms/step - loss: 17.0782 - predict_loss: 0.0000e+00 - loss_loss: 17.0782\n",
      "Epoch 432/500\n",
      "16/16 [==============================] - 1s 91ms/step - loss: 21.8250 - predict_loss: 0.0000e+00 - loss_loss: 21.8250\n",
      "Epoch 433/500\n",
      "16/16 [==============================] - 1s 93ms/step - loss: 13.8217 - predict_loss: 0.0000e+00 - loss_loss: 13.8217\n",
      "Epoch 434/500\n",
      "16/16 [==============================] - 2s 116ms/step - loss: 18.7269 - predict_loss: 0.0000e+00 - loss_loss: 18.7269\n",
      "Epoch 435/500\n",
      "16/16 [==============================] - 2s 95ms/step - loss: 17.7875 - predict_loss: 0.0000e+00 - loss_loss: 17.7875\n",
      "Epoch 436/500\n",
      "16/16 [==============================] - 1s 88ms/step - loss: 23.9066 - predict_loss: 0.0000e+00 - loss_loss: 23.9066\n",
      "Epoch 437/500\n",
      "16/16 [==============================] - 2s 97ms/step - loss: 12.9038 - predict_loss: 0.0000e+00 - loss_loss: 12.9038\n",
      "Epoch 438/500\n",
      "16/16 [==============================] - 1s 62ms/step - loss: 16.6130 - predict_loss: 0.0000e+00 - loss_loss: 16.6130\n",
      "Epoch 439/500\n",
      "16/16 [==============================] - 1s 52ms/step - loss: 13.1115 - predict_loss: 0.0000e+00 - loss_loss: 13.1115\n",
      "Epoch 440/500\n",
      "16/16 [==============================] - 1s 71ms/step - loss: 16.9595 - predict_loss: 0.0000e+00 - loss_loss: 16.9595\n",
      "Epoch 441/500\n",
      "16/16 [==============================] - 1s 66ms/step - loss: 17.2222 - predict_loss: 0.0000e+00 - loss_loss: 17.2222\n",
      "Epoch 442/500\n",
      "16/16 [==============================] - 2s 118ms/step - loss: 23.5450 - predict_loss: 0.0000e+00 - loss_loss: 23.5450\n",
      "Epoch 443/500\n",
      "16/16 [==============================] - 1s 73ms/step - loss: 25.2652 - predict_loss: 0.0000e+00 - loss_loss: 25.2652\n",
      "Epoch 444/500\n",
      "16/16 [==============================] - 1s 74ms/step - loss: 18.0655 - predict_loss: 0.0000e+00 - loss_loss: 18.0655\n",
      "Epoch 445/500\n",
      "16/16 [==============================] - 1s 74ms/step - loss: 18.1506 - predict_loss: 0.0000e+00 - loss_loss: 18.1506\n",
      "Epoch 446/500\n",
      "16/16 [==============================] - 2s 111ms/step - loss: 13.5360 - predict_loss: 0.0000e+00 - loss_loss: 13.5360\n",
      "Epoch 447/500\n",
      "16/16 [==============================] - 1s 66ms/step - loss: 16.5015 - predict_loss: 0.0000e+00 - loss_loss: 16.5015\n",
      "Epoch 448/500\n",
      "16/16 [==============================] - 1s 76ms/step - loss: 14.7985 - predict_loss: 0.0000e+00 - loss_loss: 14.7985\n",
      "Epoch 449/500\n",
      "16/16 [==============================] - 2s 95ms/step - loss: 18.8103 - predict_loss: 0.0000e+00 - loss_loss: 18.8103\n",
      "Epoch 450/500\n",
      "16/16 [==============================] - 2s 122ms/step - loss: 19.1095 - predict_loss: 0.0000e+00 - loss_loss: 19.1095\n",
      "Epoch 451/500\n",
      "16/16 [==============================] - 1s 67ms/step - loss: 30.0307 - predict_loss: 0.0000e+00 - loss_loss: 30.0307\n",
      "Epoch 452/500\n",
      "16/16 [==============================] - 1s 64ms/step - loss: 21.0572 - predict_loss: 0.0000e+00 - loss_loss: 21.0572\n",
      "Epoch 453/500\n",
      "16/16 [==============================] - 1s 60ms/step - loss: 25.0753 - predict_loss: 0.0000e+00 - loss_loss: 25.0753\n",
      "Epoch 454/500\n",
      "16/16 [==============================] - 1s 50ms/step - loss: 12.7392 - predict_loss: 0.0000e+00 - loss_loss: 12.7392\n",
      "Epoch 455/500\n",
      "16/16 [==============================] - 1s 70ms/step - loss: 16.4065 - predict_loss: 0.0000e+00 - loss_loss: 16.4065\n",
      "Epoch 456/500\n",
      "16/16 [==============================] - 1s 82ms/step - loss: 11.5476 - predict_loss: 0.0000e+00 - loss_loss: 11.5476\n",
      "Epoch 457/500\n",
      "16/16 [==============================] - 2s 96ms/step - loss: 16.6568 - predict_loss: 0.0000e+00 - loss_loss: 16.6568\n",
      "Epoch 458/500\n",
      "16/16 [==============================] - 2s 111ms/step - loss: 16.6861 - predict_loss: 0.0000e+00 - loss_loss: 16.6861\n",
      "Epoch 459/500\n",
      "16/16 [==============================] - 1s 81ms/step - loss: 21.5067 - predict_loss: 0.0000e+00 - loss_loss: 21.5067\n",
      "Epoch 460/500\n",
      "16/16 [==============================] - 1s 77ms/step - loss: 12.3846 - predict_loss: 0.0000e+00 - loss_loss: 12.3846\n",
      "Epoch 461/500\n",
      "16/16 [==============================] - 1s 89ms/step - loss: 14.5327 - predict_loss: 0.0000e+00 - loss_loss: 14.5327\n",
      "Epoch 462/500\n",
      "16/16 [==============================] - 1s 48ms/step - loss: 10.5371 - predict_loss: 0.0000e+00 - loss_loss: 10.5371\n",
      "Epoch 463/500\n",
      "16/16 [==============================] - 1s 48ms/step - loss: 13.9647 - predict_loss: 0.0000e+00 - loss_loss: 13.9647\n",
      "Epoch 464/500\n",
      "16/16 [==============================] - 1s 57ms/step - loss: 16.5242 - predict_loss: 0.0000e+00 - loss_loss: 16.5242\n",
      "Epoch 465/500\n",
      "16/16 [==============================] - 1s 83ms/step - loss: 22.9540 - predict_loss: 0.0000e+00 - loss_loss: 22.9540\n",
      "Epoch 466/500\n",
      "16/16 [==============================] - 1s 85ms/step - loss: 24.2504 - predict_loss: 0.0000e+00 - loss_loss: 24.2504\n",
      "Epoch 467/500\n",
      "16/16 [==============================] - 1s 73ms/step - loss: 16.0586 - predict_loss: 0.0000e+00 - loss_loss: 16.0586\n",
      "Epoch 468/500\n",
      "16/16 [==============================] - 1s 56ms/step - loss: 15.7141 - predict_loss: 0.0000e+00 - loss_loss: 15.7141\n",
      "Epoch 469/500\n",
      "16/16 [==============================] - 1s 49ms/step - loss: 11.8155 - predict_loss: 0.0000e+00 - loss_loss: 11.8155\n",
      "Epoch 470/500\n",
      "16/16 [==============================] - 1s 49ms/step - loss: 14.3173 - predict_loss: 0.0000e+00 - loss_loss: 14.3173\n",
      "Epoch 471/500\n",
      "16/16 [==============================] - 1s 56ms/step - loss: 13.5646 - predict_loss: 0.0000e+00 - loss_loss: 13.5646\n",
      "Epoch 472/500\n",
      "16/16 [==============================] - 1s 57ms/step - loss: 17.2304 - predict_loss: 0.0000e+00 - loss_loss: 17.2304\n",
      "Epoch 473/500\n",
      "16/16 [==============================] - 1s 46ms/step - loss: 17.8688 - predict_loss: 0.0000e+00 - loss_loss: 17.8688\n",
      "Epoch 474/500\n",
      "16/16 [==============================] - 1s 66ms/step - loss: 31.1015 - predict_loss: 0.0000e+00 - loss_loss: 31.1015\n",
      "Epoch 475/500\n",
      "16/16 [==============================] - 2s 100ms/step - loss: 21.4844 - predict_loss: 0.0000e+00 - loss_loss: 21.4844\n",
      "Epoch 476/500\n",
      "16/16 [==============================] - 1s 62ms/step - loss: 25.4749 - predict_loss: 0.0000e+00 - loss_loss: 25.4749\n",
      "Epoch 477/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 1s 64ms/step - loss: 11.2894 - predict_loss: 0.0000e+00 - loss_loss: 11.2894\n",
      "Epoch 478/500\n",
      "16/16 [==============================] - 1s 59ms/step - loss: 13.8567 - predict_loss: 0.0000e+00 - loss_loss: 13.8567\n",
      "Epoch 479/500\n",
      "16/16 [==============================] - 1s 62ms/step - loss: 9.7026 - predict_loss: 0.0000e+00 - loss_loss: 9.7026\n",
      "Epoch 480/500\n",
      "16/16 [==============================] - 1s 64ms/step - loss: 13.4565 - predict_loss: 0.0000e+00 - loss_loss: 13.4565\n",
      "Epoch 481/500\n",
      "16/16 [==============================] - 1s 55ms/step - loss: 13.9864 - predict_loss: 0.0000e+00 - loss_loss: 13.9864\n",
      "Epoch 482/500\n",
      "16/16 [==============================] - 1s 48ms/step - loss: 18.9364 - predict_loss: 0.0000e+00 - loss_loss: 18.9364\n",
      "Epoch 483/500\n",
      "16/16 [==============================] - 1s 54ms/step - loss: 13.0657 - predict_loss: 0.0000e+00 - loss_loss: 13.0657\n",
      "Epoch 484/500\n",
      "16/16 [==============================] - 1s 51ms/step - loss: 15.5882 - predict_loss: 0.0000e+00 - loss_loss: 15.5882\n",
      "Epoch 485/500\n",
      "16/16 [==============================] - 1s 47ms/step - loss: 9.0195 - predict_loss: 0.0000e+00 - loss_loss: 9.0195\n",
      "Epoch 486/500\n",
      "16/16 [==============================] - 1s 73ms/step - loss: 11.2432 - predict_loss: 0.0000e+00 - loss_loss: 11.2432\n",
      "Epoch 487/500\n",
      "16/16 [==============================] - 1s 56ms/step - loss: 11.5816 - predict_loss: 0.0000e+00 - loss_loss: 11.5816\n",
      "Epoch 488/500\n",
      "16/16 [==============================] - 1s 50ms/step - loss: 16.7882 - predict_loss: 0.0000e+00 - loss_loss: 16.7882\n",
      "Epoch 489/500\n",
      "16/16 [==============================] - 1s 57ms/step - loss: 21.1375 - predict_loss: 0.0000e+00 - loss_loss: 21.1375\n",
      "Epoch 490/500\n",
      "16/16 [==============================] - 1s 51ms/step - loss: 21.4073 - predict_loss: 0.0000e+00 - loss_loss: 21.4073\n",
      "Epoch 491/500\n",
      "16/16 [==============================] - 1s 56ms/step - loss: 18.4380 - predict_loss: 0.0000e+00 - loss_loss: 18.4380\n",
      "Epoch 492/500\n",
      "16/16 [==============================] - 1s 59ms/step - loss: 12.9133 - predict_loss: 0.0000e+00 - loss_loss: 12.9133\n",
      "Epoch 493/500\n",
      "16/16 [==============================] - 1s 55ms/step - loss: 13.0563 - predict_loss: 0.0000e+00 - loss_loss: 13.0563\n",
      "Epoch 494/500\n",
      "16/16 [==============================] - 1s 60ms/step - loss: 11.0909 - predict_loss: 0.0000e+00 - loss_loss: 11.0909\n",
      "Epoch 495/500\n",
      "16/16 [==============================] - 1s 59ms/step - loss: 13.6504 - predict_loss: 0.0000e+00 - loss_loss: 13.6504\n",
      "Epoch 496/500\n",
      "16/16 [==============================] - 1s 55ms/step - loss: 11.7381 - predict_loss: 0.0000e+00 - loss_loss: 11.7381\n",
      "Epoch 497/500\n",
      "16/16 [==============================] - 1s 70ms/step - loss: 16.0670 - predict_loss: 0.0000e+00 - loss_loss: 16.0670\n",
      "Epoch 498/500\n",
      "16/16 [==============================] - 1s 70ms/step - loss: 20.1535 - predict_loss: 0.0000e+00 - loss_loss: 20.1535\n",
      "Epoch 499/500\n",
      "16/16 [==============================] - 1s 50ms/step - loss: 30.4088 - predict_loss: 0.0000e+00 - loss_loss: 30.4088\n",
      "Epoch 500/500\n",
      "16/16 [==============================] - 2s 98ms/step - loss: 17.1743 - predict_loss: 0.0000e+00 - loss_loss: 17.1743\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x131f91dd8>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# now run the training!\n",
    "# try to overfit a batch first...\n",
    "\n",
    "INPUT_LENGTH = 8\n",
    "OUTPUT_LENGTH = 4\n",
    "TOTAL_LENGTH = INPUT_LENGTH + OUTPUT_LENGTH\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "inp_ph,target_ph, model, params, loss = vanilla_lstm_model(128,\n",
    "                                                           INPUT_LENGTH,\n",
    "                                                           OUTPUT_LENGTH,\n",
    "                                                          1e-3)\n",
    "model.summary()\n",
    "indices, input_batch, target_batch = get_batch(normalized_data,BATCH_SIZE,INPUT_LENGTH,OUTPUT_LENGTH)\n",
    "# prepare data\n",
    "input_batch_padded = np.hstack([input_batch,np.zeros((BATCH_SIZE,OUTPUT_LENGTH,2))])\n",
    "target_batch_padded = np.hstack([np.zeros((BATCH_SIZE,INPUT_LENGTH,2)),target_batch])\n",
    "\n",
    "\n",
    "# and train\n",
    "model.fit(\n",
    "    [input_batch_padded,target_batch_padded],\n",
    "    [np.zeros((BATCH_SIZE,TOTAL_LENGTH,2)),np.zeros(BATCH_SIZE)],\n",
    "    epochs = 500,\n",
    "    callbacks = get_callbacks(input_batch_padded,target_batch_padded)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "16/16 [==============================] - 5s 330ms/step - loss: 0.0854 - predict_loss: 0.0000e+00 - loss_loss: 0.0854\n",
      "Epoch 2/500\n",
      "16/16 [==============================] - 1s 48ms/step - loss: 0.1401 - predict_loss: 0.0000e+00 - loss_loss: 0.1401\n",
      "Epoch 3/500\n",
      "16/16 [==============================] - 1s 41ms/step - loss: 0.0851 - predict_loss: 0.0000e+00 - loss_loss: 0.0851\n",
      "Epoch 4/500\n",
      "16/16 [==============================] - 1s 49ms/step - loss: 0.0847 - predict_loss: 0.0000e+00 - loss_loss: 0.0847\n",
      "Epoch 5/500\n",
      "16/16 [==============================] - 1s 47ms/step - loss: 0.0855 - predict_loss: 0.0000e+00 - loss_loss: 0.0855\n",
      "Epoch 6/500\n",
      "16/16 [==============================] - 1s 57ms/step - loss: 0.0852 - predict_loss: 0.0000e+00 - loss_loss: 0.0852\n",
      "Epoch 7/500\n",
      "16/16 [==============================] - 1s 59ms/step - loss: 0.0847 - predict_loss: 0.0000e+00 - loss_loss: 0.0847\n",
      "Epoch 8/500\n",
      "16/16 [==============================] - 1s 45ms/step - loss: 0.0843 - predict_loss: 0.0000e+00 - loss_loss: 0.0843\n",
      "Epoch 9/500\n",
      "16/16 [==============================] - 1s 52ms/step - loss: 0.0839 - predict_loss: 0.0000e+00 - loss_loss: 0.0839\n",
      "Epoch 10/500\n",
      "16/16 [==============================] - 1s 55ms/step - loss: 0.0833 - predict_loss: 0.0000e+00 - loss_loss: 0.0833\n",
      "Epoch 11/500\n",
      "16/16 [==============================] - 1s 60ms/step - loss: 0.0828 - predict_loss: 0.0000e+00 - loss_loss: 0.0828\n",
      "Epoch 12/500\n",
      "16/16 [==============================] - 1s 69ms/step - loss: 0.0824 - predict_loss: 0.0000e+00 - loss_loss: 0.0824\n",
      "Epoch 13/500\n",
      "16/16 [==============================] - 1s 54ms/step - loss: 0.0821 - predict_loss: 0.0000e+00 - loss_loss: 0.0821\n",
      "Epoch 14/500\n",
      "16/16 [==============================] - 1s 48ms/step - loss: 0.0819 - predict_loss: 0.0000e+00 - loss_loss: 0.0819\n",
      "Epoch 15/500\n",
      "16/16 [==============================] - 1s 59ms/step - loss: 0.0817 - predict_loss: 0.0000e+00 - loss_loss: 0.0817\n",
      "Epoch 16/500\n",
      "16/16 [==============================] - 1s 61ms/step - loss: 0.0817 - predict_loss: 0.0000e+00 - loss_loss: 0.0817\n",
      "Epoch 17/500\n",
      "16/16 [==============================] - 1s 66ms/step - loss: 0.0818 - predict_loss: 0.0000e+00 - loss_loss: 0.0818\n",
      "Epoch 18/500\n",
      "16/16 [==============================] - 1s 54ms/step - loss: 0.0821 - predict_loss: 0.0000e+00 - loss_loss: 0.0821\n",
      "Epoch 19/500\n",
      "16/16 [==============================] - 1s 49ms/step - loss: 0.0824 - predict_loss: 0.0000e+00 - loss_loss: 0.0824\n",
      "Epoch 20/500\n",
      "16/16 [==============================] - 1s 51ms/step - loss: 0.0827 - predict_loss: 0.0000e+00 - loss_loss: 0.0827\n",
      "Epoch 21/500\n",
      "16/16 [==============================] - 1s 56ms/step - loss: 0.0826 - predict_loss: 0.0000e+00 - loss_loss: 0.0826\n",
      "Epoch 22/500\n",
      "16/16 [==============================] - 1s 61ms/step - loss: 0.0822 - predict_loss: 0.0000e+00 - loss_loss: 0.0822\n",
      "Epoch 23/500\n",
      "16/16 [==============================] - 1s 66ms/step - loss: 0.0817 - predict_loss: 0.0000e+00 - loss_loss: 0.0817\n",
      "Epoch 24/500\n",
      "16/16 [==============================] - 1s 58ms/step - loss: 0.0812 - predict_loss: 0.0000e+00 - loss_loss: 0.0812\n",
      "Epoch 25/500\n",
      "16/16 [==============================] - 1s 86ms/step - loss: 0.0810 - predict_loss: 0.0000e+00 - loss_loss: 0.0810\n",
      "Epoch 26/500\n",
      "16/16 [==============================] - 1s 51ms/step - loss: 0.0810 - predict_loss: 0.0000e+00 - loss_loss: 0.0810\n",
      "Epoch 27/500\n",
      "16/16 [==============================] - 1s 55ms/step - loss: 0.0815 - predict_loss: 0.0000e+00 - loss_loss: 0.0815\n",
      "Epoch 28/500\n",
      "16/16 [==============================] - 1s 54ms/step - loss: 0.0821 - predict_loss: 0.0000e+00 - loss_loss: 0.0821\n",
      "Epoch 29/500\n",
      "16/16 [==============================] - 1s 47ms/step - loss: 0.0822 - predict_loss: 0.0000e+00 - loss_loss: 0.0822\n",
      "Epoch 30/500\n",
      "16/16 [==============================] - 1s 56ms/step - loss: 0.0817 - predict_loss: 0.0000e+00 - loss_loss: 0.0817\n",
      "Epoch 31/500\n",
      "16/16 [==============================] - 1s 49ms/step - loss: 0.0809 - predict_loss: 0.0000e+00 - loss_loss: 0.0809\n",
      "Epoch 32/500\n",
      "16/16 [==============================] - 1s 44ms/step - loss: 0.0803 - predict_loss: 0.0000e+00 - loss_loss: 0.0803\n",
      "Epoch 33/500\n",
      "16/16 [==============================] - 1s 63ms/step - loss: 0.0799 - predict_loss: 0.0000e+00 - loss_loss: 0.0799\n",
      "Epoch 34/500\n",
      "16/16 [==============================] - 1s 50ms/step - loss: 0.0794 - predict_loss: 0.0000e+00 - loss_loss: 0.0794\n",
      "Epoch 35/500\n",
      "16/16 [==============================] - 1s 56ms/step - loss: 0.0790 - predict_loss: 0.0000e+00 - loss_loss: 0.0790\n",
      "Epoch 36/500\n",
      "16/16 [==============================] - 1s 52ms/step - loss: 0.0786 - predict_loss: 0.0000e+00 - loss_loss: 0.0786\n",
      "Epoch 37/500\n",
      "16/16 [==============================] - 1s 59ms/step - loss: 0.0784 - predict_loss: 0.0000e+00 - loss_loss: 0.0784\n",
      "Epoch 38/500\n",
      "16/16 [==============================] - 1s 65ms/step - loss: 0.0781 - predict_loss: 0.0000e+00 - loss_loss: 0.0781\n",
      "Epoch 39/500\n",
      "16/16 [==============================] - 1s 48ms/step - loss: 0.0780 - predict_loss: 0.0000e+00 - loss_loss: 0.0780\n",
      "Epoch 40/500\n",
      "16/16 [==============================] - 1s 55ms/step - loss: 0.0781 - predict_loss: 0.0000e+00 - loss_loss: 0.0781\n",
      "Epoch 41/500\n",
      "16/16 [==============================] - 1s 42ms/step - loss: 0.0784 - predict_loss: 0.0000e+00 - loss_loss: 0.0784\n",
      "Epoch 42/500\n",
      "16/16 [==============================] - 1s 49ms/step - loss: 0.0790 - predict_loss: 0.0000e+00 - loss_loss: 0.0790\n",
      "Epoch 43/500\n",
      "16/16 [==============================] - 1s 53ms/step - loss: 0.0795 - predict_loss: 0.0000e+00 - loss_loss: 0.0795\n",
      "Epoch 44/500\n",
      "16/16 [==============================] - 1s 51ms/step - loss: 0.0794 - predict_loss: 0.0000e+00 - loss_loss: 0.0794\n",
      "Epoch 45/500\n",
      "16/16 [==============================] - 1s 54ms/step - loss: 0.0789 - predict_loss: 0.0000e+00 - loss_loss: 0.0789\n",
      "Epoch 46/500\n",
      "16/16 [==============================] - 1s 47ms/step - loss: 0.0784 - predict_loss: 0.0000e+00 - loss_loss: 0.0784\n",
      "Epoch 47/500\n",
      "16/16 [==============================] - 1s 56ms/step - loss: 0.0782 - predict_loss: 0.0000e+00 - loss_loss: 0.0782\n",
      "Epoch 48/500\n",
      "16/16 [==============================] - 1s 46ms/step - loss: 0.0781 - predict_loss: 0.0000e+00 - loss_loss: 0.0781\n",
      "Epoch 49/500\n",
      "16/16 [==============================] - 1s 47ms/step - loss: 0.0778 - predict_loss: 0.0000e+00 - loss_loss: 0.0778\n",
      "Epoch 50/500\n",
      "16/16 [==============================] - 1s 58ms/step - loss: 0.0774 - predict_loss: 0.0000e+00 - loss_loss: 0.0774\n",
      "Epoch 51/500\n",
      "16/16 [==============================] - 1s 89ms/step - loss: 0.0768 - predict_loss: 0.0000e+00 - loss_loss: 0.0768\n",
      "Epoch 52/500\n",
      "16/16 [==============================] - 1s 48ms/step - loss: 0.0763 - predict_loss: 0.0000e+00 - loss_loss: 0.0763\n",
      "Epoch 53/500\n",
      "16/16 [==============================] - 1s 51ms/step - loss: 0.0758 - predict_loss: 0.0000e+00 - loss_loss: 0.0758\n",
      "Epoch 54/500\n",
      "16/16 [==============================] - 1s 53ms/step - loss: 0.0756 - predict_loss: 0.0000e+00 - loss_loss: 0.0756\n",
      "Epoch 55/500\n",
      "16/16 [==============================] - 1s 49ms/step - loss: 0.0754 - predict_loss: 0.0000e+00 - loss_loss: 0.0754\n",
      "Epoch 56/500\n",
      "16/16 [==============================] - 1s 61ms/step - loss: 0.0754 - predict_loss: 0.0000e+00 - loss_loss: 0.0754\n",
      "Epoch 57/500\n",
      "16/16 [==============================] - 1s 51ms/step - loss: 0.0756 - predict_loss: 0.0000e+00 - loss_loss: 0.0756\n",
      "Epoch 58/500\n",
      "16/16 [==============================] - 1s 55ms/step - loss: 0.0758 - predict_loss: 0.0000e+00 - loss_loss: 0.0758\n",
      "Epoch 59/500\n",
      "16/16 [==============================] - 1s 57ms/step - loss: 0.0756 - predict_loss: 0.0000e+00 - loss_loss: 0.0756\n",
      "Epoch 60/500\n",
      "16/16 [==============================] - 1s 51ms/step - loss: 0.0751 - predict_loss: 0.0000e+00 - loss_loss: 0.0751\n",
      "Epoch 61/500\n",
      "16/16 [==============================] - 1s 55ms/step - loss: 0.0746 - predict_loss: 0.0000e+00 - loss_loss: 0.0746\n",
      "Epoch 62/500\n",
      "16/16 [==============================] - 1s 49ms/step - loss: 0.0743 - predict_loss: 0.0000e+00 - loss_loss: 0.0743\n",
      "Epoch 63/500\n",
      "16/16 [==============================] - 1s 50ms/step - loss: 0.0742 - predict_loss: 0.0000e+00 - loss_loss: 0.0742\n",
      "Epoch 64/500\n",
      "16/16 [==============================] - 1s 50ms/step - loss: 0.0744 - predict_loss: 0.0000e+00 - loss_loss: 0.0744\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65/500\n",
      "16/16 [==============================] - 1s 54ms/step - loss: 0.0747 - predict_loss: 0.0000e+00 - loss_loss: 0.0747\n",
      "Epoch 66/500\n",
      "16/16 [==============================] - 1s 85ms/step - loss: 0.0750 - predict_loss: 0.0000e+00 - loss_loss: 0.0750\n",
      "Epoch 67/500\n",
      "16/16 [==============================] - 1s 72ms/step - loss: 0.0750 - predict_loss: 0.0000e+00 - loss_loss: 0.0750\n",
      "Epoch 68/500\n",
      "16/16 [==============================] - 1s 60ms/step - loss: 0.0749 - predict_loss: 0.0000e+00 - loss_loss: 0.0749\n",
      "Epoch 69/500\n",
      "16/16 [==============================] - 1s 57ms/step - loss: 0.0750 - predict_loss: 0.0000e+00 - loss_loss: 0.0750\n",
      "Epoch 70/500\n",
      "16/16 [==============================] - 1s 48ms/step - loss: 0.0751 - predict_loss: 0.0000e+00 - loss_loss: 0.0751\n",
      "Epoch 71/500\n",
      "16/16 [==============================] - 1s 50ms/step - loss: 0.0750 - predict_loss: 0.0000e+00 - loss_loss: 0.0750\n",
      "Epoch 72/500\n",
      "16/16 [==============================] - 1s 57ms/step - loss: 0.0745 - predict_loss: 0.0000e+00 - loss_loss: 0.0745\n",
      "Epoch 73/500\n",
      "16/16 [==============================] - 1s 61ms/step - loss: 0.0739 - predict_loss: 0.0000e+00 - loss_loss: 0.0739\n",
      "Epoch 74/500\n",
      "16/16 [==============================] - 1s 55ms/step - loss: 0.0731 - predict_loss: 0.0000e+00 - loss_loss: 0.0731\n",
      "Epoch 75/500\n",
      "16/16 [==============================] - 1s 62ms/step - loss: 0.0725 - predict_loss: 0.0000e+00 - loss_loss: 0.0725\n",
      "Epoch 76/500\n",
      "16/16 [==============================] - 1s 55ms/step - loss: 0.0720 - predict_loss: 0.0000e+00 - loss_loss: 0.0720\n",
      "Epoch 77/500\n",
      "16/16 [==============================] - 1s 58ms/step - loss: 0.0717 - predict_loss: 0.0000e+00 - loss_loss: 0.0717\n",
      "Epoch 78/500\n",
      "16/16 [==============================] - 2s 106ms/step - loss: 0.0714 - predict_loss: 0.0000e+00 - loss_loss: 0.0714\n",
      "Epoch 79/500\n",
      "16/16 [==============================] - 1s 55ms/step - loss: 0.0713 - predict_loss: 0.0000e+00 - loss_loss: 0.0713\n",
      "Epoch 80/500\n",
      "16/16 [==============================] - 1s 59ms/step - loss: 0.0713 - predict_loss: 0.0000e+00 - loss_loss: 0.0713\n",
      "Epoch 81/500\n",
      "16/16 [==============================] - 1s 55ms/step - loss: 0.0715 - predict_loss: 0.0000e+00 - loss_loss: 0.0715\n",
      "Epoch 82/500\n",
      "16/16 [==============================] - 1s 55ms/step - loss: 0.0719 - predict_loss: 0.0000e+00 - loss_loss: 0.0719\n",
      "Epoch 83/500\n",
      "16/16 [==============================] - 1s 85ms/step - loss: 0.0724 - predict_loss: 0.0000e+00 - loss_loss: 0.0724\n",
      "Epoch 84/500\n",
      "16/16 [==============================] - 1s 73ms/step - loss: 0.0725 - predict_loss: 0.0000e+00 - loss_loss: 0.0725\n",
      "Epoch 85/500\n",
      "16/16 [==============================] - 1s 50ms/step - loss: 0.0720 - predict_loss: 0.0000e+00 - loss_loss: 0.0720\n",
      "Epoch 86/500\n",
      "16/16 [==============================] - 1s 48ms/step - loss: 0.0712 - predict_loss: 0.0000e+00 - loss_loss: 0.0712\n",
      "Epoch 87/500\n",
      "16/16 [==============================] - 1s 57ms/step - loss: 0.0705 - predict_loss: 0.0000e+00 - loss_loss: 0.0705\n",
      "Epoch 88/500\n",
      "16/16 [==============================] - 1s 60ms/step - loss: 0.0701 - predict_loss: 0.0000e+00 - loss_loss: 0.0701\n",
      "Epoch 89/500\n",
      "16/16 [==============================] - 1s 55ms/step - loss: 0.0699 - predict_loss: 0.0000e+00 - loss_loss: 0.0699\n",
      "Epoch 90/500\n",
      "16/16 [==============================] - 1s 58ms/step - loss: 0.0700 - predict_loss: 0.0000e+00 - loss_loss: 0.0700\n",
      "Epoch 91/500\n",
      "16/16 [==============================] - 1s 55ms/step - loss: 0.0704 - predict_loss: 0.0000e+00 - loss_loss: 0.0704\n",
      "Epoch 92/500\n",
      "16/16 [==============================] - 1s 52ms/step - loss: 0.0711 - predict_loss: 0.0000e+00 - loss_loss: 0.0711\n",
      "Epoch 93/500\n",
      "16/16 [==============================] - 1s 63ms/step - loss: 0.0716 - predict_loss: 0.0000e+00 - loss_loss: 0.0716\n",
      "Epoch 94/500\n",
      "16/16 [==============================] - 1s 66ms/step - loss: 0.0716 - predict_loss: 0.0000e+00 - loss_loss: 0.0716\n",
      "Epoch 95/500\n",
      "16/16 [==============================] - 1s 57ms/step - loss: 0.0712 - predict_loss: 0.0000e+00 - loss_loss: 0.0712\n",
      "Epoch 96/500\n",
      "16/16 [==============================] - 1s 54ms/step - loss: 0.0707 - predict_loss: 0.0000e+00 - loss_loss: 0.0707\n",
      "Epoch 97/500\n",
      "16/16 [==============================] - 1s 52ms/step - loss: 0.0703 - predict_loss: 0.0000e+00 - loss_loss: 0.0703\n",
      "Epoch 98/500\n",
      "16/16 [==============================] - 1s 59ms/step - loss: 0.0700 - predict_loss: 0.0000e+00 - loss_loss: 0.0700\n",
      "Epoch 99/500\n",
      "16/16 [==============================] - 1s 58ms/step - loss: 0.0696 - predict_loss: 0.0000e+00 - loss_loss: 0.0696\n",
      "Epoch 100/500\n",
      "16/16 [==============================] - 1s 50ms/step - loss: 0.0692 - predict_loss: 0.0000e+00 - loss_loss: 0.0692\n",
      "Epoch 101/500\n",
      "16/16 [==============================] - 1s 49ms/step - loss: 0.0687 - predict_loss: 0.0000e+00 - loss_loss: 0.0687\n",
      "Epoch 102/500\n",
      "16/16 [==============================] - 1s 46ms/step - loss: 0.0683 - predict_loss: 0.0000e+00 - loss_loss: 0.0683\n",
      "Epoch 103/500\n",
      "16/16 [==============================] - 1s 52ms/step - loss: 0.0680 - predict_loss: 0.0000e+00 - loss_loss: 0.0680\n",
      "Epoch 104/500\n",
      "16/16 [==============================] - 1s 65ms/step - loss: 0.0678 - predict_loss: 0.0000e+00 - loss_loss: 0.0678\n",
      "Epoch 105/500\n",
      "16/16 [==============================] - 1s 67ms/step - loss: 0.0677 - predict_loss: 0.0000e+00 - loss_loss: 0.0677\n",
      "Epoch 106/500\n",
      "16/16 [==============================] - 2s 113ms/step - loss: 0.0676 - predict_loss: 0.0000e+00 - loss_loss: 0.0676\n",
      "Epoch 107/500\n",
      "16/16 [==============================] - 1s 56ms/step - loss: 0.0678 - predict_loss: 0.0000e+00 - loss_loss: 0.0678\n",
      "Epoch 108/500\n",
      "16/16 [==============================] - 1s 54ms/step - loss: 0.0681 - predict_loss: 0.0000e+00 - loss_loss: 0.0681\n",
      "Epoch 109/500\n",
      "16/16 [==============================] - 1s 58ms/step - loss: 0.0685 - predict_loss: 0.0000e+00 - loss_loss: 0.0685\n",
      "Epoch 110/500\n",
      "16/16 [==============================] - 1s 55ms/step - loss: 0.0687 - predict_loss: 0.0000e+00 - loss_loss: 0.0687\n",
      "Epoch 111/500\n",
      "16/16 [==============================] - 1s 51ms/step - loss: 0.0683 - predict_loss: 0.0000e+00 - loss_loss: 0.0683\n",
      "Epoch 112/500\n",
      "16/16 [==============================] - 1s 54ms/step - loss: 0.0676 - predict_loss: 0.0000e+00 - loss_loss: 0.0676\n",
      "Epoch 113/500\n",
      "16/16 [==============================] - 1s 53ms/step - loss: 0.0669 - predict_loss: 0.0000e+00 - loss_loss: 0.0669\n",
      "Epoch 114/500\n",
      "16/16 [==============================] - 1s 69ms/step - loss: 0.0665 - predict_loss: 0.0000e+00 - loss_loss: 0.0665\n",
      "Epoch 115/500\n",
      "16/16 [==============================] - 1s 53ms/step - loss: 0.0662 - predict_loss: 0.0000e+00 - loss_loss: 0.0662\n",
      "Epoch 116/500\n",
      "16/16 [==============================] - 1s 53ms/step - loss: 0.0661 - predict_loss: 0.0000e+00 - loss_loss: 0.0661\n",
      "Epoch 117/500\n",
      "16/16 [==============================] - 1s 50ms/step - loss: 0.0662 - predict_loss: 0.0000e+00 - loss_loss: 0.0662\n",
      "Epoch 118/500\n",
      "16/16 [==============================] - 1s 52ms/step - loss: 0.0666 - predict_loss: 0.0000e+00 - loss_loss: 0.0666\n",
      "Epoch 119/500\n",
      "16/16 [==============================] - 1s 53ms/step - loss: 0.0674 - predict_loss: 0.0000e+00 - loss_loss: 0.0674\n",
      "Epoch 120/500\n",
      "16/16 [==============================] - 1s 64ms/step - loss: 0.0680 - predict_loss: 0.0000e+00 - loss_loss: 0.0680\n",
      "Epoch 121/500\n",
      "16/16 [==============================] - 1s 56ms/step - loss: 0.0680 - predict_loss: 0.0000e+00 - loss_loss: 0.0680\n",
      "Epoch 122/500\n",
      "16/16 [==============================] - 1s 54ms/step - loss: 0.0674 - predict_loss: 0.0000e+00 - loss_loss: 0.0674\n",
      "Epoch 123/500\n",
      "16/16 [==============================] - 1s 59ms/step - loss: 0.0669 - predict_loss: 0.0000e+00 - loss_loss: 0.0669\n",
      "Epoch 124/500\n",
      "16/16 [==============================] - 1s 77ms/step - loss: 0.0665 - predict_loss: 0.0000e+00 - loss_loss: 0.0665\n",
      "Epoch 125/500\n",
      "16/16 [==============================] - 1s 89ms/step - loss: 0.0663 - predict_loss: 0.0000e+00 - loss_loss: 0.0663\n",
      "Epoch 126/500\n",
      "16/16 [==============================] - 1s 54ms/step - loss: 0.0659 - predict_loss: 0.0000e+00 - loss_loss: 0.0659\n",
      "Epoch 127/500\n",
      "16/16 [==============================] - 1s 52ms/step - loss: 0.0656 - predict_loss: 0.0000e+00 - loss_loss: 0.0656\n",
      "Epoch 128/500\n",
      "16/16 [==============================] - 1s 53ms/step - loss: 0.0652 - predict_loss: 0.0000e+00 - loss_loss: 0.0652\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 129/500\n",
      "16/16 [==============================] - 1s 50ms/step - loss: 0.0648 - predict_loss: 0.0000e+00 - loss_loss: 0.0648\n",
      "Epoch 130/500\n",
      "16/16 [==============================] - 1s 71ms/step - loss: 0.0645 - predict_loss: 0.0000e+00 - loss_loss: 0.0645\n",
      "Epoch 131/500\n",
      "16/16 [==============================] - 1s 54ms/step - loss: 0.0643 - predict_loss: 0.0000e+00 - loss_loss: 0.0643\n",
      "Epoch 132/500\n",
      "16/16 [==============================] - 1s 57ms/step - loss: 0.0641 - predict_loss: 0.0000e+00 - loss_loss: 0.0641\n",
      "Epoch 133/500\n",
      "16/16 [==============================] - 2s 101ms/step - loss: 0.0641 - predict_loss: 0.0000e+00 - loss_loss: 0.0641\n",
      "Epoch 134/500\n",
      "16/16 [==============================] - 1s 55ms/step - loss: 0.0641 - predict_loss: 0.0000e+00 - loss_loss: 0.0641\n",
      "Epoch 135/500\n",
      "16/16 [==============================] - 1s 71ms/step - loss: 0.0645 - predict_loss: 0.0000e+00 - loss_loss: 0.0645\n",
      "Epoch 136/500\n",
      "16/16 [==============================] - 1s 61ms/step - loss: 0.0648 - predict_loss: 0.0000e+00 - loss_loss: 0.0648\n",
      "Epoch 137/500\n",
      "16/16 [==============================] - 1s 52ms/step - loss: 0.0650 - predict_loss: 0.0000e+00 - loss_loss: 0.0650\n",
      "Epoch 138/500\n",
      "16/16 [==============================] - 1s 54ms/step - loss: 0.0647 - predict_loss: 0.0000e+00 - loss_loss: 0.0647\n",
      "Epoch 139/500\n",
      "16/16 [==============================] - 1s 51ms/step - loss: 0.0641 - predict_loss: 0.0000e+00 - loss_loss: 0.0641\n",
      "Epoch 140/500\n",
      "16/16 [==============================] - 1s 58ms/step - loss: 0.0635 - predict_loss: 0.0000e+00 - loss_loss: 0.0635\n",
      "Epoch 141/500\n",
      "16/16 [==============================] - 1s 50ms/step - loss: 0.0630 - predict_loss: 0.0000e+00 - loss_loss: 0.0630\n",
      "Epoch 142/500\n",
      "16/16 [==============================] - 1s 53ms/step - loss: 0.0627 - predict_loss: 0.0000e+00 - loss_loss: 0.0627\n",
      "Epoch 143/500\n",
      "16/16 [==============================] - 1s 50ms/step - loss: 0.0626 - predict_loss: 0.0000e+00 - loss_loss: 0.0626\n",
      "Epoch 144/500\n",
      "16/16 [==============================] - 1s 57ms/step - loss: 0.0626 - predict_loss: 0.0000e+00 - loss_loss: 0.0626\n",
      "Epoch 145/500\n",
      "16/16 [==============================] - 1s 59ms/step - loss: 0.0628 - predict_loss: 0.0000e+00 - loss_loss: 0.0628\n",
      "Epoch 146/500\n",
      "16/16 [==============================] - 1s 57ms/step - loss: 0.0632 - predict_loss: 0.0000e+00 - loss_loss: 0.0632\n",
      "Epoch 147/500\n",
      "16/16 [==============================] - 1s 50ms/step - loss: 0.0637 - predict_loss: 0.0000e+00 - loss_loss: 0.0637\n",
      "Epoch 148/500\n",
      "16/16 [==============================] - 1s 53ms/step - loss: 0.0642 - predict_loss: 0.0000e+00 - loss_loss: 0.0642\n",
      "Epoch 149/500\n",
      "16/16 [==============================] - 1s 53ms/step - loss: 0.0644 - predict_loss: 0.0000e+00 - loss_loss: 0.0644\n",
      "Epoch 150/500\n",
      "16/16 [==============================] - 1s 52ms/step - loss: 0.0642 - predict_loss: 0.0000e+00 - loss_loss: 0.0642\n",
      "Epoch 151/500\n",
      "16/16 [==============================] - 1s 55ms/step - loss: 0.0637 - predict_loss: 0.0000e+00 - loss_loss: 0.0637\n",
      "Epoch 152/500\n",
      "16/16 [==============================] - 1s 65ms/step - loss: 0.0632 - predict_loss: 0.0000e+00 - loss_loss: 0.0632\n",
      "Epoch 153/500\n",
      "16/16 [==============================] - 1s 60ms/step - loss: 0.0628 - predict_loss: 0.0000e+00 - loss_loss: 0.0628\n",
      "Epoch 154/500\n",
      "16/16 [==============================] - 1s 54ms/step - loss: 0.0624 - predict_loss: 0.0000e+00 - loss_loss: 0.0624\n",
      "Epoch 155/500\n",
      "16/16 [==============================] - 1s 52ms/step - loss: 0.0620 - predict_loss: 0.0000e+00 - loss_loss: 0.0620\n",
      "Epoch 156/500\n",
      "16/16 [==============================] - 1s 53ms/step - loss: 0.0616 - predict_loss: 0.0000e+00 - loss_loss: 0.0616\n",
      "Epoch 157/500\n",
      "16/16 [==============================] - 1s 64ms/step - loss: 0.0613 - predict_loss: 0.0000e+00 - loss_loss: 0.0613\n",
      "Epoch 158/500\n",
      "16/16 [==============================] - 1s 53ms/step - loss: 0.0610 - predict_loss: 0.0000e+00 - loss_loss: 0.0610\n",
      "Epoch 159/500\n",
      "16/16 [==============================] - 1s 59ms/step - loss: 0.0608 - predict_loss: 0.0000e+00 - loss_loss: 0.0608\n",
      "Epoch 160/500\n",
      "16/16 [==============================] - 2s 103ms/step - loss: 0.0607 - predict_loss: 0.0000e+00 - loss_loss: 0.0607\n",
      "Epoch 161/500\n",
      "16/16 [==============================] - 1s 55ms/step - loss: 0.0607 - predict_loss: 0.0000e+00 - loss_loss: 0.0607\n",
      "Epoch 162/500\n",
      "16/16 [==============================] - 1s 63ms/step - loss: 0.0608 - predict_loss: 0.0000e+00 - loss_loss: 0.0608\n",
      "Epoch 163/500\n",
      "16/16 [==============================] - 1s 54ms/step - loss: 0.0611 - predict_loss: 0.0000e+00 - loss_loss: 0.0611\n",
      "Epoch 164/500\n",
      "16/16 [==============================] - 1s 57ms/step - loss: 0.0614 - predict_loss: 0.0000e+00 - loss_loss: 0.0614\n",
      "Epoch 165/500\n",
      "16/16 [==============================] - 1s 54ms/step - loss: 0.0616 - predict_loss: 0.0000e+00 - loss_loss: 0.0616\n",
      "Epoch 166/500\n",
      "16/16 [==============================] - 1s 51ms/step - loss: 0.0614 - predict_loss: 0.0000e+00 - loss_loss: 0.0614\n",
      "Epoch 167/500\n",
      "16/16 [==============================] - 1s 65ms/step - loss: 0.0608 - predict_loss: 0.0000e+00 - loss_loss: 0.0608\n",
      "Epoch 168/500\n",
      "16/16 [==============================] - 1s 52ms/step - loss: 0.0602 - predict_loss: 0.0000e+00 - loss_loss: 0.0602\n",
      "Epoch 169/500\n",
      "16/16 [==============================] - 1s 57ms/step - loss: 0.0597 - predict_loss: 0.0000e+00 - loss_loss: 0.0597\n",
      "Epoch 170/500\n",
      "16/16 [==============================] - 1s 57ms/step - loss: 0.0594 - predict_loss: 0.0000e+00 - loss_loss: 0.0594\n",
      "Epoch 171/500\n",
      "16/16 [==============================] - 1s 54ms/step - loss: 0.0592 - predict_loss: 0.0000e+00 - loss_loss: 0.0592\n",
      "Epoch 172/500\n",
      "16/16 [==============================] - 1s 70ms/step - loss: 0.0592 - predict_loss: 0.0000e+00 - loss_loss: 0.0592\n",
      "Epoch 173/500\n",
      "16/16 [==============================] - 1s 87ms/step - loss: 0.0593 - predict_loss: 0.0000e+00 - loss_loss: 0.0593\n",
      "Epoch 174/500\n",
      "16/16 [==============================] - 1s 61ms/step - loss: 0.0596 - predict_loss: 0.0000e+00 - loss_loss: 0.0596\n",
      "Epoch 175/500\n",
      "16/16 [==============================] - 1s 57ms/step - loss: 0.0598 - predict_loss: 0.0000e+00 - loss_loss: 0.0598\n",
      "Epoch 176/500\n",
      "16/16 [==============================] - 1s 55ms/step - loss: 0.0601 - predict_loss: 0.0000e+00 - loss_loss: 0.0601\n",
      "Epoch 177/500\n",
      "16/16 [==============================] - 1s 63ms/step - loss: 0.0603 - predict_loss: 0.0000e+00 - loss_loss: 0.0603\n",
      "Epoch 178/500\n",
      "16/16 [==============================] - 1s 60ms/step - loss: 0.0608 - predict_loss: 0.0000e+00 - loss_loss: 0.0608\n",
      "Epoch 179/500\n",
      "16/16 [==============================] - 1s 61ms/step - loss: 0.0614 - predict_loss: 0.0000e+00 - loss_loss: 0.0614\n",
      "Epoch 180/500\n",
      "16/16 [==============================] - 1s 58ms/step - loss: 0.0612 - predict_loss: 0.0000e+00 - loss_loss: 0.0612\n",
      "Epoch 181/500\n",
      "16/16 [==============================] - 1s 62ms/step - loss: 0.0607 - predict_loss: 0.0000e+00 - loss_loss: 0.0607\n",
      "Epoch 182/500\n",
      "16/16 [==============================] - 1s 58ms/step - loss: 0.0598 - predict_loss: 0.0000e+00 - loss_loss: 0.0598\n",
      "Epoch 183/500\n",
      "16/16 [==============================] - 1s 61ms/step - loss: 0.0592 - predict_loss: 0.0000e+00 - loss_loss: 0.0592\n",
      "Epoch 184/500\n",
      "16/16 [==============================] - 1s 55ms/step - loss: 0.0586 - predict_loss: 0.0000e+00 - loss_loss: 0.0586\n",
      "Epoch 185/500\n",
      "16/16 [==============================] - 1s 49ms/step - loss: 0.0583 - predict_loss: 0.0000e+00 - loss_loss: 0.0583\n",
      "Epoch 186/500\n",
      "16/16 [==============================] - 1s 55ms/step - loss: 0.0579 - predict_loss: 0.0000e+00 - loss_loss: 0.0579\n",
      "Epoch 187/500\n",
      "16/16 [==============================] - 2s 112ms/step - loss: 0.0578 - predict_loss: 0.0000e+00 - loss_loss: 0.0578\n",
      "Epoch 188/500\n",
      "16/16 [==============================] - 1s 61ms/step - loss: 0.0576 - predict_loss: 0.0000e+00 - loss_loss: 0.0576\n",
      "Epoch 189/500\n",
      "16/16 [==============================] - 1s 59ms/step - loss: 0.0576 - predict_loss: 0.0000e+00 - loss_loss: 0.0576\n",
      "Epoch 190/500\n",
      "16/16 [==============================] - 1s 57ms/step - loss: 0.0576 - predict_loss: 0.0000e+00 - loss_loss: 0.0576\n",
      "Epoch 191/500\n",
      "16/16 [==============================] - 1s 59ms/step - loss: 0.0578 - predict_loss: 0.0000e+00 - loss_loss: 0.0578\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 192/500\n",
      "16/16 [==============================] - 1s 61ms/step - loss: 0.0580 - predict_loss: 0.0000e+00 - loss_loss: 0.0580\n",
      "Epoch 193/500\n",
      "16/16 [==============================] - 1s 72ms/step - loss: 0.0583 - predict_loss: 0.0000e+00 - loss_loss: 0.0583\n",
      "Epoch 194/500\n",
      "16/16 [==============================] - 1s 59ms/step - loss: 0.0585 - predict_loss: 0.0000e+00 - loss_loss: 0.0585\n",
      "Epoch 195/500\n",
      "16/16 [==============================] - 1s 56ms/step - loss: 0.0583 - predict_loss: 0.0000e+00 - loss_loss: 0.0583\n",
      "Epoch 196/500\n",
      "16/16 [==============================] - 1s 50ms/step - loss: 0.0578 - predict_loss: 0.0000e+00 - loss_loss: 0.0578\n",
      "Epoch 197/500\n",
      "16/16 [==============================] - 1s 64ms/step - loss: 0.0572 - predict_loss: 0.0000e+00 - loss_loss: 0.0572\n",
      "Epoch 198/500\n",
      "16/16 [==============================] - 1s 69ms/step - loss: 0.0567 - predict_loss: 0.0000e+00 - loss_loss: 0.0567\n",
      "Epoch 199/500\n",
      "16/16 [==============================] - 1s 57ms/step - loss: 0.0564 - predict_loss: 0.0000e+00 - loss_loss: 0.0564\n",
      "Epoch 200/500\n",
      "16/16 [==============================] - 1s 49ms/step - loss: 0.0562 - predict_loss: 0.0000e+00 - loss_loss: 0.0562\n",
      "Epoch 201/500\n",
      "16/16 [==============================] - 1s 53ms/step - loss: 0.0562 - predict_loss: 0.0000e+00 - loss_loss: 0.0562\n",
      "Epoch 202/500\n",
      "16/16 [==============================] - 1s 50ms/step - loss: 0.0563 - predict_loss: 0.0000e+00 - loss_loss: 0.0563\n",
      "Epoch 203/500\n",
      "16/16 [==============================] - 1s 69ms/step - loss: 0.0567 - predict_loss: 0.0000e+00 - loss_loss: 0.0567\n",
      "Epoch 204/500\n",
      "16/16 [==============================] - 1s 62ms/step - loss: 0.0573 - predict_loss: 0.0000e+00 - loss_loss: 0.0573\n",
      "Epoch 205/500\n",
      "16/16 [==============================] - 1s 53ms/step - loss: 0.0583 - predict_loss: 0.0000e+00 - loss_loss: 0.0583\n",
      "Epoch 206/500\n",
      "16/16 [==============================] - 1s 52ms/step - loss: 0.0590 - predict_loss: 0.0000e+00 - loss_loss: 0.0590\n",
      "Epoch 207/500\n",
      "16/16 [==============================] - 1s 50ms/step - loss: 0.0590 - predict_loss: 0.0000e+00 - loss_loss: 0.0590\n",
      "Epoch 208/500\n",
      "16/16 [==============================] - 1s 57ms/step - loss: 0.0579 - predict_loss: 0.0000e+00 - loss_loss: 0.0579\n",
      "Epoch 209/500\n",
      "16/16 [==============================] - 1s 58ms/step - loss: 0.0567 - predict_loss: 0.0000e+00 - loss_loss: 0.0567\n",
      "Epoch 210/500\n",
      "16/16 [==============================] - 1s 53ms/step - loss: 0.0559 - predict_loss: 0.0000e+00 - loss_loss: 0.0559\n",
      "Epoch 211/500\n",
      "16/16 [==============================] - 1s 52ms/step - loss: 0.0554 - predict_loss: 0.0000e+00 - loss_loss: 0.0554\n",
      "Epoch 212/500\n",
      "16/16 [==============================] - 1s 56ms/step - loss: 0.0553 - predict_loss: 0.0000e+00 - loss_loss: 0.0553\n",
      "Epoch 213/500\n",
      "16/16 [==============================] - 1s 51ms/step - loss: 0.0552 - predict_loss: 0.0000e+00 - loss_loss: 0.0552\n",
      "Epoch 214/500\n",
      "16/16 [==============================] - 2s 113ms/step - loss: 0.0553 - predict_loss: 0.0000e+00 - loss_loss: 0.0553\n",
      "Epoch 215/500\n",
      "16/16 [==============================] - 1s 64ms/step - loss: 0.0554 - predict_loss: 0.0000e+00 - loss_loss: 0.0554\n",
      "Epoch 216/500\n",
      "16/16 [==============================] - 1s 58ms/step - loss: 0.0556 - predict_loss: 0.0000e+00 - loss_loss: 0.0556\n",
      "Epoch 217/500\n",
      "16/16 [==============================] - 1s 55ms/step - loss: 0.0556 - predict_loss: 0.0000e+00 - loss_loss: 0.0556\n",
      "Epoch 218/500\n",
      "16/16 [==============================] - 1s 57ms/step - loss: 0.0556 - predict_loss: 0.0000e+00 - loss_loss: 0.0556\n",
      "Epoch 219/500\n",
      "16/16 [==============================] - 1s 64ms/step - loss: 0.0555 - predict_loss: 0.0000e+00 - loss_loss: 0.0555\n",
      "Epoch 220/500\n",
      "16/16 [==============================] - 1s 53ms/step - loss: 0.0555 - predict_loss: 0.0000e+00 - loss_loss: 0.0555\n",
      "Epoch 221/500\n",
      "16/16 [==============================] - 1s 53ms/step - loss: 0.0555 - predict_loss: 0.0000e+00 - loss_loss: 0.0555\n",
      "Epoch 222/500\n",
      "16/16 [==============================] - 1s 53ms/step - loss: 0.0557 - predict_loss: 0.0000e+00 - loss_loss: 0.0557\n",
      "Epoch 223/500\n",
      "16/16 [==============================] - 1s 56ms/step - loss: 0.0556 - predict_loss: 0.0000e+00 - loss_loss: 0.0556\n",
      "Epoch 224/500\n",
      "16/16 [==============================] - 1s 82ms/step - loss: 0.0555 - predict_loss: 0.0000e+00 - loss_loss: 0.0555\n",
      "Epoch 225/500\n",
      "16/16 [==============================] - 1s 61ms/step - loss: 0.0552 - predict_loss: 0.0000e+00 - loss_loss: 0.0552\n",
      "Epoch 226/500\n",
      "16/16 [==============================] - 1s 51ms/step - loss: 0.0548 - predict_loss: 0.0000e+00 - loss_loss: 0.0548\n",
      "Epoch 227/500\n",
      "16/16 [==============================] - 1s 52ms/step - loss: 0.0543 - predict_loss: 0.0000e+00 - loss_loss: 0.0543\n",
      "Epoch 228/500\n",
      "16/16 [==============================] - 1s 51ms/step - loss: 0.0539 - predict_loss: 0.0000e+00 - loss_loss: 0.0539\n",
      "Epoch 229/500\n",
      "16/16 [==============================] - 1s 57ms/step - loss: 0.0536 - predict_loss: 0.0000e+00 - loss_loss: 0.0536\n",
      "Epoch 230/500\n",
      "16/16 [==============================] - 1s 69ms/step - loss: 0.0534 - predict_loss: 0.0000e+00 - loss_loss: 0.0534\n",
      "Epoch 231/500\n",
      "16/16 [==============================] - 1s 73ms/step - loss: 0.0533 - predict_loss: 0.0000e+00 - loss_loss: 0.0533\n",
      "Epoch 232/500\n",
      "16/16 [==============================] - 1s 61ms/step - loss: 0.0533 - predict_loss: 0.0000e+00 - loss_loss: 0.0533\n",
      "Epoch 233/500\n",
      "16/16 [==============================] - 1s 58ms/step - loss: 0.0535 - predict_loss: 0.0000e+00 - loss_loss: 0.0535\n",
      "Epoch 234/500\n",
      "16/16 [==============================] - 1s 62ms/step - loss: 0.0540 - predict_loss: 0.0000e+00 - loss_loss: 0.0540\n",
      "Epoch 235/500\n",
      "16/16 [==============================] - 1s 57ms/step - loss: 0.0548 - predict_loss: 0.0000e+00 - loss_loss: 0.0548\n",
      "Epoch 236/500\n",
      "16/16 [==============================] - 1s 56ms/step - loss: 0.0552 - predict_loss: 0.0000e+00 - loss_loss: 0.0552\n",
      "Epoch 237/500\n",
      "16/16 [==============================] - 1s 53ms/step - loss: 0.0549 - predict_loss: 0.0000e+00 - loss_loss: 0.0549\n",
      "Epoch 238/500\n",
      "16/16 [==============================] - 1s 55ms/step - loss: 0.0541 - predict_loss: 0.0000e+00 - loss_loss: 0.0541\n",
      "Epoch 239/500\n",
      "16/16 [==============================] - 1s 62ms/step - loss: 0.0537 - predict_loss: 0.0000e+00 - loss_loss: 0.0537\n",
      "Epoch 240/500\n",
      "16/16 [==============================] - 1s 77ms/step - loss: 0.0538 - predict_loss: 0.0000e+00 - loss_loss: 0.0538\n",
      "Epoch 241/500\n",
      "16/16 [==============================] - 1s 66ms/step - loss: 0.0546 - predict_loss: 0.0000e+00 - loss_loss: 0.0546\n",
      "Epoch 242/500\n",
      "16/16 [==============================] - 2s 111ms/step - loss: 0.0552 - predict_loss: 0.0000e+00 - loss_loss: 0.0552\n",
      "Epoch 243/500\n",
      "16/16 [==============================] - 1s 68ms/step - loss: 0.0550 - predict_loss: 0.0000e+00 - loss_loss: 0.0550\n",
      "Epoch 244/500\n",
      "16/16 [==============================] - 1s 72ms/step - loss: 0.0541 - predict_loss: 0.0000e+00 - loss_loss: 0.0541\n",
      "Epoch 245/500\n",
      "16/16 [==============================] - 1s 59ms/step - loss: 0.0533 - predict_loss: 0.0000e+00 - loss_loss: 0.0533\n",
      "Epoch 246/500\n",
      "16/16 [==============================] - 1s 57ms/step - loss: 0.0526 - predict_loss: 0.0000e+00 - loss_loss: 0.0526\n",
      "Epoch 247/500\n",
      "16/16 [==============================] - 1s 52ms/step - loss: 0.0521 - predict_loss: 0.0000e+00 - loss_loss: 0.0521\n",
      "Epoch 248/500\n",
      "16/16 [==============================] - 1s 54ms/step - loss: 0.0519 - predict_loss: 0.0000e+00 - loss_loss: 0.0519\n",
      "Epoch 249/500\n",
      "16/16 [==============================] - 1s 67ms/step - loss: 0.0518 - predict_loss: 0.0000e+00 - loss_loss: 0.0518\n",
      "Epoch 250/500\n",
      "16/16 [==============================] - 1s 54ms/step - loss: 0.0517 - predict_loss: 0.0000e+00 - loss_loss: 0.0517\n",
      "Epoch 251/500\n",
      "16/16 [==============================] - 1s 54ms/step - loss: 0.0519 - predict_loss: 0.0000e+00 - loss_loss: 0.0519\n",
      "Epoch 252/500\n",
      "16/16 [==============================] - 1s 54ms/step - loss: 0.0521 - predict_loss: 0.0000e+00 - loss_loss: 0.0521\n",
      "Epoch 253/500\n",
      "16/16 [==============================] - 1s 75ms/step - loss: 0.0523 - predict_loss: 0.0000e+00 - loss_loss: 0.0523\n",
      "Epoch 254/500\n",
      "16/16 [==============================] - 1s 83ms/step - loss: 0.0524 - predict_loss: 0.0000e+00 - loss_loss: 0.0524\n",
      "Epoch 255/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 1s 60ms/step - loss: 0.0524 - predict_loss: 0.0000e+00 - loss_loss: 0.0524\n",
      "Epoch 256/500\n",
      "16/16 [==============================] - 1s 56ms/step - loss: 0.0522 - predict_loss: 0.0000e+00 - loss_loss: 0.0522\n",
      "Epoch 257/500\n",
      "16/16 [==============================] - 1s 57ms/step - loss: 0.0518 - predict_loss: 0.0000e+00 - loss_loss: 0.0518\n",
      "Epoch 258/500\n",
      "16/16 [==============================] - 1s 53ms/step - loss: 0.0515 - predict_loss: 0.0000e+00 - loss_loss: 0.0515\n",
      "Epoch 259/500\n",
      "16/16 [==============================] - 1s 65ms/step - loss: 0.0513 - predict_loss: 0.0000e+00 - loss_loss: 0.0513\n",
      "Epoch 260/500\n",
      "16/16 [==============================] - 1s 61ms/step - loss: 0.0512 - predict_loss: 0.0000e+00 - loss_loss: 0.0512\n",
      "Epoch 261/500\n",
      "16/16 [==============================] - 1s 59ms/step - loss: 0.0512 - predict_loss: 0.0000e+00 - loss_loss: 0.0512\n",
      "Epoch 262/500\n",
      "16/16 [==============================] - 1s 53ms/step - loss: 0.0513 - predict_loss: 0.0000e+00 - loss_loss: 0.0513\n",
      "Epoch 263/500\n",
      "16/16 [==============================] - 1s 52ms/step - loss: 0.0515 - predict_loss: 0.0000e+00 - loss_loss: 0.0515\n",
      "Epoch 264/500\n",
      "16/16 [==============================] - 1s 68ms/step - loss: 0.0519 - predict_loss: 0.0000e+00 - loss_loss: 0.0519\n",
      "Epoch 265/500\n",
      "16/16 [==============================] - 1s 66ms/step - loss: 0.0524 - predict_loss: 0.0000e+00 - loss_loss: 0.0524\n",
      "Epoch 266/500\n",
      "16/16 [==============================] - 1s 58ms/step - loss: 0.0531 - predict_loss: 0.0000e+00 - loss_loss: 0.0531\n",
      "Epoch 267/500\n",
      "16/16 [==============================] - 1s 55ms/step - loss: 0.0534 - predict_loss: 0.0000e+00 - loss_loss: 0.0534\n",
      "Epoch 268/500\n",
      "16/16 [==============================] - 1s 54ms/step - loss: 0.0535 - predict_loss: 0.0000e+00 - loss_loss: 0.0535\n",
      "Epoch 269/500\n",
      "16/16 [==============================] - 4s 237ms/step - loss: 0.0529 - predict_loss: 0.0000e+00 - loss_loss: 0.0529\n",
      "Epoch 270/500\n",
      "16/16 [==============================] - 1s 74ms/step - loss: 0.0521 - predict_loss: 0.0000e+00 - loss_loss: 0.0521\n",
      "Epoch 271/500\n",
      "16/16 [==============================] - 1s 80ms/step - loss: 0.0511 - predict_loss: 0.0000e+00 - loss_loss: 0.0511\n",
      "Epoch 272/500\n",
      "16/16 [==============================] - 4s 229ms/step - loss: 0.0505 - predict_loss: 0.0000e+00 - loss_loss: 0.0505\n",
      "Epoch 273/500\n",
      "16/16 [==============================] - 2s 147ms/step - loss: 0.0500 - predict_loss: 0.0000e+00 - loss_loss: 0.0500\n",
      "Epoch 274/500\n",
      "16/16 [==============================] - 2s 119ms/step - loss: 0.0498 - predict_loss: 0.0000e+00 - loss_loss: 0.0498\n",
      "Epoch 275/500\n",
      "16/16 [==============================] - 1s 93ms/step - loss: 0.0496 - predict_loss: 0.0000e+00 - loss_loss: 0.0496\n",
      "Epoch 276/500\n",
      "16/16 [==============================] - 2s 102ms/step - loss: 0.0496 - predict_loss: 0.0000e+00 - loss_loss: 0.0496\n",
      "Epoch 277/500\n",
      "16/16 [==============================] - 1s 64ms/step - loss: 0.0498 - predict_loss: 0.0000e+00 - loss_loss: 0.0498\n",
      "Epoch 278/500\n",
      "16/16 [==============================] - 3s 160ms/step - loss: 0.0501 - predict_loss: 0.0000e+00 - loss_loss: 0.0501\n",
      "Epoch 279/500\n",
      "16/16 [==============================] - 1s 75ms/step - loss: 0.0504 - predict_loss: 0.0000e+00 - loss_loss: 0.0504\n",
      "Epoch 280/500\n",
      "16/16 [==============================] - 3s 158ms/step - loss: 0.0507 - predict_loss: 0.0000e+00 - loss_loss: 0.0507\n",
      "Epoch 281/500\n",
      "16/16 [==============================] - 1s 39ms/step - loss: 0.0508 - predict_loss: 0.0000e+00 - loss_loss: 0.0508\n",
      "Epoch 282/500\n",
      "16/16 [==============================] - 1s 68ms/step - loss: 0.0505 - predict_loss: 0.0000e+00 - loss_loss: 0.0505\n",
      "Epoch 283/500\n",
      "16/16 [==============================] - 1s 81ms/step - loss: 0.0500 - predict_loss: 0.0000e+00 - loss_loss: 0.0500\n",
      "Epoch 284/500\n",
      "16/16 [==============================] - 3s 177ms/step - loss: 0.0495 - predict_loss: 0.0000e+00 - loss_loss: 0.0495\n",
      "Epoch 285/500\n",
      "16/16 [==============================] - 2s 133ms/step - loss: 0.0492 - predict_loss: 0.0000e+00 - loss_loss: 0.0492\n",
      "Epoch 286/500\n",
      "16/16 [==============================] - 2s 136ms/step - loss: 0.0491 - predict_loss: 0.0000e+00 - loss_loss: 0.0491\n",
      "Epoch 287/500\n",
      "16/16 [==============================] - 2s 97ms/step - loss: 0.0493 - predict_loss: 0.0000e+00 - loss_loss: 0.0493\n",
      "Epoch 288/500\n",
      "16/16 [==============================] - 2s 96ms/step - loss: 0.0499 - predict_loss: 0.0000e+00 - loss_loss: 0.0499\n",
      "Epoch 289/500\n",
      "16/16 [==============================] - 1s 77ms/step - loss: 0.0507 - predict_loss: 0.0000e+00 - loss_loss: 0.0507\n",
      "Epoch 290/500\n",
      "16/16 [==============================] - 1s 65ms/step - loss: 0.0515 - predict_loss: 0.0000e+00 - loss_loss: 0.0515\n",
      "Epoch 291/500\n",
      "16/16 [==============================] - 1s 73ms/step - loss: 0.0512 - predict_loss: 0.0000e+00 - loss_loss: 0.0512\n",
      "Epoch 292/500\n",
      "16/16 [==============================] - 1s 76ms/step - loss: 0.0503 - predict_loss: 0.0000e+00 - loss_loss: 0.0503\n",
      "Epoch 293/500\n",
      "16/16 [==============================] - 1s 71ms/step - loss: 0.0492 - predict_loss: 0.0000e+00 - loss_loss: 0.0492\n",
      "Epoch 294/500\n",
      "16/16 [==============================] - 1s 77ms/step - loss: 0.0486 - predict_loss: 0.0000e+00 - loss_loss: 0.0486\n",
      "Epoch 295/500\n",
      "16/16 [==============================] - 1s 80ms/step - loss: 0.0482 - predict_loss: 0.0000e+00 - loss_loss: 0.0482\n",
      "Epoch 296/500\n",
      "16/16 [==============================] - 3s 214ms/step - loss: 0.0481 - predict_loss: 0.0000e+00 - loss_loss: 0.0481\n",
      "Epoch 297/500\n",
      "16/16 [==============================] - 1s 66ms/step - loss: 0.0483 - predict_loss: 0.0000e+00 - loss_loss: 0.0483\n",
      "Epoch 298/500\n",
      "16/16 [==============================] - 1s 63ms/step - loss: 0.0488 - predict_loss: 0.0000e+00 - loss_loss: 0.0488\n",
      "Epoch 299/500\n",
      "16/16 [==============================] - 1s 62ms/step - loss: 0.0493 - predict_loss: 0.0000e+00 - loss_loss: 0.0493\n",
      "Epoch 300/500\n",
      "16/16 [==============================] - 1s 52ms/step - loss: 0.0496 - predict_loss: 0.0000e+00 - loss_loss: 0.0496\n",
      "Epoch 301/500\n",
      "16/16 [==============================] - 1s 65ms/step - loss: 0.0496 - predict_loss: 0.0000e+00 - loss_loss: 0.0496\n",
      "Epoch 302/500\n",
      "16/16 [==============================] - 1s 59ms/step - loss: 0.0492 - predict_loss: 0.0000e+00 - loss_loss: 0.0492\n",
      "Epoch 303/500\n",
      "16/16 [==============================] - 1s 78ms/step - loss: 0.0489 - predict_loss: 0.0000e+00 - loss_loss: 0.0489\n",
      "Epoch 304/500\n",
      "16/16 [==============================] - 1s 64ms/step - loss: 0.0486 - predict_loss: 0.0000e+00 - loss_loss: 0.0486\n",
      "Epoch 305/500\n",
      "16/16 [==============================] - 1s 64ms/step - loss: 0.0485 - predict_loss: 0.0000e+00 - loss_loss: 0.0485\n",
      "Epoch 306/500\n",
      "16/16 [==============================] - 1s 68ms/step - loss: 0.0484 - predict_loss: 0.0000e+00 - loss_loss: 0.0484\n",
      "Epoch 307/500\n",
      "16/16 [==============================] - 1s 76ms/step - loss: 0.0485 - predict_loss: 0.0000e+00 - loss_loss: 0.0485\n",
      "Epoch 308/500\n",
      "16/16 [==============================] - 1s 55ms/step - loss: 0.0484 - predict_loss: 0.0000e+00 - loss_loss: 0.0484\n",
      "Epoch 309/500\n",
      "16/16 [==============================] - 1s 53ms/step - loss: 0.0482 - predict_loss: 0.0000e+00 - loss_loss: 0.0482\n",
      "Epoch 310/500\n",
      "16/16 [==============================] - 1s 63ms/step - loss: 0.0479 - predict_loss: 0.0000e+00 - loss_loss: 0.0479\n",
      "Epoch 311/500\n",
      "16/16 [==============================] - 1s 71ms/step - loss: 0.0477 - predict_loss: 0.0000e+00 - loss_loss: 0.0477\n",
      "Epoch 312/500\n",
      "16/16 [==============================] - 1s 53ms/step - loss: 0.0475 - predict_loss: 0.0000e+00 - loss_loss: 0.0475\n",
      "Epoch 313/500\n",
      "16/16 [==============================] - 1s 51ms/step - loss: 0.0475 - predict_loss: 0.0000e+00 - loss_loss: 0.0475\n",
      "Epoch 314/500\n",
      "16/16 [==============================] - 1s 52ms/step - loss: 0.0475 - predict_loss: 0.0000e+00 - loss_loss: 0.0475\n",
      "Epoch 315/500\n",
      "16/16 [==============================] - 1s 57ms/step - loss: 0.0475 - predict_loss: 0.0000e+00 - loss_loss: 0.0475\n",
      "Epoch 316/500\n",
      "16/16 [==============================] - 1s 61ms/step - loss: 0.0472 - predict_loss: 0.0000e+00 - loss_loss: 0.0472\n",
      "Epoch 317/500\n",
      "16/16 [==============================] - 1s 54ms/step - loss: 0.0468 - predict_loss: 0.0000e+00 - loss_loss: 0.0468\n",
      "Epoch 318/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 1s 63ms/step - loss: 0.0464 - predict_loss: 0.0000e+00 - loss_loss: 0.0464\n",
      "Epoch 319/500\n",
      "16/16 [==============================] - 1s 49ms/step - loss: 0.0463 - predict_loss: 0.0000e+00 - loss_loss: 0.0463\n",
      "Epoch 320/500\n",
      "16/16 [==============================] - 1s 48ms/step - loss: 0.0465 - predict_loss: 0.0000e+00 - loss_loss: 0.0465\n",
      "Epoch 321/500\n",
      "16/16 [==============================] - 1s 56ms/step - loss: 0.0472 - predict_loss: 0.0000e+00 - loss_loss: 0.0472\n",
      "Epoch 322/500\n",
      "16/16 [==============================] - 1s 67ms/step - loss: 0.0483 - predict_loss: 0.0000e+00 - loss_loss: 0.0483\n",
      "Epoch 323/500\n",
      "16/16 [==============================] - 2s 103ms/step - loss: 0.0489 - predict_loss: 0.0000e+00 - loss_loss: 0.0489\n",
      "Epoch 324/500\n",
      "16/16 [==============================] - 1s 52ms/step - loss: 0.0486 - predict_loss: 0.0000e+00 - loss_loss: 0.0486\n",
      "Epoch 325/500\n",
      "16/16 [==============================] - 1s 73ms/step - loss: 0.0482 - predict_loss: 0.0000e+00 - loss_loss: 0.0482\n",
      "Epoch 326/500\n",
      "16/16 [==============================] - 2s 107ms/step - loss: 0.0481 - predict_loss: 0.0000e+00 - loss_loss: 0.0481\n",
      "Epoch 327/500\n",
      "16/16 [==============================] - 1s 72ms/step - loss: 0.0481 - predict_loss: 0.0000e+00 - loss_loss: 0.0481\n",
      "Epoch 328/500\n",
      "16/16 [==============================] - 1s 54ms/step - loss: 0.0481 - predict_loss: 0.0000e+00 - loss_loss: 0.0481\n",
      "Epoch 329/500\n",
      "16/16 [==============================] - 1s 65ms/step - loss: 0.0475 - predict_loss: 0.0000e+00 - loss_loss: 0.0475\n",
      "Epoch 330/500\n",
      "16/16 [==============================] - 1s 71ms/step - loss: 0.0469 - predict_loss: 0.0000e+00 - loss_loss: 0.0469\n",
      "Epoch 331/500\n",
      "16/16 [==============================] - 1s 61ms/step - loss: 0.0462 - predict_loss: 0.0000e+00 - loss_loss: 0.0462\n",
      "Epoch 332/500\n",
      "16/16 [==============================] - 1s 70ms/step - loss: 0.0458 - predict_loss: 0.0000e+00 - loss_loss: 0.0458\n",
      "Epoch 333/500\n",
      "16/16 [==============================] - 1s 70ms/step - loss: 0.0454 - predict_loss: 0.0000e+00 - loss_loss: 0.0454\n",
      "Epoch 334/500\n",
      "16/16 [==============================] - 1s 70ms/step - loss: 0.0453 - predict_loss: 0.0000e+00 - loss_loss: 0.0453\n",
      "Epoch 335/500\n",
      "16/16 [==============================] - 1s 90ms/step - loss: 0.0452 - predict_loss: 0.0000e+00 - loss_loss: 0.0452\n",
      "Epoch 336/500\n",
      "16/16 [==============================] - 1s 79ms/step - loss: 0.0453 - predict_loss: 0.0000e+00 - loss_loss: 0.0453\n",
      "Epoch 337/500\n",
      "16/16 [==============================] - 1s 71ms/step - loss: 0.0455 - predict_loss: 0.0000e+00 - loss_loss: 0.0455\n",
      "Epoch 338/500\n",
      "16/16 [==============================] - 2s 95ms/step - loss: 0.0460 - predict_loss: 0.0000e+00 - loss_loss: 0.0460\n",
      "Epoch 339/500\n",
      "16/16 [==============================] - 1s 73ms/step - loss: 0.0465 - predict_loss: 0.0000e+00 - loss_loss: 0.0465\n",
      "Epoch 340/500\n",
      "16/16 [==============================] - 1s 59ms/step - loss: 0.0467 - predict_loss: 0.0000e+00 - loss_loss: 0.0467\n",
      "Epoch 341/500\n",
      "16/16 [==============================] - 1s 64ms/step - loss: 0.0463 - predict_loss: 0.0000e+00 - loss_loss: 0.0463\n",
      "Epoch 342/500\n",
      "16/16 [==============================] - 1s 63ms/step - loss: 0.0458 - predict_loss: 0.0000e+00 - loss_loss: 0.0458\n",
      "Epoch 343/500\n",
      "16/16 [==============================] - 1s 68ms/step - loss: 0.0453 - predict_loss: 0.0000e+00 - loss_loss: 0.0453\n",
      "Epoch 344/500\n",
      "16/16 [==============================] - 2s 133ms/step - loss: 0.0450 - predict_loss: 0.0000e+00 - loss_loss: 0.0450\n",
      "Epoch 345/500\n",
      "16/16 [==============================] - 1s 78ms/step - loss: 0.0449 - predict_loss: 0.0000e+00 - loss_loss: 0.0449\n",
      "Epoch 346/500\n",
      "16/16 [==============================] - 1s 82ms/step - loss: 0.0449 - predict_loss: 0.0000e+00 - loss_loss: 0.0449\n",
      "Epoch 347/500\n",
      "16/16 [==============================] - 1s 64ms/step - loss: 0.0452 - predict_loss: 0.0000e+00 - loss_loss: 0.0452\n",
      "Epoch 348/500\n",
      "16/16 [==============================] - 1s 76ms/step - loss: 0.0460 - predict_loss: 0.0000e+00 - loss_loss: 0.0460\n",
      "Epoch 349/500\n",
      "16/16 [==============================] - 1s 64ms/step - loss: 0.0474 - predict_loss: 0.0000e+00 - loss_loss: 0.0474\n",
      "Epoch 350/500\n",
      "16/16 [==============================] - 2s 139ms/step - loss: 0.0488 - predict_loss: 0.0000e+00 - loss_loss: 0.0488\n",
      "Epoch 351/500\n",
      "16/16 [==============================] - 1s 64ms/step - loss: 0.0480 - predict_loss: 0.0000e+00 - loss_loss: 0.0480\n",
      "Epoch 352/500\n",
      "16/16 [==============================] - 1s 58ms/step - loss: 0.0464 - predict_loss: 0.0000e+00 - loss_loss: 0.0464\n",
      "Epoch 353/500\n",
      "16/16 [==============================] - 1s 86ms/step - loss: 0.0449 - predict_loss: 0.0000e+00 - loss_loss: 0.0449\n",
      "Epoch 354/500\n",
      "16/16 [==============================] - 1s 82ms/step - loss: 0.0442 - predict_loss: 0.0000e+00 - loss_loss: 0.0442\n",
      "Epoch 355/500\n",
      "16/16 [==============================] - 1s 74ms/step - loss: 0.0438 - predict_loss: 0.0000e+00 - loss_loss: 0.0438\n",
      "Epoch 356/500\n",
      "16/16 [==============================] - 1s 47ms/step - loss: 0.0437 - predict_loss: 0.0000e+00 - loss_loss: 0.0437\n",
      "Epoch 357/500\n",
      "16/16 [==============================] - 1s 37ms/step - loss: 0.0438 - predict_loss: 0.0000e+00 - loss_loss: 0.0438\n",
      "Epoch 358/500\n",
      "16/16 [==============================] - 1s 38ms/step - loss: 0.0440 - predict_loss: 0.0000e+00 - loss_loss: 0.0440\n",
      "Epoch 359/500\n",
      "16/16 [==============================] - 1s 38ms/step - loss: 0.0445 - predict_loss: 0.0000e+00 - loss_loss: 0.0445\n",
      "Epoch 360/500\n",
      "16/16 [==============================] - 1s 38ms/step - loss: 0.0449 - predict_loss: 0.0000e+00 - loss_loss: 0.0449\n",
      "Epoch 361/500\n",
      "16/16 [==============================] - 1s 53ms/step - loss: 0.0452 - predict_loss: 0.0000e+00 - loss_loss: 0.0452\n",
      "Epoch 362/500\n",
      "16/16 [==============================] - 1s 53ms/step - loss: 0.0451 - predict_loss: 0.0000e+00 - loss_loss: 0.0451\n",
      "Epoch 363/500\n",
      "16/16 [==============================] - 2s 95ms/step - loss: 0.0450 - predict_loss: 0.0000e+00 - loss_loss: 0.0450\n",
      "Epoch 364/500\n",
      "16/16 [==============================] - 1s 69ms/step - loss: 0.0448 - predict_loss: 0.0000e+00 - loss_loss: 0.0448\n",
      "Epoch 365/500\n",
      "16/16 [==============================] - 1s 53ms/step - loss: 0.0449 - predict_loss: 0.0000e+00 - loss_loss: 0.0449\n",
      "Epoch 366/500\n",
      "16/16 [==============================] - 2s 110ms/step - loss: 0.0450 - predict_loss: 0.0000e+00 - loss_loss: 0.0450\n",
      "Epoch 367/500\n",
      "16/16 [==============================] - 2s 97ms/step - loss: 0.0451 - predict_loss: 0.0000e+00 - loss_loss: 0.0451\n",
      "Epoch 368/500\n",
      "16/16 [==============================] - 1s 68ms/step - loss: 0.0449 - predict_loss: 0.0000e+00 - loss_loss: 0.0449\n",
      "Epoch 369/500\n",
      "16/16 [==============================] - 1s 42ms/step - loss: 0.0447 - predict_loss: 0.0000e+00 - loss_loss: 0.0447\n",
      "Epoch 370/500\n",
      "16/16 [==============================] - 2s 105ms/step - loss: 0.0441 - predict_loss: 0.0000e+00 - loss_loss: 0.0441\n",
      "Epoch 371/500\n",
      "16/16 [==============================] - 4s 223ms/step - loss: 0.0437 - predict_loss: 0.0000e+00 - loss_loss: 0.0437\n",
      "Epoch 372/500\n",
      "16/16 [==============================] - 1s 92ms/step - loss: 0.0433 - predict_loss: 0.0000e+00 - loss_loss: 0.0433\n",
      "Epoch 373/500\n",
      "16/16 [==============================] - 1s 89ms/step - loss: 0.0431 - predict_loss: 0.0000e+00 - loss_loss: 0.0431\n",
      "Epoch 374/500\n",
      "16/16 [==============================] - 1s 87ms/step - loss: 0.0430 - predict_loss: 0.0000e+00 - loss_loss: 0.0430\n",
      "Epoch 375/500\n",
      "16/16 [==============================] - 1s 61ms/step - loss: 0.0431 - predict_loss: 0.0000e+00 - loss_loss: 0.0431\n",
      "Epoch 376/500\n",
      "16/16 [==============================] - 1s 84ms/step - loss: 0.0433 - predict_loss: 0.0000e+00 - loss_loss: 0.0433\n",
      "Epoch 377/500\n",
      "16/16 [==============================] - 2s 129ms/step - loss: 0.0438 - predict_loss: 0.0000e+00 - loss_loss: 0.0438\n",
      "Epoch 378/500\n",
      "16/16 [==============================] - 1s 60ms/step - loss: 0.0441 - predict_loss: 0.0000e+00 - loss_loss: 0.0441\n",
      "Epoch 379/500\n",
      "16/16 [==============================] - 1s 69ms/step - loss: 0.0439 - predict_loss: 0.0000e+00 - loss_loss: 0.0439\n",
      "Epoch 380/500\n",
      "16/16 [==============================] - 1s 74ms/step - loss: 0.0433 - predict_loss: 0.0000e+00 - loss_loss: 0.0433\n",
      "Epoch 381/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 1s 55ms/step - loss: 0.0427 - predict_loss: 0.0000e+00 - loss_loss: 0.0427\n",
      "Epoch 382/500\n",
      "16/16 [==============================] - 1s 61ms/step - loss: 0.0424 - predict_loss: 0.0000e+00 - loss_loss: 0.0424\n",
      "Epoch 383/500\n",
      "16/16 [==============================] - 1s 80ms/step - loss: 0.0424 - predict_loss: 0.0000e+00 - loss_loss: 0.0424\n",
      "Epoch 384/500\n",
      "16/16 [==============================] - 1s 70ms/step - loss: 0.0428 - predict_loss: 0.0000e+00 - loss_loss: 0.0428\n",
      "Epoch 385/500\n",
      "16/16 [==============================] - 1s 58ms/step - loss: 0.0441 - predict_loss: 0.0000e+00 - loss_loss: 0.0441\n",
      "Epoch 386/500\n",
      "16/16 [==============================] - 1s 51ms/step - loss: 0.0463 - predict_loss: 0.0000e+00 - loss_loss: 0.0463\n",
      "Epoch 387/500\n",
      "16/16 [==============================] - 1s 62ms/step - loss: 0.0473 - predict_loss: 0.0000e+00 - loss_loss: 0.0473\n",
      "Epoch 388/500\n",
      "16/16 [==============================] - 2s 111ms/step - loss: 0.0455 - predict_loss: 0.0000e+00 - loss_loss: 0.0455\n",
      "Epoch 389/500\n",
      "16/16 [==============================] - 1s 86ms/step - loss: 0.0435 - predict_loss: 0.0000e+00 - loss_loss: 0.0435\n",
      "Epoch 390/500\n",
      "16/16 [==============================] - 1s 68ms/step - loss: 0.0422 - predict_loss: 0.0000e+00 - loss_loss: 0.0422\n",
      "Epoch 391/500\n",
      "16/16 [==============================] - 1s 67ms/step - loss: 0.0416 - predict_loss: 0.0000e+00 - loss_loss: 0.0416\n",
      "Epoch 392/500\n",
      "16/16 [==============================] - 1s 60ms/step - loss: 0.0413 - predict_loss: 0.0000e+00 - loss_loss: 0.0413\n",
      "Epoch 393/500\n",
      "16/16 [==============================] - 1s 73ms/step - loss: 0.0411 - predict_loss: 0.0000e+00 - loss_loss: 0.0411\n",
      "Epoch 394/500\n",
      "16/16 [==============================] - 1s 57ms/step - loss: 0.0412 - predict_loss: 0.0000e+00 - loss_loss: 0.0412\n",
      "Epoch 395/500\n",
      "16/16 [==============================] - 1s 68ms/step - loss: 0.0414 - predict_loss: 0.0000e+00 - loss_loss: 0.0414\n",
      "Epoch 396/500\n",
      "16/16 [==============================] - 1s 64ms/step - loss: 0.0418 - predict_loss: 0.0000e+00 - loss_loss: 0.0418\n",
      "Epoch 397/500\n",
      "16/16 [==============================] - 1s 74ms/step - loss: 0.0425 - predict_loss: 0.0000e+00 - loss_loss: 0.0425\n",
      "Epoch 398/500\n",
      "16/16 [==============================] - 1s 90ms/step - loss: 0.0432 - predict_loss: 0.0000e+00 - loss_loss: 0.0432\n",
      "Epoch 399/500\n",
      "16/16 [==============================] - 1s 71ms/step - loss: 0.0435 - predict_loss: 0.0000e+00 - loss_loss: 0.0435\n",
      "Epoch 400/500\n",
      "16/16 [==============================] - 1s 63ms/step - loss: 0.0433 - predict_loss: 0.0000e+00 - loss_loss: 0.0433\n",
      "Epoch 401/500\n",
      "16/16 [==============================] - 1s 63ms/step - loss: 0.0430 - predict_loss: 0.0000e+00 - loss_loss: 0.0430\n",
      "Epoch 402/500\n",
      "16/16 [==============================] - 1s 72ms/step - loss: 0.0430 - predict_loss: 0.0000e+00 - loss_loss: 0.0430\n",
      "Epoch 403/500\n",
      "16/16 [==============================] - 1s 79ms/step - loss: 0.0430 - predict_loss: 0.0000e+00 - loss_loss: 0.0430\n",
      "Epoch 404/500\n",
      "16/16 [==============================] - 1s 71ms/step - loss: 0.0431 - predict_loss: 0.0000e+00 - loss_loss: 0.0431\n",
      "Epoch 405/500\n",
      "16/16 [==============================] - 2s 111ms/step - loss: 0.0431 - predict_loss: 0.0000e+00 - loss_loss: 0.0431\n",
      "Epoch 406/500\n",
      "16/16 [==============================] - 1s 70ms/step - loss: 0.0431 - predict_loss: 0.0000e+00 - loss_loss: 0.0431\n",
      "Epoch 407/500\n",
      "16/16 [==============================] - 1s 57ms/step - loss: 0.0427 - predict_loss: 0.0000e+00 - loss_loss: 0.0427\n",
      "Epoch 408/500\n",
      "16/16 [==============================] - 1s 54ms/step - loss: 0.0422 - predict_loss: 0.0000e+00 - loss_loss: 0.0422\n",
      "Epoch 409/500\n",
      "16/16 [==============================] - 1s 64ms/step - loss: 0.0416 - predict_loss: 0.0000e+00 - loss_loss: 0.0416\n",
      "Epoch 410/500\n",
      "16/16 [==============================] - 1s 67ms/step - loss: 0.0413 - predict_loss: 0.0000e+00 - loss_loss: 0.0413\n",
      "Epoch 411/500\n",
      "16/16 [==============================] - 1s 77ms/step - loss: 0.0409 - predict_loss: 0.0000e+00 - loss_loss: 0.0409\n",
      "Epoch 412/500\n",
      "16/16 [==============================] - 1s 57ms/step - loss: 0.0408 - predict_loss: 0.0000e+00 - loss_loss: 0.0408\n",
      "Epoch 413/500\n",
      "16/16 [==============================] - 1s 60ms/step - loss: 0.0406 - predict_loss: 0.0000e+00 - loss_loss: 0.0406\n",
      "Epoch 414/500\n",
      "16/16 [==============================] - 1s 68ms/step - loss: 0.0406 - predict_loss: 0.0000e+00 - loss_loss: 0.0406\n",
      "Epoch 415/500\n",
      "16/16 [==============================] - 1s 54ms/step - loss: 0.0406 - predict_loss: 0.0000e+00 - loss_loss: 0.0406\n",
      "Epoch 416/500\n",
      "16/16 [==============================] - 1s 82ms/step - loss: 0.0408 - predict_loss: 0.0000e+00 - loss_loss: 0.0408\n",
      "Epoch 417/500\n",
      "16/16 [==============================] - 1s 62ms/step - loss: 0.0408 - predict_loss: 0.0000e+00 - loss_loss: 0.0408\n",
      "Epoch 418/500\n",
      "16/16 [==============================] - 2s 98ms/step - loss: 0.0410 - predict_loss: 0.0000e+00 - loss_loss: 0.0410\n",
      "Epoch 419/500\n",
      "16/16 [==============================] - 1s 75ms/step - loss: 0.0412 - predict_loss: 0.0000e+00 - loss_loss: 0.0412\n",
      "Epoch 420/500\n",
      "16/16 [==============================] - 2s 106ms/step - loss: 0.0418 - predict_loss: 0.0000e+00 - loss_loss: 0.0418\n",
      "Epoch 421/500\n",
      "16/16 [==============================] - 2s 116ms/step - loss: 0.0427 - predict_loss: 0.0000e+00 - loss_loss: 0.0427\n",
      "Epoch 422/500\n",
      "16/16 [==============================] - 1s 58ms/step - loss: 0.0439 - predict_loss: 0.0000e+00 - loss_loss: 0.0439\n",
      "Epoch 423/500\n",
      "16/16 [==============================] - 2s 94ms/step - loss: 0.0437 - predict_loss: 0.0000e+00 - loss_loss: 0.0437\n",
      "Epoch 424/500\n",
      "16/16 [==============================] - 1s 77ms/step - loss: 0.0424 - predict_loss: 0.0000e+00 - loss_loss: 0.0424\n",
      "Epoch 425/500\n",
      "16/16 [==============================] - 1s 60ms/step - loss: 0.0410 - predict_loss: 0.0000e+00 - loss_loss: 0.0410\n",
      "Epoch 426/500\n",
      "16/16 [==============================] - 1s 50ms/step - loss: 0.0401 - predict_loss: 0.0000e+00 - loss_loss: 0.0401\n",
      "Epoch 427/500\n",
      "16/16 [==============================] - 1s 85ms/step - loss: 0.0397 - predict_loss: 0.0000e+00 - loss_loss: 0.0397\n",
      "Epoch 428/500\n",
      "16/16 [==============================] - 1s 66ms/step - loss: 0.0395 - predict_loss: 0.0000e+00 - loss_loss: 0.0395\n",
      "Epoch 429/500\n",
      "16/16 [==============================] - 1s 61ms/step - loss: 0.0395 - predict_loss: 0.0000e+00 - loss_loss: 0.0395\n",
      "Epoch 430/500\n",
      "16/16 [==============================] - 1s 54ms/step - loss: 0.0399 - predict_loss: 0.0000e+00 - loss_loss: 0.0399\n",
      "Epoch 431/500\n",
      "16/16 [==============================] - 1s 56ms/step - loss: 0.0406 - predict_loss: 0.0000e+00 - loss_loss: 0.0406\n",
      "Epoch 432/500\n",
      "16/16 [==============================] - 2s 113ms/step - loss: 0.0414 - predict_loss: 0.0000e+00 - loss_loss: 0.0414\n",
      "Epoch 433/500\n",
      "16/16 [==============================] - 1s 53ms/step - loss: 0.0420 - predict_loss: 0.0000e+00 - loss_loss: 0.0420\n",
      "Epoch 434/500\n",
      "16/16 [==============================] - 1s 52ms/step - loss: 0.0419 - predict_loss: 0.0000e+00 - loss_loss: 0.0419\n",
      "Epoch 435/500\n",
      "16/16 [==============================] - 1s 55ms/step - loss: 0.0416 - predict_loss: 0.0000e+00 - loss_loss: 0.0416\n",
      "Epoch 436/500\n",
      "16/16 [==============================] - 1s 55ms/step - loss: 0.0411 - predict_loss: 0.0000e+00 - loss_loss: 0.0411\n",
      "Epoch 437/500\n",
      "16/16 [==============================] - 1s 84ms/step - loss: 0.0409 - predict_loss: 0.0000e+00 - loss_loss: 0.0409\n",
      "Epoch 438/500\n",
      "16/16 [==============================] - 1s 87ms/step - loss: 0.0406 - predict_loss: 0.0000e+00 - loss_loss: 0.0406\n",
      "Epoch 439/500\n",
      "16/16 [==============================] - 1s 72ms/step - loss: 0.0404 - predict_loss: 0.0000e+00 - loss_loss: 0.0404\n",
      "Epoch 440/500\n",
      "16/16 [==============================] - 1s 60ms/step - loss: 0.0402 - predict_loss: 0.0000e+00 - loss_loss: 0.0402\n",
      "Epoch 441/500\n",
      "16/16 [==============================] - 1s 62ms/step - loss: 0.0401 - predict_loss: 0.0000e+00 - loss_loss: 0.0401\n",
      "Epoch 442/500\n",
      "16/16 [==============================] - 1s 72ms/step - loss: 0.0399 - predict_loss: 0.0000e+00 - loss_loss: 0.0399\n",
      "Epoch 443/500\n",
      "16/16 [==============================] - 1s 58ms/step - loss: 0.0397 - predict_loss: 0.0000e+00 - loss_loss: 0.0397\n",
      "Epoch 444/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 1s 51ms/step - loss: 0.0395 - predict_loss: 0.0000e+00 - loss_loss: 0.0395\n",
      "Epoch 445/500\n",
      "16/16 [==============================] - 1s 58ms/step - loss: 0.0393 - predict_loss: 0.0000e+00 - loss_loss: 0.0393\n",
      "Epoch 446/500\n",
      "16/16 [==============================] - 1s 72ms/step - loss: 0.0392 - predict_loss: 0.0000e+00 - loss_loss: 0.0392\n",
      "Epoch 447/500\n",
      "16/16 [==============================] - 1s 85ms/step - loss: 0.0392 - predict_loss: 0.0000e+00 - loss_loss: 0.0392\n",
      "Epoch 448/500\n",
      "16/16 [==============================] - 1s 63ms/step - loss: 0.0393 - predict_loss: 0.0000e+00 - loss_loss: 0.0393\n",
      "Epoch 449/500\n",
      "16/16 [==============================] - 1s 77ms/step - loss: 0.0395 - predict_loss: 0.0000e+00 - loss_loss: 0.0395\n",
      "Epoch 450/500\n",
      "16/16 [==============================] - 1s 91ms/step - loss: 0.0397 - predict_loss: 0.0000e+00 - loss_loss: 0.0397\n",
      "Epoch 451/500\n",
      "16/16 [==============================] - 1s 89ms/step - loss: 0.0396 - predict_loss: 0.0000e+00 - loss_loss: 0.0396\n",
      "Epoch 452/500\n",
      "16/16 [==============================] - 1s 77ms/step - loss: 0.0392 - predict_loss: 0.0000e+00 - loss_loss: 0.0392\n",
      "Epoch 453/500\n",
      "16/16 [==============================] - 1s 57ms/step - loss: 0.0388 - predict_loss: 0.0000e+00 - loss_loss: 0.0388\n",
      "Epoch 454/500\n",
      "16/16 [==============================] - 1s 57ms/step - loss: 0.0388 - predict_loss: 0.0000e+00 - loss_loss: 0.0388\n",
      "Epoch 455/500\n",
      "16/16 [==============================] - 1s 71ms/step - loss: 0.0392 - predict_loss: 0.0000e+00 - loss_loss: 0.0392\n",
      "Epoch 456/500\n",
      "16/16 [==============================] - 1s 50ms/step - loss: 0.0406 - predict_loss: 0.0000e+00 - loss_loss: 0.0406\n",
      "Epoch 457/500\n",
      "16/16 [==============================] - 1s 60ms/step - loss: 0.0418 - predict_loss: 0.0000e+00 - loss_loss: 0.0418\n",
      "Epoch 458/500\n",
      "16/16 [==============================] - 1s 61ms/step - loss: 0.0421 - predict_loss: 0.0000e+00 - loss_loss: 0.0421\n",
      "Epoch 459/500\n",
      "16/16 [==============================] - 2s 125ms/step - loss: 0.0414 - predict_loss: 0.0000e+00 - loss_loss: 0.0414\n",
      "Epoch 460/500\n",
      "16/16 [==============================] - 1s 61ms/step - loss: 0.0408 - predict_loss: 0.0000e+00 - loss_loss: 0.0408\n",
      "Epoch 461/500\n",
      "16/16 [==============================] - 1s 53ms/step - loss: 0.0400 - predict_loss: 0.0000e+00 - loss_loss: 0.0400\n",
      "Epoch 462/500\n",
      "16/16 [==============================] - 1s 56ms/step - loss: 0.0394 - predict_loss: 0.0000e+00 - loss_loss: 0.0394\n",
      "Epoch 463/500\n",
      "16/16 [==============================] - 1s 53ms/step - loss: 0.0388 - predict_loss: 0.0000e+00 - loss_loss: 0.0388\n",
      "Epoch 464/500\n",
      "16/16 [==============================] - 1s 65ms/step - loss: 0.0384 - predict_loss: 0.0000e+00 - loss_loss: 0.0384\n",
      "Epoch 465/500\n",
      "16/16 [==============================] - 1s 62ms/step - loss: 0.0381 - predict_loss: 0.0000e+00 - loss_loss: 0.0381\n",
      "Epoch 466/500\n",
      "16/16 [==============================] - 1s 63ms/step - loss: 0.0379 - predict_loss: 0.0000e+00 - loss_loss: 0.0379\n",
      "Epoch 467/500\n",
      "16/16 [==============================] - 1s 74ms/step - loss: 0.0378 - predict_loss: 0.0000e+00 - loss_loss: 0.0378\n",
      "Epoch 468/500\n",
      "16/16 [==============================] - 1s 67ms/step - loss: 0.0378 - predict_loss: 0.0000e+00 - loss_loss: 0.0378\n",
      "Epoch 469/500\n",
      "16/16 [==============================] - 1s 78ms/step - loss: 0.0379 - predict_loss: 0.0000e+00 - loss_loss: 0.0379\n",
      "Epoch 470/500\n",
      "16/16 [==============================] - 1s 59ms/step - loss: 0.0381 - predict_loss: 0.0000e+00 - loss_loss: 0.0381\n",
      "Epoch 471/500\n",
      "16/16 [==============================] - 1s 54ms/step - loss: 0.0383 - predict_loss: 0.0000e+00 - loss_loss: 0.0383\n",
      "Epoch 472/500\n",
      "16/16 [==============================] - 1s 54ms/step - loss: 0.0387 - predict_loss: 0.0000e+00 - loss_loss: 0.0387\n",
      "Epoch 473/500\n",
      "16/16 [==============================] - 1s 54ms/step - loss: 0.0391 - predict_loss: 0.0000e+00 - loss_loss: 0.0391\n",
      "Epoch 474/500\n",
      "16/16 [==============================] - 1s 67ms/step - loss: 0.0393 - predict_loss: 0.0000e+00 - loss_loss: 0.0393\n",
      "Epoch 475/500\n",
      "16/16 [==============================] - 1s 59ms/step - loss: 0.0389 - predict_loss: 0.0000e+00 - loss_loss: 0.0389\n",
      "Epoch 476/500\n",
      "16/16 [==============================] - 1s 50ms/step - loss: 0.0383 - predict_loss: 0.0000e+00 - loss_loss: 0.0383\n",
      "Epoch 477/500\n",
      "16/16 [==============================] - 1s 51ms/step - loss: 0.0379 - predict_loss: 0.0000e+00 - loss_loss: 0.0379\n",
      "Epoch 478/500\n",
      "16/16 [==============================] - 1s 56ms/step - loss: 0.0381 - predict_loss: 0.0000e+00 - loss_loss: 0.0381\n",
      "Epoch 479/500\n",
      "16/16 [==============================] - 1s 56ms/step - loss: 0.0390 - predict_loss: 0.0000e+00 - loss_loss: 0.0390\n",
      "Epoch 480/500\n",
      "16/16 [==============================] - 1s 64ms/step - loss: 0.0405 - predict_loss: 0.0000e+00 - loss_loss: 0.0405\n",
      "Epoch 481/500\n",
      "16/16 [==============================] - 1s 82ms/step - loss: 0.0409 - predict_loss: 0.0000e+00 - loss_loss: 0.0409\n",
      "Epoch 482/500\n",
      "16/16 [==============================] - 1s 54ms/step - loss: 0.0399 - predict_loss: 0.0000e+00 - loss_loss: 0.0399\n",
      "Epoch 483/500\n",
      "16/16 [==============================] - 1s 62ms/step - loss: 0.0384 - predict_loss: 0.0000e+00 - loss_loss: 0.0384\n",
      "Epoch 484/500\n",
      "16/16 [==============================] - 1s 67ms/step - loss: 0.0374 - predict_loss: 0.0000e+00 - loss_loss: 0.0374\n",
      "Epoch 485/500\n",
      "16/16 [==============================] - 1s 66ms/step - loss: 0.0368 - predict_loss: 0.0000e+00 - loss_loss: 0.0368\n",
      "Epoch 486/500\n",
      "16/16 [==============================] - 1s 66ms/step - loss: 0.0365 - predict_loss: 0.0000e+00 - loss_loss: 0.0365\n",
      "Epoch 487/500\n",
      "16/16 [==============================] - 2s 119ms/step - loss: 0.0363 - predict_loss: 0.0000e+00 - loss_loss: 0.0363\n",
      "Epoch 488/500\n",
      "16/16 [==============================] - 1s 72ms/step - loss: 0.0363 - predict_loss: 0.0000e+00 - loss_loss: 0.0363\n",
      "Epoch 489/500\n",
      "16/16 [==============================] - 1s 71ms/step - loss: 0.0365 - predict_loss: 0.0000e+00 - loss_loss: 0.0365\n",
      "Epoch 490/500\n",
      "16/16 [==============================] - 1s 55ms/step - loss: 0.0369 - predict_loss: 0.0000e+00 - loss_loss: 0.0369\n",
      "Epoch 491/500\n",
      "16/16 [==============================] - 1s 54ms/step - loss: 0.0374 - predict_loss: 0.0000e+00 - loss_loss: 0.0374\n",
      "Epoch 492/500\n",
      "16/16 [==============================] - 1s 48ms/step - loss: 0.0382 - predict_loss: 0.0000e+00 - loss_loss: 0.0382\n",
      "Epoch 493/500\n",
      "16/16 [==============================] - 1s 53ms/step - loss: 0.0390 - predict_loss: 0.0000e+00 - loss_loss: 0.0390\n",
      "Epoch 494/500\n",
      "16/16 [==============================] - 1s 69ms/step - loss: 0.0395 - predict_loss: 0.0000e+00 - loss_loss: 0.0395\n",
      "Epoch 495/500\n",
      "16/16 [==============================] - 1s 54ms/step - loss: 0.0393 - predict_loss: 0.0000e+00 - loss_loss: 0.0393\n",
      "Epoch 496/500\n",
      "16/16 [==============================] - 1s 53ms/step - loss: 0.0387 - predict_loss: 0.0000e+00 - loss_loss: 0.0387\n",
      "Epoch 497/500\n",
      "16/16 [==============================] - 1s 57ms/step - loss: 0.0384 - predict_loss: 0.0000e+00 - loss_loss: 0.0384\n",
      "Epoch 498/500\n",
      "16/16 [==============================] - 1s 62ms/step - loss: 0.0382 - predict_loss: 0.0000e+00 - loss_loss: 0.0382\n",
      "Epoch 499/500\n",
      "16/16 [==============================] - 1s 93ms/step - loss: 0.0382 - predict_loss: 0.0000e+00 - loss_loss: 0.0382\n",
      "Epoch 500/500\n",
      "16/16 [==============================] - 1s 60ms/step - loss: 0.0379 - predict_loss: 0.0000e+00 - loss_loss: 0.0379\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x13649f208>"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# finetuning\n",
    "model.compile(optimizer=optimizers.RMSprop(lr = 1e-5, clipvalue = 10.),\n",
    "              loss= {\n",
    "                    'predict': lambda _, loss: loss - loss,\n",
    "                      'loss': lambda _, loss: loss\n",
    "                })\n",
    "# and train\n",
    "model.fit(\n",
    "    [input_batch_padded,target_batch_padded],\n",
    "    [np.zeros((BATCH_SIZE,TOTAL_LENGTH,2)),np.zeros(BATCH_SIZE)],\n",
    "    epochs = 500,\n",
    "    callbacks = get_callbacks(input_batch_padded,target_batch_padded, True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Function that takes a \"close\" look of each tragectory\n",
    "    Input:\n",
    "        model: trained Keras model\n",
    "        input: the batch of input of shape (N, input_length + target_length, 2)\n",
    "        target: the batch of input of the same shape with input\n",
    "        input_length: self-explanatory\n",
    "    Output:\n",
    "        Nothing, but generate N pictures of the plot with:\n",
    "            target tragectories (should be input_length + target_length long) in black,\n",
    "            predicted tragectories (shoudl be target_length long) in blue\n",
    "'''\n",
    "def close_visualize(model, input_batch,target_batch,input_length):\n",
    "    if not input_batch.shape == target_batch.shape:\n",
    "        raise ValueError(\"input batch and target batch should have the same size\")\n",
    "    batch_size,_,__ = input_batch.shape\n",
    "    \n",
    "    prediction, loss = model.predict([input_batch,target_batch])\n",
    "    for batch_id in range(batch_size):\n",
    "        # first clear the plot...\n",
    "        plt.gcf().clear()\n",
    "        \n",
    "        # then retrieve the tragectories\n",
    "        target_tragectories = target_batch[batch_id][input_length:]\n",
    "        predicted_tragectories = prediction[batch_id]\n",
    "        # evaluate the boundary to plot\n",
    "        x_min = np.min(target_tragectories[:,0] - .1)\n",
    "        x_max = np.max(target_tragectories[:,0] + .1)\n",
    "        \n",
    "        y_min = np.min(target_tragectories[:,1] - .1)\n",
    "        y_max = np.max(target_tragectories[:,1] + .1)\n",
    "        \n",
    "        x_min_predicted = np.min(predicted_tragectories[:,0] - .1)\n",
    "        x_max_predicted = np.max(predicted_tragectories[:,0] + .1)\n",
    "        \n",
    "        y_min_predicted = np.min(predicted_tragectories[:,1] - .1)\n",
    "        y_max_predicted = np.max(predicted_tragectories[:,1] + .1)\n",
    "        \n",
    "        # set the boundary\n",
    "        x_min = min(x_min,x_min_predicted)\n",
    "        x_max = max(x_max,x_max_predicted)\n",
    "        y_min = min(y_min,y_min_predicted)\n",
    "        y_max = max(y_max,y_max_predicted)\n",
    "        \n",
    "        plt.xlim(x_min,x_max)\n",
    "        plt.ylim(y_min,y_max)\n",
    "        # plot line...\n",
    "        # first the target tragectories\n",
    "#         plt.plot(target_tragectories[:,0],target_tragectories[:,1],c = 'black')\n",
    "        # then the prediction\n",
    "        for i in range(len(target_tragectories) - 1):\n",
    "            cur_point = target_tragectories[i,:]\n",
    "            next_point = target_tragectories[i + 1,:]\n",
    "            plt.plot([cur_point[0],next_point[0]],[cur_point[1],next_point[1]],c = 'black')\n",
    "            # and predicted...\n",
    "            cur_point = predicted_tragectories[i,:]\n",
    "            next_point = predicted_tragectories[i + 1,:]\n",
    "            print(cur_point,next_point)\n",
    "            plt.plot([cur_point[0],next_point[0]],[cur_point[1],next_point[1]],\n",
    "                     c = 'blue',\n",
    "                     linestyle = ':'\n",
    "            )\n",
    "#         plt.plot(predicted_tragectories[:,0],\n",
    "#                   predicted_tragectories[:,1],\n",
    "#                   linestyle = ':',\n",
    "#                   c = 'blue')\n",
    "        # then save the plot...\n",
    "        plt.savefig('close-{}.png'.format(batch_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9.129274  1.2102317] [9.081666  1.2347597]\n",
      "[9.081666  1.2347597] [9.040708  1.2574404]\n",
      "[9.040708  1.2574404] [9.002562  1.2729093]\n",
      "[8.9769   2.304382] [8.960338 2.314553]\n",
      "[8.960338 2.314553] [8.94669  2.319512]\n",
      "[8.94669  2.319512] [8.927369  2.3255901]\n",
      "[8.850335  2.5511096] [8.819266 2.549107]\n",
      "[8.819266 2.549107] [8.805489  2.5491567]\n",
      "[8.805489  2.5491567] [8.785894  2.5528386]\n",
      "[8.886613  2.5878088] [8.849275  2.5819893]\n",
      "[8.849275  2.5819893] [8.835579 2.580866]\n",
      "[8.835579 2.580866] [8.815897  2.5833383]\n",
      "[7.820026  2.9885628] [7.803195  3.0074983]\n",
      "[7.803195  3.0074983] [7.773543  3.0232258]\n",
      "[7.773543  3.0232258] [7.7370405 3.0484507]\n",
      "[9.602262  0.6891499] [9.576425  0.7092054]\n",
      "[9.576425  0.7092054] [9.548221  0.7454339]\n",
      "[9.548221  0.7454339] [9.529905  0.7640031]\n",
      "[9.294632   0.75532115] [9.277597  0.7840056]\n",
      "[9.277597  0.7840056] [9.242183   0.81894505]\n",
      "[9.242183   0.81894505] [9.216642  0.8413931]\n",
      "[8.895726  2.5589657] [8.863403  2.5554442]\n",
      "[8.863403  2.5554442] [8.851133 2.554605]\n",
      "[8.851133 2.554605] [8.833009  2.5570657]\n",
      "[2.4940348 9.3730135] [2.4928696 9.3666315]\n",
      "[2.4928696 9.3666315] [2.4904397 9.359227 ]\n",
      "[2.4904397 9.359227 ] [2.4859307 9.361257 ]\n",
      "[8.870498  2.3407187] [8.856486  2.3518302]\n",
      "[8.856486  2.3518302] [8.841692  2.3569913]\n",
      "[8.841692  2.3569913] [8.821363  2.3643153]\n",
      "[8.327146 2.438074] [8.336699  2.4621606]\n",
      "[8.336699  2.4621606] [8.316607  2.4726672]\n",
      "[8.316607  2.4726672] [8.291471 2.489376]\n",
      "[9.600902  0.6764832] [9.579551  0.6956533]\n",
      "[9.579551  0.6956533] [9.552409  0.7318617]\n",
      "[9.552409  0.7318617] [9.535329   0.75045973]\n",
      "[2.7895133 7.369996 ] [2.7669227 7.3960257]\n",
      "[2.7669227 7.3960257] [2.743559  7.4172425]\n",
      "[2.743559  7.4172425] [2.7298958 7.424047 ]\n",
      "[8.837642 2.588817] [8.803872  2.5844114]\n",
      "[8.803872  2.5844114] [8.790264  2.5837054]\n",
      "[8.790264  2.5837054] [8.770789  2.5869627]\n",
      "[2.4797752 9.2877865] [2.4769835 9.277196 ]\n",
      "[2.4769835 9.277196 ] [2.472918 9.269674]\n",
      "[2.472918 9.269674] [2.467436 9.270098]\n",
      "[8.405919 2.39809 ] [8.413545  2.4214709]\n",
      "[8.413545  2.4214709] [8.393852  2.4316578]\n",
      "[8.393852  2.4316578] [8.369131 2.447378]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "close_visualize(model,input_batch_padded,target_batch_padded,INPUT_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
