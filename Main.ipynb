{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data.txt','\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_data = data.copy()\n",
    "# normalize X\n",
    "high = data['X'].max()\n",
    "low = data['X'].min()\n",
    "normalized_data['X'] = ((data['X'] - low) / (high - low)) * 10 # within [-0.1,0.1]\n",
    "\n",
    "# normalize Y\n",
    "high = data['Y'].max()\n",
    "low = data['Y'].min()\n",
    "normalized_data['Y'] = ((data['Y'] - low) / (high - low)) * 10 # within [-0.1,0.1]\n",
    "\n",
    "# timestamp / 40\n",
    "normalized_data['timestamp'] = (data['timestamp'] / 40).map(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_data.to_csv('data_normalized.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 23,\n",
       " 24,\n",
       " 25,\n",
       " 26,\n",
       " 27,\n",
       " 28,\n",
       " 29,\n",
       " 30,\n",
       " 31,\n",
       " 32,\n",
       " 33,\n",
       " 34,\n",
       " 35,\n",
       " 36,\n",
       " 37,\n",
       " 38,\n",
       " 39,\n",
       " 40,\n",
       " 41,\n",
       " 42,\n",
       " 43,\n",
       " 44,\n",
       " 45,\n",
       " 46,\n",
       " 47,\n",
       " 48,\n",
       " 49,\n",
       " 50,\n",
       " 51,\n",
       " 52,\n",
       " 53,\n",
       " 54,\n",
       " 55,\n",
       " 56,\n",
       " 57,\n",
       " 58,\n",
       " 59,\n",
       " 60,\n",
       " 61,\n",
       " 62,\n",
       " 63,\n",
       " 64,\n",
       " 65,\n",
       " 66,\n",
       " 67,\n",
       " 68,\n",
       " 69,\n",
       " 70,\n",
       " 71,\n",
       " 72,\n",
       " 73,\n",
       " 74,\n",
       " 75,\n",
       " 76,\n",
       " 77,\n",
       " 78,\n",
       " 79,\n",
       " 80,\n",
       " 81,\n",
       " 82,\n",
       " 83,\n",
       " 84,\n",
       " 85,\n",
       " 86,\n",
       " 87,\n",
       " 88,\n",
       " 89,\n",
       " 90,\n",
       " 91,\n",
       " 92,\n",
       " 93,\n",
       " 94,\n",
       " 95,\n",
       " 96,\n",
       " 97,\n",
       " 98,\n",
       " 99,\n",
       " 100,\n",
       " 101,\n",
       " 102,\n",
       " 103,\n",
       " 104,\n",
       " 105,\n",
       " 106,\n",
       " 107,\n",
       " 108,\n",
       " 109,\n",
       " 110,\n",
       " 111,\n",
       " 112,\n",
       " 113,\n",
       " 114,\n",
       " 115,\n",
       " 116,\n",
       " 117,\n",
       " 118,\n",
       " 119,\n",
       " 120,\n",
       " 121,\n",
       " 122,\n",
       " 123,\n",
       " 124,\n",
       " 125,\n",
       " 126,\n",
       " 127,\n",
       " 128,\n",
       " 129,\n",
       " 130,\n",
       " 131,\n",
       " 132,\n",
       " 133,\n",
       " 134,\n",
       " 135,\n",
       " 136,\n",
       " 137,\n",
       " 138,\n",
       " 139,\n",
       " 140,\n",
       " 141,\n",
       " 142,\n",
       " 143,\n",
       " 144,\n",
       " 145,\n",
       " 146,\n",
       " 147,\n",
       " 148,\n",
       " 149,\n",
       " 150,\n",
       " 151,\n",
       " 152,\n",
       " 153,\n",
       " 154,\n",
       " 155,\n",
       " 156,\n",
       " 157,\n",
       " 158,\n",
       " 159,\n",
       " 160,\n",
       " 161,\n",
       " 162,\n",
       " 163,\n",
       " 164,\n",
       " 165,\n",
       " 166,\n",
       " 167,\n",
       " 168,\n",
       " 169,\n",
       " 170,\n",
       " 171,\n",
       " 172,\n",
       " 173,\n",
       " 174,\n",
       " 175,\n",
       " 176,\n",
       " 177,\n",
       " 178,\n",
       " 179,\n",
       " 180,\n",
       " 181,\n",
       " 182,\n",
       " 183,\n",
       " 184,\n",
       " 185,\n",
       " 186,\n",
       " 187,\n",
       " 188,\n",
       " 189,\n",
       " 190,\n",
       " 191,\n",
       " 192,\n",
       " 193,\n",
       " 194,\n",
       " 195,\n",
       " 196,\n",
       " 197,\n",
       " 198,\n",
       " 199,\n",
       " 200,\n",
       " 201,\n",
       " 202,\n",
       " 203,\n",
       " 204,\n",
       " 205,\n",
       " 206,\n",
       " 207,\n",
       " 208,\n",
       " 209,\n",
       " 210,\n",
       " 211,\n",
       " 212,\n",
       " 213,\n",
       " 214,\n",
       " 215,\n",
       " 216,\n",
       " 217,\n",
       " 218,\n",
       " 219,\n",
       " 220,\n",
       " 221,\n",
       " 222,\n",
       " 223,\n",
       " 224,\n",
       " 225,\n",
       " 226,\n",
       " 227,\n",
       " 228,\n",
       " 229,\n",
       " 230,\n",
       " 231,\n",
       " 232,\n",
       " 233,\n",
       " 234,\n",
       " 235,\n",
       " 236,\n",
       " 237,\n",
       " 238,\n",
       " 239,\n",
       " 240,\n",
       " 241,\n",
       " 242,\n",
       " 243,\n",
       " 244,\n",
       " 245,\n",
       " 246,\n",
       " 247,\n",
       " 248,\n",
       " 249,\n",
       " 250,\n",
       " 251,\n",
       " 252,\n",
       " 253,\n",
       " 254,\n",
       " 255,\n",
       " 256,\n",
       " 257,\n",
       " 258,\n",
       " 259,\n",
       " 260,\n",
       " 261,\n",
       " 262,\n",
       " 263,\n",
       " 264,\n",
       " 265]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "list(map(lambda tup: tup[0],candidate_id_counts.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(128,), (128, 8, 2), (128, 4, 2)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# functions for generate a batch of sample\n",
    "'''\n",
    "    Input:\n",
    "        data: the CSV\n",
    "        num_data: size of batch\n",
    "    Output:\n",
    "        IDs: list of selected IDs\n",
    "        input_sequence: batch with shape (num_data,input_length, 2)\n",
    "        output_sequence: batch with shape (num_data, output_length, 2)\n",
    "'''\n",
    "from random import shuffle\n",
    "def get_batch(data,num_data = 128,input_length = 8, output_length = 4):\n",
    "    # evaluate the total length of series required\n",
    "    total_length = input_length + output_length\n",
    "    # filter out the series that has at least the number of `total_length` long\n",
    "    id_counts = data.groupby('ID').ID.count()\n",
    "    # get a table of candidate id, whose sequence is longer than (or eq. to) total_length\n",
    "    candidate_id_counts = id_counts[id_counts >= total_length]\n",
    "    # get the random sequence...\n",
    "    # the candidate_id_counts is a series with ID as x and count as y\n",
    "    # to get the usable indices, get list series as a list of tuple like (id, count),\n",
    "    # then take the first one (list of id)\n",
    "    # and make it a list, and shuffle on it\n",
    "    random_ids_selected = list(map(lambda tup: tup[0],candidate_id_counts.items()))\n",
    "    shuffle(random_ids_selected)\n",
    "    \n",
    "    selected_ids = []\n",
    "    input_batch = []\n",
    "    target_batch = []\n",
    "    # retrieve the coordinates of the sequence (from the beginning to `total_length`)\n",
    "    for i in random_ids_selected[:num_data]:\n",
    "        selected_ids.append(i)\n",
    "        # select X,Y from ID where ID == i order by timestamp...\n",
    "        sequence_of_i = data[data.ID == i].sort_values(by = \"timestamp\")[[\"X\",\"Y\"]]\n",
    "        # divide the sequence into two parts...\n",
    "        input_sequence = sequence_of_i.iloc[:input_length]\n",
    "        target_sequence = sequence_of_i.iloc[input_length:total_length]\n",
    "        # and append the new sequence to existing arrays\n",
    "        input_batch.append(np.array(input_sequence))\n",
    "        target_batch.append(np.array(target_sequence))\n",
    "    \n",
    "    # return and array of selected ids as well as the batch...\n",
    "    return np.stack(selected_ids), np.stack(input_batch), np.stack(target_batch)\n",
    "    \n",
    "\n",
    "# verify the shape is right...\n",
    "list(map(lambda a: a.shape,get_batch(normalized_data)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''\n",
    "    Visualize the traces in a batch\n",
    "    If batch size = B, sequence length = L...\n",
    "    Input:\n",
    "        batch: batch of sequence of arbitrary length, i.e. array of shape (B,L,2)\n",
    "    Output:\n",
    "        None, a graph will be drawn instead..\n",
    "'''\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "def visualize_trace(batch,target_batch):\n",
    "    # first we make sure that the shape of the batch looks like (_, _, 2)\n",
    "    def check_shape(shape):\n",
    "        if len(shape) != 3:\n",
    "            raise ValueError(\"batch should be in 3 dimension\")\n",
    "        if shape[-1] != 2:\n",
    "            raise ValueError(\"Last axis should be storing X,Y coordinates\")\n",
    "    \n",
    "    check_shape(batch.shape)\n",
    "    check_shape(target_batch.shape)\n",
    "    # sub-routine for draw a particular batch\n",
    "    def draw_batch(batch,linestyle = None):\n",
    "        # now extract the dimension\n",
    "        batch_size, sequence_length, _ = batch.shape\n",
    "        for batch_id in range(batch_size):\n",
    "            # pick a random color for this trace\n",
    "            line_color = np.random.rand(3)\n",
    "            for sequence_pos in range(sequence_length - 1):\n",
    "                # get the two adjacent coordinates\n",
    "                cur_coord = batch[batch_id, sequence_pos]\n",
    "                next_coord = batch[batch_id, sequence_pos + 1]\n",
    "                # and draw the line...\n",
    "                # sneaky plot function requires x-coordinates to be put in the same argument, so are y-coordinates...\n",
    "                plt.plot([cur_coord[0],next_coord[0]],\n",
    "                         [cur_coord[1],next_coord[1]],\n",
    "                         linestyle = linestyle,\n",
    "                         c = line_color)\n",
    "    \n",
    "    draw_batch(batch)\n",
    "    draw_batch(target_batch, \":\")\n",
    "    # finally show the graph\n",
    "#     plt.show()\n",
    "    \n",
    "# let's test this visualization,\n",
    "_, input_batch, target_batch = get_batch(normalized_data,128,16,8)\n",
    "visualize_trace(input_batch,target_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now lets define a vanilla LSTM model\n",
    "'''\n",
    "    According to the paper, there should be an RNN that takes a sequence and gives a sequence (like seq-to-seq)\n",
    "    except this output are hidden layers, like vectors of length 128\n",
    "    To interpret such result, a dense layer with ReLU is added to condense the output to 5 numbers,\n",
    "    namely, the mean_x, mean_y, sxx, syy, and sxy \n",
    "    of the bivariate gaussian of the probability of the agent at that given timestamp.\n",
    "    \n",
    "    The negative log likelihood between the real coordinate and this estimated distribution will be the loss.\n",
    "'''\n",
    "# first, the loss function, in Keras backend\n",
    "from keras.layers import *\n",
    "import keras.backend as K\n",
    "'''\n",
    "    The function takes a series of params of bivariate normal distribution, and a batch of observed coordinates,\n",
    "    and return the log likelikhood of them...\n",
    "    \n",
    "    probability (likelihood) of the observed point (x,y) given the 5 parameters (mx,my,sx,sy,sp):\n",
    "        det(2 * pi * [[sx,sp],[sp,sy]]) ^(-0.5) \n",
    "            * exp(-0.5 * ((x,y) - (mx,my)).T * [[sx,sp],[sp,sy]] * ((x,y) - (mx,my)))\n",
    "    \n",
    "    after taking log and add a minus (* -1)...\n",
    "        -( (-0.5 * log(4 * pi ^ 2 * sx * sy - sp * sp)) + (-0.5 * (...)))\n",
    "    \n",
    "    If batch size = B, sequence length = D...\n",
    "    Input:\n",
    "        Batch bivariate parameters (estimated): K.variable with shape (B,D,5),\n",
    "        Batch of overserved coordinates (label): K.variable with shape (B,D,2)\n",
    "    \n",
    "    Output:\n",
    "        a scaler (K.variable with shape ()), which is the sum of negative log likelihood\n",
    "'''\n",
    "\n",
    "def negative_log_likelihood_loss(batch_observed_coordinates,batch_bivariate_params):\n",
    "    # first check the dimension...\n",
    "    input_shape = K.int_shape(batch_bivariate_params)\n",
    "    target_shape = K.int_shape(batch_observed_coordinates)\n",
    "    \n",
    "    if len(input_shape) != 3 or len(target_shape) != 3:\n",
    "        raise ValueError(\"Dimension of both tensors should be 3\")\n",
    "    \n",
    "    if input_shape[0] != target_shape[0]:\n",
    "        raise ValueError(\"Batch size of both tensors should be the same\")\n",
    "    \n",
    "    if input_shape[1] != target_shape[1]:\n",
    "        raise ValueError(\"Sequence length of both tensors should be the same\")\n",
    "    \n",
    "    if input_shape[2] != 5:\n",
    "        raise ValueError(\"Number of predicted parameters should be 5. Namely, (mx,my,sx,sy,sp)\")\n",
    "    \n",
    "    if target_shape[2] != 2:\n",
    "        raise ValueError(\"Dimension of target coordinates should be 2. Namely, (x,y)\")\n",
    "    \n",
    "    # then split the tensors into (mx,my,sx,sy,sp)...\n",
    "    # all of them should be of shape (B,D)\n",
    "    batch_mx = batch_bivariate_params[:,:,0]\n",
    "    batch_my = batch_bivariate_params[:,:,1]\n",
    "    batch_sx = batch_bivariate_params[:,:,2]\n",
    "    batch_sy = batch_bivariate_params[:,:,3]\n",
    "    batch_sp = batch_bivariate_params[:,:,4]\n",
    "    \n",
    "    batch_x = batch_observed_coordinates[:,:,0]\n",
    "    batch_y = batch_observed_coordinates[:,:,1]\n",
    "    \n",
    "    dx = batch_x - batch_mx # (B,D), (x - mx)\n",
    "    dy = batch_y - batch_my # (B,D), (y - my)\n",
    "    dydx = Multiply()([dx,dy])\n",
    "    \n",
    "    batch_x_change =  K.concatenate([batch_mx[:,0:1], batch_mx[:,1:] - batch_mx[:,:-1]])\n",
    "    batch_y_change =  K.concatenate([batch_my[:,0:1], batch_my[:,1:] - batch_my[:,:-1]])\n",
    "    target_x_change =  K.concatenate([batch_x[:,0:1], batch_x[:,1:] - batch_x[:,:-1]])\n",
    "    target_y_change =  K.concatenate([batch_y[:,0:1], batch_y[:,1:] - batch_y[:,:-1]])\n",
    "    \n",
    "    xy_dot = Multiply()([batch_x_change,target_x_change]) + Multiply()([batch_y_change,target_y_change])\n",
    "    batch_change_norm = K.sqrt(K.square(batch_x_change) + K.square(batch_y_change))\n",
    "    target_change_norm = K.sqrt(K.square(target_x_change) + K.square(target_y_change))\n",
    "    \n",
    "    norm_prod = Multiply()([batch_change_norm,target_change_norm]) + 1e-6\n",
    "    norm_prod_inv = K.pow(norm_prod,-1) # for numerical stability\n",
    "    direction_loss =  - K.mean(Multiply()([xy_dot,norm_prod_inv]))\n",
    "#     det_inv = K.print_tensor(K.pow(det,-1), message=\"det_inv\") # (B,D), (sx * sy - sp^2) ^-1\n",
    "    # (B,D), (dx^2 * sy - 2 * sp * dy * dx + sx * dy^2)\n",
    "    exp = -0.5 * Multiply()([K.square(dx),batch_sy]) - 2 * Multiply()([dydx, batch_sp]) + Multiply()([K.square(dy),batch_sx])\n",
    "    # (B,D), -0.5 * (dx^2 * sy - 2 * sp * dy * dx + sx * dy^2) * det(Cov)^(-1)\n",
    "#     exp = Multiply()([det_inv,exp]) * (-0.5)\n",
    "    \n",
    "    # evaluate the final NLL\n",
    "    '''\n",
    "        A remark here: it is determined that the determininat of the covariance matrix will not be considered as a loss,\n",
    "        as the value of that generally became very large (under the log function)\n",
    "        therefore only the exponents are used as the loss\n",
    "    '''\n",
    "#     batch_nll = - (exp)\n",
    "    batch_nll = K.square(dx) + K.square(dy) - 0.1 * exp + 0.1 * direction_loss\n",
    "    batch_error_total = K.sum(batch_nll, axis = 1) # (B,) sum of NLL in a sequence...\n",
    "    return K.print_tensor(K.mean(batch_error_total, axis = 0)) # (), average of sum of NLL...\n",
    "\n",
    "# now test it...\n",
    "# a regular 0-centered, non-skewing normal\n",
    "[mx,my,sx,sy,sp] = [0,0,.1,.1,0.]\n",
    "bivariate_params = np.array([[[mx,my,sx,sy,sp]]]) # (1,1,5)\n",
    "target_point = np.array([[[-0.,0]]]) # (1,1,2)\n",
    "\n",
    "bivariate_ph = K.variable(value = bivariate_params, dtype = \"float32\")\n",
    "target_ph = K.variable(value = target_point, dtype = \"float32\")\n",
    "nll = negative_log_likelihood_loss(target_ph,bivariate_ph)\n",
    "K.eval(nll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_layer(batch_predicted_coordinates):\n",
    "    return K.cumsum(batch_predicted_coordinates,axis = 1)\n",
    "    \n",
    "def check_tensor(batch_observed_coordinates,batch_predicted_coordinates):\n",
    "    input_shape = K.int_shape(batch_predicted_coordinates)\n",
    "    target_shape = K.int_shape(batch_observed_coordinates)\n",
    "\n",
    "    if len(input_shape) != 3 or len(target_shape) != 3:\n",
    "        raise ValueError(\"Dimension of both tensors should be 3\")\n",
    "\n",
    "    if input_shape[0] != target_shape[0]:\n",
    "        raise ValueError(\"Batch size of both tensors should be the same\")\n",
    "\n",
    "    if input_shape[1] != target_shape[1]:\n",
    "        raise ValueError(\"Sequence length of both tensors should be the same\")\n",
    "\n",
    "    if input_shape[2] != 2:\n",
    "        raise ValueError(\"Number of predicted parameters should be 2. Namely, (mx,my)\")\n",
    "\n",
    "    if target_shape[2] != 2:\n",
    "        raise ValueError(\"Dimension of target coordinates should be 2. Namely, (x,y)\")\n",
    "    \n",
    "def ms_loss(input_tensor):\n",
    "    batch_observed_coordinates,batch_predicted_coordinates = input_tensor\n",
    "    # first check the dimension...\n",
    "    check_tensor(batch_observed_coordinates,batch_predicted_coordinates)\n",
    "    \n",
    "    diff = K.square(batch_predicted_coordinates - batch_observed_coordinates)\n",
    "\n",
    "    return K.sum(diff)\n",
    "\n",
    "def dir_loss(input_tensor):\n",
    "    batch_observed_coordinates,batch_predicted_coordinates = input_tensor\n",
    "    check_tensor(batch_observed_coordinates,batch_predicted_coordinates)\n",
    "    \n",
    "    predict_dir = batch_predicted_coordinates[:,1:] - batch_predicted_coordinates[:,:-1]\n",
    "    \n",
    "    target_dir = batch_observed_coordinates[:,1:] - batch_observed_coordinates[:,:-1]\n",
    "    \n",
    "#     predic_dir_norm = \n",
    "    # this is to maximize the cosine (therefore angle = 0)\n",
    "    return K.sum(K.square((predict_dir - target_dir)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# then, the Vanilla LSTM\n",
    "from keras.models import Model\n",
    "from keras.initializers import RandomNormal\n",
    "from keras import optimizers\n",
    "def vanilla_lstm_model(num_hidden, input_length, predict_length, lr = 1e-3):\n",
    "# def vanilla_lstm_model(num_hidden,input_length, predict_length, input_tensor, target_tensor):\n",
    "    total_length = input_length + predict_length\n",
    "    # the input\n",
    "    input_sequence = Input(shape = (total_length,2), name = 'input_sequence', dtype = 'float32') # (T, 2)\n",
    "    target_sequence = Input(shape = (total_length,2), name = 'target_sequence', dtype = 'float32') # (T, 2)\n",
    "    lstm = LSTM(num_hidden, return_sequences = True)(input_sequence) # (B,T,num_hidden)\n",
    "#     params = TimeDistributed(Dense(5, activation = 'elu'), name = 'params')(lstm) # (B,T,5)\n",
    "    predicted_coordinates_raw = TimeDistributed(Dense(2, activation = 'elu'), name = 'params')(lstm)\n",
    "    \n",
    "    # retrieve the prediction\n",
    "    extract_target_sequence_layer = Lambda(lambda x: x[:,input_length:,:])\n",
    "    predicted_coordinates_masked = extract_target_sequence_layer(predicted_coordinates_raw)\n",
    "    target_coordinates_masked = extract_target_sequence_layer(target_sequence)\n",
    "    # the output layer\n",
    "    predicted_output = Lambda(infer_layer, name = \"predict\")(predicted_coordinates_masked)\n",
    "    # compute the loss\n",
    "    \n",
    "    # first part: the square loss\n",
    "    sq_loss = Lambda(ms_loss, name = 'square_loss')([target_coordinates_masked, predicted_output])\n",
    "    # second part: the direction loss\n",
    "    ori_loss = Lambda(dir_loss, name = 'dir_loss')([target_coordinates_masked, predicted_output])\n",
    "    \n",
    "    loss = Lambda(lambda ts: ts[0] + ts[1],name = 'loss')([sq_loss, ori_loss])\n",
    "    \n",
    "    model = Model(\n",
    "        inputs = [input_sequence,target_sequence],\n",
    "        outputs = [predicted_output,loss])\n",
    "    \n",
    "    model.compile(optimizer=optimizers.RMSprop(lr = lr, clipvalue = 10., decay = 1e-6),\n",
    "                  # since there are two outputs of the model, the estimated params and the NLL,\n",
    "                  # their loss value should be specified.\n",
    "                  # for params there are no loss regarding its value, but I just assign a zero as loss (or the computational graph will break)\n",
    "                  # I made it loss - loss = 0.\n",
    "                  # and for the NLL, return the loss as-is.\n",
    "                  loss= {\n",
    "                        'predict': lambda _, loss: loss - loss, # meh...\n",
    "                          'loss': lambda _, loss: loss\n",
    "                    })\n",
    "    \n",
    "    return input_sequence, target_sequence, model, predicted_output,loss\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# helper function for monitoring training progress\n",
    "from scipy.stats import multivariate_normal\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "'''\n",
    "    Given a set of parameter (array of 5), visualize the heatmap of the bivariate normal distribution\n",
    "'''\n",
    "def draw_heatmap(params):\n",
    "    resolution = 100\n",
    "    interval = 1. / resolution\n",
    "    \n",
    "    x,y = np.mgrid[0:1:interval,0:1:interval]\n",
    "    pos = np.empty(x.shape + (2,))\n",
    "    pos[:,:,0] = x\n",
    "    pos[:,:,1] = y\n",
    "    result_distribution = None\n",
    "    for param in params:\n",
    "        mx,my,sx,sy,sp = param\n",
    "        F = multivariate_normal([mx,my],[[sx,sp],[sp,sy]])\n",
    "        result_distribution = F.pdf(pos) if result_distribution is None else result_distribution + F.pdf(pos)\n",
    "    plt.contourf(x,y,result_distribution)\n",
    "\n",
    "# try it out\n",
    "# fig = plt.figure()\n",
    "draw_heatmap(np.array([[0.3,0.1,0.4,0.2,.2],[0.9,0.2,0.1,0.9,0.]]))\n",
    "visualize_trace(input_batch,target_batch)\n",
    "# draw_heatmap(0.3,0.2,0.01,0.2,0.,fig)\n",
    "plt.show()\n",
    "\n",
    "'''\n",
    "    Draw the mean of the predicted params of all timestamps\n",
    "'''\n",
    "import matplotlib.cm as cm\n",
    "def draw_mean(params):\n",
    "    plt.xlim(0,10)\n",
    "    plt.ylim(0,10)\n",
    "    \n",
    "    \n",
    "    for batch in range(params.shape[0]):\n",
    "        line_color = np.random.rand(3) # choose a color to tell that these scatter points belong to the same prediction\n",
    "        xy_series = params[batch,:,:] # (D,2)\n",
    "        # prgressively change the color to indicate the direction\n",
    "        colors = cm.rainbow(np.linspace(0, 1, params.shape[1]))\n",
    "        plt.scatter(xy_series[:,0],xy_series[:,1], c = colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ReduceLROnPlateau, CSVLogger, LambdaCallback\n",
    "\n",
    "def get_callbacks(input_batch_padded,target_batch_padded,finetune = False):\n",
    "    # prepare callbacks\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='loss', \n",
    "                                  factor=0.1,\n",
    "                                  patience=50, \n",
    "                                  min_lr=1e-6)\n",
    "    csv_logger = CSVLogger(\"log.csv\")\n",
    "\n",
    "    def visualize_prediction(epoch, logs):\n",
    "        params, loss = model.predict([input_batch_padded,target_batch_padded])\n",
    "        # visualize the trace, as well as the distributions generated by the params...\n",
    "        # first clear the previous drawing...\n",
    "    #     try:\n",
    "        plt.gcf().clear()\n",
    "        visualize_trace(input_batch,target_batch)\n",
    "    #     params = params[:,INPUT_LENGTH + 1, :] # (B,5), and it should be the params immediately after the input\n",
    "    #     draw_heatmap(params)\n",
    "        draw_mean(params)\n",
    "        filename = '{}.png' if not finetune else '{}-finetune.png'\n",
    "        plt.savefig(filename.format(epoch))\n",
    "\n",
    "    plot_callback = LambdaCallback(on_epoch_begin = visualize_prediction)\n",
    "    \n",
    "    return [reduce_lr, \n",
    "            csv_logger, \n",
    "           # plot_callback\n",
    "           ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_sequence (InputLayer)     (None, 12, 2)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_4 (LSTM)                   (None, 12, 128)      67072       input_sequence[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "params (TimeDistributed)        (None, 12, 2)        258         lstm_4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_4 (Lambda)               (None, 4, 2)         0           params[0][0]                     \n",
      "                                                                 target_sequence[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "target_sequence (InputLayer)    (None, 12, 2)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "predict (Lambda)                (None, 4, 2)         0           lambda_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "square_loss (Lambda)            ()                   0           lambda_4[1][0]                   \n",
      "                                                                 predict[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dir_loss (Lambda)               ()                   0           lambda_4[1][0]                   \n",
      "                                                                 predict[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "loss (Lambda)                   ()                   0           square_loss[0][0]                \n",
      "                                                                 dir_loss[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 67,330\n",
      "Trainable params: 67,330\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/500\n",
      "256/256 [==============================] - 0s 2ms/step - loss: 4608.8242 - predict_loss: 0.0000e+00 - loss_loss: 4608.8242\n",
      "Epoch 2/500\n",
      "256/256 [==============================] - 0s 474us/step - loss: 2157.0138 - predict_loss: 0.0000e+00 - loss_loss: 2157.0138\n",
      "Epoch 3/500\n",
      "256/256 [==============================] - 0s 456us/step - loss: 1680.0027 - predict_loss: 0.0000e+00 - loss_loss: 1680.0027\n",
      "Epoch 4/500\n",
      "256/256 [==============================] - 0s 417us/step - loss: 1373.2075 - predict_loss: 0.0000e+00 - loss_loss: 1373.2075\n",
      "Epoch 5/500\n",
      "256/256 [==============================] - 0s 478us/step - loss: 1185.8922 - predict_loss: 0.0000e+00 - loss_loss: 1185.8922\n",
      "Epoch 6/500\n",
      "256/256 [==============================] - 0s 463us/step - loss: 1039.2469 - predict_loss: 0.0000e+00 - loss_loss: 1039.2469\n",
      "Epoch 7/500\n",
      "256/256 [==============================] - 0s 403us/step - loss: 913.2998 - predict_loss: 0.0000e+00 - loss_loss: 913.2998\n",
      "Epoch 8/500\n",
      "256/256 [==============================] - 0s 464us/step - loss: 804.7729 - predict_loss: 0.0000e+00 - loss_loss: 804.7729\n",
      "Epoch 9/500\n",
      "256/256 [==============================] - 0s 480us/step - loss: 702.7603 - predict_loss: 0.0000e+00 - loss_loss: 702.7603\n",
      "Epoch 10/500\n",
      "256/256 [==============================] - 0s 417us/step - loss: 605.7766 - predict_loss: 0.0000e+00 - loss_loss: 605.7766\n",
      "Epoch 11/500\n",
      "256/256 [==============================] - 0s 538us/step - loss: 567.0729 - predict_loss: 0.0000e+00 - loss_loss: 567.0729\n",
      "Epoch 12/500\n",
      "256/256 [==============================] - 0s 537us/step - loss: 490.5259 - predict_loss: 0.0000e+00 - loss_loss: 490.5259\n",
      "Epoch 13/500\n",
      "256/256 [==============================] - 0s 529us/step - loss: 453.8744 - predict_loss: 0.0000e+00 - loss_loss: 453.8744\n",
      "Epoch 14/500\n",
      "256/256 [==============================] - 0s 491us/step - loss: 414.1043 - predict_loss: 0.0000e+00 - loss_loss: 414.1043\n",
      "Epoch 15/500\n",
      "256/256 [==============================] - 0s 554us/step - loss: 374.8303 - predict_loss: 0.0000e+00 - loss_loss: 374.8303\n",
      "Epoch 16/500\n",
      "256/256 [==============================] - 0s 580us/step - loss: 347.5660 - predict_loss: 0.0000e+00 - loss_loss: 347.5660\n",
      "Epoch 17/500\n",
      "256/256 [==============================] - 0s 488us/step - loss: 311.5045 - predict_loss: 0.0000e+00 - loss_loss: 311.5045\n",
      "Epoch 18/500\n",
      "256/256 [==============================] - 0s 457us/step - loss: 295.3979 - predict_loss: 0.0000e+00 - loss_loss: 295.3979\n",
      "Epoch 19/500\n",
      "256/256 [==============================] - 0s 476us/step - loss: 259.2149 - predict_loss: 0.0000e+00 - loss_loss: 259.2149\n",
      "Epoch 20/500\n",
      "256/256 [==============================] - 0s 512us/step - loss: 248.6478 - predict_loss: 0.0000e+00 - loss_loss: 248.6478\n",
      "Epoch 21/500\n",
      "256/256 [==============================] - 0s 449us/step - loss: 218.8782 - predict_loss: 0.0000e+00 - loss_loss: 218.8782\n",
      "Epoch 22/500\n",
      "256/256 [==============================] - 0s 407us/step - loss: 195.0121 - predict_loss: 0.0000e+00 - loss_loss: 195.0121\n",
      "Epoch 23/500\n",
      "256/256 [==============================] - 0s 450us/step - loss: 178.2395 - predict_loss: 0.0000e+00 - loss_loss: 178.2395\n",
      "Epoch 24/500\n",
      "256/256 [==============================] - 0s 468us/step - loss: 191.6810 - predict_loss: 0.0000e+00 - loss_loss: 191.6810\n",
      "Epoch 25/500\n",
      "256/256 [==============================] - 0s 536us/step - loss: 159.3953 - predict_loss: 0.0000e+00 - loss_loss: 159.3953\n",
      "Epoch 26/500\n",
      "256/256 [==============================] - 0s 557us/step - loss: 154.3232 - predict_loss: 0.0000e+00 - loss_loss: 154.3232\n",
      "Epoch 27/500\n",
      "256/256 [==============================] - 0s 496us/step - loss: 139.4863 - predict_loss: 0.0000e+00 - loss_loss: 139.4863\n",
      "Epoch 28/500\n",
      "256/256 [==============================] - 0s 478us/step - loss: 139.4794 - predict_loss: 0.0000e+00 - loss_loss: 139.4794\n",
      "Epoch 29/500\n",
      "256/256 [==============================] - 0s 413us/step - loss: 121.3233 - predict_loss: 0.0000e+00 - loss_loss: 121.3233\n",
      "Epoch 30/500\n",
      "256/256 [==============================] - 0s 435us/step - loss: 110.7291 - predict_loss: 0.0000e+00 - loss_loss: 110.7291\n",
      "Epoch 31/500\n",
      "256/256 [==============================] - 0s 453us/step - loss: 116.4854 - predict_loss: 0.0000e+00 - loss_loss: 116.4854\n",
      "Epoch 32/500\n",
      "256/256 [==============================] - 0s 440us/step - loss: 99.3953 - predict_loss: 0.0000e+00 - loss_loss: 99.3953\n",
      "Epoch 33/500\n",
      "256/256 [==============================] - 0s 430us/step - loss: 94.2549 - predict_loss: 0.0000e+00 - loss_loss: 94.2549\n",
      "Epoch 34/500\n",
      "256/256 [==============================] - 0s 449us/step - loss: 96.9963 - predict_loss: 0.0000e+00 - loss_loss: 96.9963\n",
      "Epoch 35/500\n",
      "256/256 [==============================] - 0s 457us/step - loss: 78.6787 - predict_loss: 0.0000e+00 - loss_loss: 78.6787\n",
      "Epoch 36/500\n",
      "256/256 [==============================] - 0s 458us/step - loss: 81.4248 - predict_loss: 0.0000e+00 - loss_loss: 81.4248\n",
      "Epoch 37/500\n",
      "256/256 [==============================] - 0s 519us/step - loss: 77.7552 - predict_loss: 0.0000e+00 - loss_loss: 77.7552\n",
      "Epoch 38/500\n",
      "256/256 [==============================] - 0s 603us/step - loss: 68.0432 - predict_loss: 0.0000e+00 - loss_loss: 68.0432\n",
      "Epoch 39/500\n",
      "256/256 [==============================] - 0s 444us/step - loss: 71.5287 - predict_loss: 0.0000e+00 - loss_loss: 71.5287\n",
      "Epoch 40/500\n",
      "256/256 [==============================] - 0s 463us/step - loss: 60.8668 - predict_loss: 0.0000e+00 - loss_loss: 60.8668\n",
      "Epoch 41/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256/256 [==============================] - 0s 480us/step - loss: 56.8364 - predict_loss: 0.0000e+00 - loss_loss: 56.8364\n",
      "Epoch 42/500\n",
      "256/256 [==============================] - 0s 416us/step - loss: 58.6745 - predict_loss: 0.0000e+00 - loss_loss: 58.6745\n",
      "Epoch 43/500\n",
      "256/256 [==============================] - 0s 405us/step - loss: 53.1601 - predict_loss: 0.0000e+00 - loss_loss: 53.1601\n",
      "Epoch 44/500\n",
      "256/256 [==============================] - 0s 429us/step - loss: 55.4040 - predict_loss: 0.0000e+00 - loss_loss: 55.4040\n",
      "Epoch 45/500\n",
      "256/256 [==============================] - 0s 524us/step - loss: 49.9378 - predict_loss: 0.0000e+00 - loss_loss: 49.9378\n",
      "Epoch 46/500\n",
      "256/256 [==============================] - 0s 581us/step - loss: 46.8868 - predict_loss: 0.0000e+00 - loss_loss: 46.8868\n",
      "Epoch 47/500\n",
      "256/256 [==============================] - 0s 500us/step - loss: 53.0918 - predict_loss: 0.0000e+00 - loss_loss: 53.0918\n",
      "Epoch 48/500\n",
      "256/256 [==============================] - 0s 422us/step - loss: 41.2759 - predict_loss: 0.0000e+00 - loss_loss: 41.2759\n",
      "Epoch 49/500\n",
      "256/256 [==============================] - 0s 443us/step - loss: 39.8763 - predict_loss: 0.0000e+00 - loss_loss: 39.8763\n",
      "Epoch 50/500\n",
      "256/256 [==============================] - 0s 395us/step - loss: 44.7040 - predict_loss: 0.0000e+00 - loss_loss: 44.7040\n",
      "Epoch 51/500\n",
      "256/256 [==============================] - 0s 350us/step - loss: 41.9287 - predict_loss: 0.0000e+00 - loss_loss: 41.9287\n",
      "Epoch 52/500\n",
      "256/256 [==============================] - 0s 396us/step - loss: 39.1815 - predict_loss: 0.0000e+00 - loss_loss: 39.1815\n",
      "Epoch 53/500\n",
      "256/256 [==============================] - 0s 371us/step - loss: 30.2186 - predict_loss: 0.0000e+00 - loss_loss: 30.2186\n",
      "Epoch 54/500\n",
      "256/256 [==============================] - 0s 401us/step - loss: 40.9874 - predict_loss: 0.0000e+00 - loss_loss: 40.9874\n",
      "Epoch 55/500\n",
      "256/256 [==============================] - 0s 391us/step - loss: 35.2864 - predict_loss: 0.0000e+00 - loss_loss: 35.2864\n",
      "Epoch 56/500\n",
      "256/256 [==============================] - 0s 367us/step - loss: 30.6906 - predict_loss: 0.0000e+00 - loss_loss: 30.6906\n",
      "Epoch 57/500\n",
      "256/256 [==============================] - 0s 450us/step - loss: 30.4867 - predict_loss: 0.0000e+00 - loss_loss: 30.4867\n",
      "Epoch 58/500\n",
      "256/256 [==============================] - 0s 502us/step - loss: 37.1051 - predict_loss: 0.0000e+00 - loss_loss: 37.1051\n",
      "Epoch 59/500\n",
      "256/256 [==============================] - 0s 506us/step - loss: 28.0540 - predict_loss: 0.0000e+00 - loss_loss: 28.0540\n",
      "Epoch 60/500\n",
      "256/256 [==============================] - 0s 423us/step - loss: 29.9055 - predict_loss: 0.0000e+00 - loss_loss: 29.9055\n",
      "Epoch 61/500\n",
      "256/256 [==============================] - 0s 505us/step - loss: 25.4587 - predict_loss: 0.0000e+00 - loss_loss: 25.4587\n",
      "Epoch 62/500\n",
      "256/256 [==============================] - 0s 501us/step - loss: 31.6669 - predict_loss: 0.0000e+00 - loss_loss: 31.6669\n",
      "Epoch 63/500\n",
      "256/256 [==============================] - 0s 520us/step - loss: 23.8922 - predict_loss: 0.0000e+00 - loss_loss: 23.8922\n",
      "Epoch 64/500\n",
      "256/256 [==============================] - 0s 453us/step - loss: 29.6268 - predict_loss: 0.0000e+00 - loss_loss: 29.6268\n",
      "Epoch 65/500\n",
      "256/256 [==============================] - 0s 439us/step - loss: 29.0660 - predict_loss: 0.0000e+00 - loss_loss: 29.0660\n",
      "Epoch 66/500\n",
      "256/256 [==============================] - 0s 449us/step - loss: 23.1690 - predict_loss: 0.0000e+00 - loss_loss: 23.1690\n",
      "Epoch 67/500\n",
      "256/256 [==============================] - 0s 528us/step - loss: 18.6101 - predict_loss: 0.0000e+00 - loss_loss: 18.6101\n",
      "Epoch 68/500\n",
      "256/256 [==============================] - 0s 531us/step - loss: 24.9456 - predict_loss: 0.0000e+00 - loss_loss: 24.9456\n",
      "Epoch 69/500\n",
      "256/256 [==============================] - 0s 440us/step - loss: 26.2047 - predict_loss: 0.0000e+00 - loss_loss: 26.2047\n",
      "Epoch 70/500\n",
      "256/256 [==============================] - 0s 442us/step - loss: 24.5166 - predict_loss: 0.0000e+00 - loss_loss: 24.5166\n",
      "Epoch 71/500\n",
      "256/256 [==============================] - 0s 398us/step - loss: 19.4927 - predict_loss: 0.0000e+00 - loss_loss: 19.4927\n",
      "Epoch 72/500\n",
      "256/256 [==============================] - 0s 385us/step - loss: 29.9487 - predict_loss: 0.0000e+00 - loss_loss: 29.9487\n",
      "Epoch 73/500\n",
      "256/256 [==============================] - 0s 405us/step - loss: 22.7834 - predict_loss: 0.0000e+00 - loss_loss: 22.7834\n",
      "Epoch 74/500\n",
      "256/256 [==============================] - 0s 473us/step - loss: 21.3273 - predict_loss: 0.0000e+00 - loss_loss: 21.3273\n",
      "Epoch 75/500\n",
      "256/256 [==============================] - 0s 495us/step - loss: 20.1954 - predict_loss: 0.0000e+00 - loss_loss: 20.1954\n",
      "Epoch 76/500\n",
      "256/256 [==============================] - 0s 453us/step - loss: 24.7637 - predict_loss: 0.0000e+00 - loss_loss: 24.7637\n",
      "Epoch 77/500\n",
      "256/256 [==============================] - 0s 439us/step - loss: 19.0847 - predict_loss: 0.0000e+00 - loss_loss: 19.0847\n",
      "Epoch 78/500\n",
      "256/256 [==============================] - 0s 494us/step - loss: 21.1235 - predict_loss: 0.0000e+00 - loss_loss: 21.1235\n",
      "Epoch 79/500\n",
      "256/256 [==============================] - 0s 465us/step - loss: 19.7958 - predict_loss: 0.0000e+00 - loss_loss: 19.7958\n",
      "Epoch 80/500\n",
      "256/256 [==============================] - 0s 401us/step - loss: 20.5778 - predict_loss: 0.0000e+00 - loss_loss: 20.5778\n",
      "Epoch 81/500\n",
      "256/256 [==============================] - 0s 410us/step - loss: 21.8982 - predict_loss: 0.0000e+00 - loss_loss: 21.8982\n",
      "Epoch 82/500\n",
      "256/256 [==============================] - 0s 377us/step - loss: 22.9653 - predict_loss: 0.0000e+00 - loss_loss: 22.9653\n",
      "Epoch 83/500\n",
      "256/256 [==============================] - 0s 418us/step - loss: 20.0156 - predict_loss: 0.0000e+00 - loss_loss: 20.0156\n",
      "Epoch 84/500\n",
      "256/256 [==============================] - 0s 431us/step - loss: 20.7022 - predict_loss: 0.0000e+00 - loss_loss: 20.7022\n",
      "Epoch 85/500\n",
      "256/256 [==============================] - 0s 444us/step - loss: 19.7560 - predict_loss: 0.0000e+00 - loss_loss: 19.7560\n",
      "Epoch 86/500\n",
      "256/256 [==============================] - 0s 391us/step - loss: 16.6860 - predict_loss: 0.0000e+00 - loss_loss: 16.6860\n",
      "Epoch 87/500\n",
      "256/256 [==============================] - 0s 636us/step - loss: 16.8017 - predict_loss: 0.0000e+00 - loss_loss: 16.8017\n",
      "Epoch 88/500\n",
      "256/256 [==============================] - 0s 615us/step - loss: 19.9637 - predict_loss: 0.0000e+00 - loss_loss: 19.9637\n",
      "Epoch 89/500\n",
      "256/256 [==============================] - 0s 446us/step - loss: 18.4246 - predict_loss: 0.0000e+00 - loss_loss: 18.4246\n",
      "Epoch 90/500\n",
      "256/256 [==============================] - 0s 559us/step - loss: 16.7369 - predict_loss: 0.0000e+00 - loss_loss: 16.7369\n",
      "Epoch 91/500\n",
      "256/256 [==============================] - 0s 565us/step - loss: 15.0386 - predict_loss: 0.0000e+00 - loss_loss: 15.0386\n",
      "Epoch 92/500\n",
      "256/256 [==============================] - 0s 407us/step - loss: 21.0663 - predict_loss: 0.0000e+00 - loss_loss: 21.0663\n",
      "Epoch 93/500\n",
      "256/256 [==============================] - 0s 450us/step - loss: 16.6351 - predict_loss: 0.0000e+00 - loss_loss: 16.6351\n",
      "Epoch 94/500\n",
      "256/256 [==============================] - 0s 394us/step - loss: 19.0740 - predict_loss: 0.0000e+00 - loss_loss: 19.0740\n",
      "Epoch 95/500\n",
      "256/256 [==============================] - 0s 455us/step - loss: 16.1028 - predict_loss: 0.0000e+00 - loss_loss: 16.1028\n",
      "Epoch 96/500\n",
      "256/256 [==============================] - 0s 422us/step - loss: 14.9568 - predict_loss: 0.0000e+00 - loss_loss: 14.9568\n",
      "Epoch 97/500\n",
      "256/256 [==============================] - 0s 447us/step - loss: 19.2807 - predict_loss: 0.0000e+00 - loss_loss: 19.2807\n",
      "Epoch 98/500\n",
      "256/256 [==============================] - 0s 417us/step - loss: 16.7920 - predict_loss: 0.0000e+00 - loss_loss: 16.7920\n",
      "Epoch 99/500\n",
      "256/256 [==============================] - 0s 398us/step - loss: 12.8267 - predict_loss: 0.0000e+00 - loss_loss: 12.8267\n",
      "Epoch 100/500\n",
      "256/256 [==============================] - 0s 443us/step - loss: 15.3232 - predict_loss: 0.0000e+00 - loss_loss: 15.3232\n",
      "Epoch 101/500\n",
      "256/256 [==============================] - 0s 422us/step - loss: 18.4140 - predict_loss: 0.0000e+00 - loss_loss: 18.4140\n",
      "Epoch 102/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256/256 [==============================] - 0s 408us/step - loss: 15.5753 - predict_loss: 0.0000e+00 - loss_loss: 15.5753\n",
      "Epoch 103/500\n",
      "256/256 [==============================] - 0s 429us/step - loss: 13.6402 - predict_loss: 0.0000e+00 - loss_loss: 13.6402\n",
      "Epoch 104/500\n",
      "256/256 [==============================] - 0s 458us/step - loss: 15.3605 - predict_loss: 0.0000e+00 - loss_loss: 15.3605\n",
      "Epoch 105/500\n",
      "256/256 [==============================] - 0s 373us/step - loss: 14.9595 - predict_loss: 0.0000e+00 - loss_loss: 14.9595\n",
      "Epoch 106/500\n",
      "256/256 [==============================] - 0s 427us/step - loss: 17.8946 - predict_loss: 0.0000e+00 - loss_loss: 17.8946\n",
      "Epoch 107/500\n",
      "256/256 [==============================] - 0s 369us/step - loss: 11.7580 - predict_loss: 0.0000e+00 - loss_loss: 11.7580\n",
      "Epoch 108/500\n",
      "256/256 [==============================] - 0s 336us/step - loss: 14.5222 - predict_loss: 0.0000e+00 - loss_loss: 14.5222\n",
      "Epoch 109/500\n",
      "256/256 [==============================] - 0s 350us/step - loss: 18.4633 - predict_loss: 0.0000e+00 - loss_loss: 18.4633\n",
      "Epoch 110/500\n",
      "256/256 [==============================] - 0s 456us/step - loss: 13.6835 - predict_loss: 0.0000e+00 - loss_loss: 13.6835\n",
      "Epoch 111/500\n",
      "256/256 [==============================] - 0s 502us/step - loss: 15.6325 - predict_loss: 0.0000e+00 - loss_loss: 15.6325\n",
      "Epoch 112/500\n",
      "256/256 [==============================] - 0s 391us/step - loss: 12.4654 - predict_loss: 0.0000e+00 - loss_loss: 12.4654\n",
      "Epoch 113/500\n",
      "256/256 [==============================] - 0s 432us/step - loss: 13.3449 - predict_loss: 0.0000e+00 - loss_loss: 13.3449\n",
      "Epoch 114/500\n",
      "256/256 [==============================] - 0s 400us/step - loss: 12.4271 - predict_loss: 0.0000e+00 - loss_loss: 12.4271\n",
      "Epoch 115/500\n",
      "256/256 [==============================] - 0s 402us/step - loss: 13.8591 - predict_loss: 0.0000e+00 - loss_loss: 13.8591\n",
      "Epoch 116/500\n",
      "256/256 [==============================] - 0s 436us/step - loss: 13.3029 - predict_loss: 0.0000e+00 - loss_loss: 13.3029\n",
      "Epoch 117/500\n",
      "256/256 [==============================] - 0s 375us/step - loss: 13.2557 - predict_loss: 0.0000e+00 - loss_loss: 13.2557\n",
      "Epoch 118/500\n",
      "256/256 [==============================] - 0s 439us/step - loss: 13.1738 - predict_loss: 0.0000e+00 - loss_loss: 13.1738\n",
      "Epoch 119/500\n",
      "256/256 [==============================] - 0s 444us/step - loss: 17.8196 - predict_loss: 0.0000e+00 - loss_loss: 17.8196\n",
      "Epoch 120/500\n",
      "256/256 [==============================] - 0s 376us/step - loss: 9.9621 - predict_loss: 0.0000e+00 - loss_loss: 9.9621\n",
      "Epoch 121/500\n",
      "256/256 [==============================] - 0s 444us/step - loss: 12.1410 - predict_loss: 0.0000e+00 - loss_loss: 12.1410\n",
      "Epoch 122/500\n",
      "256/256 [==============================] - 0s 407us/step - loss: 12.8325 - predict_loss: 0.0000e+00 - loss_loss: 12.8325\n",
      "Epoch 123/500\n",
      "256/256 [==============================] - 0s 452us/step - loss: 13.0042 - predict_loss: 0.0000e+00 - loss_loss: 13.0042\n",
      "Epoch 124/500\n",
      "256/256 [==============================] - 0s 439us/step - loss: 12.4218 - predict_loss: 0.0000e+00 - loss_loss: 12.4218\n",
      "Epoch 125/500\n",
      "256/256 [==============================] - 0s 454us/step - loss: 9.3300 - predict_loss: 0.0000e+00 - loss_loss: 9.3300\n",
      "Epoch 126/500\n",
      "256/256 [==============================] - 0s 457us/step - loss: 12.2166 - predict_loss: 0.0000e+00 - loss_loss: 12.2166\n",
      "Epoch 127/500\n",
      "256/256 [==============================] - 0s 431us/step - loss: 14.8103 - predict_loss: 0.0000e+00 - loss_loss: 14.8103\n",
      "Epoch 128/500\n",
      "256/256 [==============================] - 0s 411us/step - loss: 13.1419 - predict_loss: 0.0000e+00 - loss_loss: 13.1419\n",
      "Epoch 129/500\n",
      "256/256 [==============================] - 0s 443us/step - loss: 12.1477 - predict_loss: 0.0000e+00 - loss_loss: 12.1477\n",
      "Epoch 130/500\n",
      "256/256 [==============================] - 0s 405us/step - loss: 10.6159 - predict_loss: 0.0000e+00 - loss_loss: 10.6159\n",
      "Epoch 131/500\n",
      "256/256 [==============================] - 0s 410us/step - loss: 12.5265 - predict_loss: 0.0000e+00 - loss_loss: 12.5265\n",
      "Epoch 132/500\n",
      "256/256 [==============================] - 0s 429us/step - loss: 9.6370 - predict_loss: 0.0000e+00 - loss_loss: 9.6370\n",
      "Epoch 133/500\n",
      "256/256 [==============================] - 0s 450us/step - loss: 14.6031 - predict_loss: 0.0000e+00 - loss_loss: 14.6031\n",
      "Epoch 134/500\n",
      "256/256 [==============================] - 0s 448us/step - loss: 11.1011 - predict_loss: 0.0000e+00 - loss_loss: 11.1011\n",
      "Epoch 135/500\n",
      "256/256 [==============================] - 0s 420us/step - loss: 9.4583 - predict_loss: 0.0000e+00 - loss_loss: 9.4583\n",
      "Epoch 136/500\n",
      "256/256 [==============================] - 0s 432us/step - loss: 14.7041 - predict_loss: 0.0000e+00 - loss_loss: 14.7041\n",
      "Epoch 137/500\n",
      "256/256 [==============================] - 0s 459us/step - loss: 11.7818 - predict_loss: 0.0000e+00 - loss_loss: 11.7818\n",
      "Epoch 138/500\n",
      "256/256 [==============================] - 0s 436us/step - loss: 9.1464 - predict_loss: 0.0000e+00 - loss_loss: 9.1464\n",
      "Epoch 139/500\n",
      "256/256 [==============================] - 0s 477us/step - loss: 11.8104 - predict_loss: 0.0000e+00 - loss_loss: 11.8104\n",
      "Epoch 140/500\n",
      "256/256 [==============================] - 0s 422us/step - loss: 9.5733 - predict_loss: 0.0000e+00 - loss_loss: 9.5733\n",
      "Epoch 141/500\n",
      "256/256 [==============================] - 0s 477us/step - loss: 12.9151 - predict_loss: 0.0000e+00 - loss_loss: 12.9151\n",
      "Epoch 142/500\n",
      "256/256 [==============================] - 0s 446us/step - loss: 8.8818 - predict_loss: 0.0000e+00 - loss_loss: 8.8818\n",
      "Epoch 143/500\n",
      "256/256 [==============================] - 0s 409us/step - loss: 10.9723 - predict_loss: 0.0000e+00 - loss_loss: 10.9723\n",
      "Epoch 144/500\n",
      "256/256 [==============================] - 0s 398us/step - loss: 11.1281 - predict_loss: 0.0000e+00 - loss_loss: 11.1281\n",
      "Epoch 145/500\n",
      "256/256 [==============================] - 0s 419us/step - loss: 12.7809 - predict_loss: 0.0000e+00 - loss_loss: 12.7809\n",
      "Epoch 146/500\n",
      "256/256 [==============================] - 0s 400us/step - loss: 9.4016 - predict_loss: 0.0000e+00 - loss_loss: 9.4016\n",
      "Epoch 147/500\n",
      "256/256 [==============================] - 0s 352us/step - loss: 9.9937 - predict_loss: 0.0000e+00 - loss_loss: 9.9937\n",
      "Epoch 148/500\n",
      "256/256 [==============================] - 0s 436us/step - loss: 9.2001 - predict_loss: 0.0000e+00 - loss_loss: 9.2001\n",
      "Epoch 149/500\n",
      "256/256 [==============================] - 0s 436us/step - loss: 10.1098 - predict_loss: 0.0000e+00 - loss_loss: 10.1098\n",
      "Epoch 150/500\n",
      "256/256 [==============================] - 0s 383us/step - loss: 11.3012 - predict_loss: 0.0000e+00 - loss_loss: 11.3012\n",
      "Epoch 151/500\n",
      "256/256 [==============================] - 0s 528us/step - loss: 10.7929 - predict_loss: 0.0000e+00 - loss_loss: 10.7929\n",
      "Epoch 152/500\n",
      "256/256 [==============================] - 0s 559us/step - loss: 11.3607 - predict_loss: 0.0000e+00 - loss_loss: 11.3607\n",
      "Epoch 153/500\n",
      "256/256 [==============================] - 0s 519us/step - loss: 10.3337 - predict_loss: 0.0000e+00 - loss_loss: 10.3337\n",
      "Epoch 154/500\n",
      "256/256 [==============================] - 0s 505us/step - loss: 9.4966 - predict_loss: 0.0000e+00 - loss_loss: 9.4966\n",
      "Epoch 155/500\n",
      "256/256 [==============================] - 0s 437us/step - loss: 9.2812 - predict_loss: 0.0000e+00 - loss_loss: 9.2812\n",
      "Epoch 156/500\n",
      "256/256 [==============================] - 0s 409us/step - loss: 11.1137 - predict_loss: 0.0000e+00 - loss_loss: 11.1137\n",
      "Epoch 157/500\n",
      "256/256 [==============================] - 0s 453us/step - loss: 8.4846 - predict_loss: 0.0000e+00 - loss_loss: 8.4846\n",
      "Epoch 158/500\n",
      "256/256 [==============================] - 0s 443us/step - loss: 10.8479 - predict_loss: 0.0000e+00 - loss_loss: 10.8479\n",
      "Epoch 159/500\n",
      "256/256 [==============================] - 0s 419us/step - loss: 9.1894 - predict_loss: 0.0000e+00 - loss_loss: 9.1894\n",
      "Epoch 160/500\n",
      "256/256 [==============================] - 0s 386us/step - loss: 10.7689 - predict_loss: 0.0000e+00 - loss_loss: 10.7689\n",
      "Epoch 161/500\n",
      "256/256 [==============================] - 0s 440us/step - loss: 8.7539 - predict_loss: 0.0000e+00 - loss_loss: 8.7539\n",
      "Epoch 162/500\n",
      "256/256 [==============================] - 0s 483us/step - loss: 8.7537 - predict_loss: 0.0000e+00 - loss_loss: 8.7537\n",
      "Epoch 163/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256/256 [==============================] - 0s 505us/step - loss: 11.3761 - predict_loss: 0.0000e+00 - loss_loss: 11.3761\n",
      "Epoch 164/500\n",
      "256/256 [==============================] - 0s 449us/step - loss: 7.2790 - predict_loss: 0.0000e+00 - loss_loss: 7.2790\n",
      "Epoch 165/500\n",
      "256/256 [==============================] - 0s 436us/step - loss: 12.4217 - predict_loss: 0.0000e+00 - loss_loss: 12.4217\n",
      "Epoch 166/500\n",
      "256/256 [==============================] - 0s 441us/step - loss: 8.2477 - predict_loss: 0.0000e+00 - loss_loss: 8.2477\n",
      "Epoch 167/500\n",
      "256/256 [==============================] - 0s 406us/step - loss: 7.3662 - predict_loss: 0.0000e+00 - loss_loss: 7.3662\n",
      "Epoch 168/500\n",
      "256/256 [==============================] - 0s 497us/step - loss: 10.0731 - predict_loss: 0.0000e+00 - loss_loss: 10.0731\n",
      "Epoch 169/500\n",
      "256/256 [==============================] - 0s 411us/step - loss: 8.2848 - predict_loss: 0.0000e+00 - loss_loss: 8.2848\n",
      "Epoch 170/500\n",
      "256/256 [==============================] - 0s 408us/step - loss: 8.3917 - predict_loss: 0.0000e+00 - loss_loss: 8.3917\n",
      "Epoch 171/500\n",
      "256/256 [==============================] - 0s 412us/step - loss: 11.7990 - predict_loss: 0.0000e+00 - loss_loss: 11.7990\n",
      "Epoch 172/500\n",
      "256/256 [==============================] - 0s 415us/step - loss: 9.3227 - predict_loss: 0.0000e+00 - loss_loss: 9.3227\n",
      "Epoch 173/500\n",
      "256/256 [==============================] - 0s 427us/step - loss: 6.2659 - predict_loss: 0.0000e+00 - loss_loss: 6.2659\n",
      "Epoch 174/500\n",
      "256/256 [==============================] - 0s 480us/step - loss: 9.7381 - predict_loss: 0.0000e+00 - loss_loss: 9.7381\n",
      "Epoch 175/500\n",
      "256/256 [==============================] - 0s 444us/step - loss: 9.9603 - predict_loss: 0.0000e+00 - loss_loss: 9.9603\n",
      "Epoch 176/500\n",
      "256/256 [==============================] - 0s 429us/step - loss: 9.0191 - predict_loss: 0.0000e+00 - loss_loss: 9.0191\n",
      "Epoch 177/500\n",
      "256/256 [==============================] - 0s 461us/step - loss: 8.8743 - predict_loss: 0.0000e+00 - loss_loss: 8.8743\n",
      "Epoch 178/500\n",
      "256/256 [==============================] - 0s 433us/step - loss: 7.7948 - predict_loss: 0.0000e+00 - loss_loss: 7.7948\n",
      "Epoch 179/500\n",
      "256/256 [==============================] - 0s 413us/step - loss: 7.8678 - predict_loss: 0.0000e+00 - loss_loss: 7.8678\n",
      "Epoch 180/500\n",
      "256/256 [==============================] - 0s 414us/step - loss: 10.8934 - predict_loss: 0.0000e+00 - loss_loss: 10.8934\n",
      "Epoch 181/500\n",
      "256/256 [==============================] - 0s 412us/step - loss: 5.4104 - predict_loss: 0.0000e+00 - loss_loss: 5.4104\n",
      "Epoch 182/500\n",
      "256/256 [==============================] - 0s 442us/step - loss: 8.3516 - predict_loss: 0.0000e+00 - loss_loss: 8.3516\n",
      "Epoch 183/500\n",
      "256/256 [==============================] - 0s 440us/step - loss: 10.8878 - predict_loss: 0.0000e+00 - loss_loss: 10.8878\n",
      "Epoch 184/500\n",
      "256/256 [==============================] - 0s 426us/step - loss: 7.2952 - predict_loss: 0.0000e+00 - loss_loss: 7.2952\n",
      "Epoch 185/500\n",
      "256/256 [==============================] - 0s 381us/step - loss: 9.5733 - predict_loss: 0.0000e+00 - loss_loss: 9.5733\n",
      "Epoch 186/500\n",
      "256/256 [==============================] - 0s 456us/step - loss: 7.4778 - predict_loss: 0.0000e+00 - loss_loss: 7.4778\n",
      "Epoch 187/500\n",
      "256/256 [==============================] - 0s 376us/step - loss: 8.4304 - predict_loss: 0.0000e+00 - loss_loss: 8.4304\n",
      "Epoch 188/500\n",
      "256/256 [==============================] - 0s 387us/step - loss: 7.3680 - predict_loss: 0.0000e+00 - loss_loss: 7.3680\n",
      "Epoch 189/500\n",
      "256/256 [==============================] - 0s 446us/step - loss: 8.5149 - predict_loss: 0.0000e+00 - loss_loss: 8.5149\n",
      "Epoch 190/500\n",
      "256/256 [==============================] - 0s 394us/step - loss: 9.0628 - predict_loss: 0.0000e+00 - loss_loss: 9.0628\n",
      "Epoch 191/500\n",
      "256/256 [==============================] - 0s 407us/step - loss: 7.5540 - predict_loss: 0.0000e+00 - loss_loss: 7.5540\n",
      "Epoch 192/500\n",
      "256/256 [==============================] - 0s 417us/step - loss: 8.7929 - predict_loss: 0.0000e+00 - loss_loss: 8.7929\n",
      "Epoch 193/500\n",
      "256/256 [==============================] - 0s 407us/step - loss: 8.3861 - predict_loss: 0.0000e+00 - loss_loss: 8.3861\n",
      "Epoch 194/500\n",
      "256/256 [==============================] - 0s 382us/step - loss: 10.2319 - predict_loss: 0.0000e+00 - loss_loss: 10.2319\n",
      "Epoch 195/500\n",
      "256/256 [==============================] - 0s 378us/step - loss: 8.2657 - predict_loss: 0.0000e+00 - loss_loss: 8.2657\n",
      "Epoch 196/500\n",
      "256/256 [==============================] - 0s 429us/step - loss: 6.6573 - predict_loss: 0.0000e+00 - loss_loss: 6.6573\n",
      "Epoch 197/500\n",
      "256/256 [==============================] - 0s 479us/step - loss: 8.1732 - predict_loss: 0.0000e+00 - loss_loss: 8.1732\n",
      "Epoch 198/500\n",
      "256/256 [==============================] - 0s 422us/step - loss: 7.3234 - predict_loss: 0.0000e+00 - loss_loss: 7.3234\n",
      "Epoch 199/500\n",
      "256/256 [==============================] - 0s 413us/step - loss: 8.3563 - predict_loss: 0.0000e+00 - loss_loss: 8.3563\n",
      "Epoch 200/500\n",
      "256/256 [==============================] - 0s 415us/step - loss: 9.9888 - predict_loss: 0.0000e+00 - loss_loss: 9.9888\n",
      "Epoch 201/500\n",
      "256/256 [==============================] - 0s 392us/step - loss: 6.8494 - predict_loss: 0.0000e+00 - loss_loss: 6.8494\n",
      "Epoch 202/500\n",
      "256/256 [==============================] - 0s 456us/step - loss: 6.5997 - predict_loss: 0.0000e+00 - loss_loss: 6.5997\n",
      "Epoch 203/500\n",
      "256/256 [==============================] - 0s 429us/step - loss: 7.4333 - predict_loss: 0.0000e+00 - loss_loss: 7.4333\n",
      "Epoch 204/500\n",
      "256/256 [==============================] - 0s 401us/step - loss: 6.9627 - predict_loss: 0.0000e+00 - loss_loss: 6.9627\n",
      "Epoch 205/500\n",
      "256/256 [==============================] - 0s 443us/step - loss: 8.9719 - predict_loss: 0.0000e+00 - loss_loss: 8.9719\n",
      "Epoch 206/500\n",
      "256/256 [==============================] - 0s 403us/step - loss: 8.3738 - predict_loss: 0.0000e+00 - loss_loss: 8.3738\n",
      "Epoch 207/500\n",
      "256/256 [==============================] - 0s 396us/step - loss: 7.0460 - predict_loss: 0.0000e+00 - loss_loss: 7.0460\n",
      "Epoch 208/500\n",
      "256/256 [==============================] - 0s 432us/step - loss: 6.6617 - predict_loss: 0.0000e+00 - loss_loss: 6.6617\n",
      "Epoch 209/500\n",
      "256/256 [==============================] - 0s 396us/step - loss: 8.5939 - predict_loss: 0.0000e+00 - loss_loss: 8.5939\n",
      "Epoch 210/500\n",
      "256/256 [==============================] - 0s 434us/step - loss: 6.5552 - predict_loss: 0.0000e+00 - loss_loss: 6.5552\n",
      "Epoch 211/500\n",
      "256/256 [==============================] - 0s 422us/step - loss: 7.1280 - predict_loss: 0.0000e+00 - loss_loss: 7.1280\n",
      "Epoch 212/500\n",
      "256/256 [==============================] - 0s 477us/step - loss: 7.7073 - predict_loss: 0.0000e+00 - loss_loss: 7.7073\n",
      "Epoch 213/500\n",
      "256/256 [==============================] - 0s 416us/step - loss: 7.3600 - predict_loss: 0.0000e+00 - loss_loss: 7.3600\n",
      "Epoch 214/500\n",
      "256/256 [==============================] - 0s 420us/step - loss: 7.0421 - predict_loss: 0.0000e+00 - loss_loss: 7.0421\n",
      "Epoch 215/500\n",
      "256/256 [==============================] - 0s 456us/step - loss: 5.7106 - predict_loss: 0.0000e+00 - loss_loss: 5.7106\n",
      "Epoch 216/500\n",
      "256/256 [==============================] - 0s 416us/step - loss: 8.8212 - predict_loss: 0.0000e+00 - loss_loss: 8.8212\n",
      "Epoch 217/500\n",
      "256/256 [==============================] - 0s 392us/step - loss: 8.0027 - predict_loss: 0.0000e+00 - loss_loss: 8.0027\n",
      "Epoch 218/500\n",
      "256/256 [==============================] - 0s 429us/step - loss: 7.2189 - predict_loss: 0.0000e+00 - loss_loss: 7.2189\n",
      "Epoch 219/500\n",
      "256/256 [==============================] - 0s 412us/step - loss: 7.2636 - predict_loss: 0.0000e+00 - loss_loss: 7.2636\n",
      "Epoch 220/500\n",
      "256/256 [==============================] - 0s 462us/step - loss: 6.2238 - predict_loss: 0.0000e+00 - loss_loss: 6.2238\n",
      "Epoch 221/500\n",
      "256/256 [==============================] - 0s 402us/step - loss: 7.9614 - predict_loss: 0.0000e+00 - loss_loss: 7.9614\n",
      "Epoch 222/500\n",
      "256/256 [==============================] - 0s 391us/step - loss: 6.7841 - predict_loss: 0.0000e+00 - loss_loss: 6.7841\n",
      "Epoch 223/500\n",
      "256/256 [==============================] - 0s 459us/step - loss: 6.1219 - predict_loss: 0.0000e+00 - loss_loss: 6.1219\n",
      "Epoch 224/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256/256 [==============================] - 0s 439us/step - loss: 6.3519 - predict_loss: 0.0000e+00 - loss_loss: 6.3519\n",
      "Epoch 225/500\n",
      "256/256 [==============================] - 0s 400us/step - loss: 8.0161 - predict_loss: 0.0000e+00 - loss_loss: 8.0161\n",
      "Epoch 226/500\n",
      "256/256 [==============================] - 0s 413us/step - loss: 7.7166 - predict_loss: 0.0000e+00 - loss_loss: 7.7166\n",
      "Epoch 227/500\n",
      "256/256 [==============================] - 0s 435us/step - loss: 6.1856 - predict_loss: 0.0000e+00 - loss_loss: 6.1856\n",
      "Epoch 228/500\n",
      "256/256 [==============================] - 0s 485us/step - loss: 6.6096 - predict_loss: 0.0000e+00 - loss_loss: 6.6096\n",
      "Epoch 229/500\n",
      "256/256 [==============================] - 0s 432us/step - loss: 6.8404 - predict_loss: 0.0000e+00 - loss_loss: 6.8404\n",
      "Epoch 230/500\n",
      "256/256 [==============================] - 0s 446us/step - loss: 8.0740 - predict_loss: 0.0000e+00 - loss_loss: 8.0740\n",
      "Epoch 231/500\n",
      "256/256 [==============================] - 0s 421us/step - loss: 5.3635 - predict_loss: 0.0000e+00 - loss_loss: 5.3635\n",
      "Epoch 232/500\n",
      "256/256 [==============================] - 0s 425us/step - loss: 7.1531 - predict_loss: 0.0000e+00 - loss_loss: 7.1531\n",
      "Epoch 233/500\n",
      "256/256 [==============================] - 0s 424us/step - loss: 6.8144 - predict_loss: 0.0000e+00 - loss_loss: 6.8144\n",
      "Epoch 234/500\n",
      "256/256 [==============================] - 0s 435us/step - loss: 7.4289 - predict_loss: 0.0000e+00 - loss_loss: 7.4289\n",
      "Epoch 235/500\n",
      "256/256 [==============================] - 0s 384us/step - loss: 7.2852 - predict_loss: 0.0000e+00 - loss_loss: 7.2852\n",
      "Epoch 236/500\n",
      "256/256 [==============================] - 0s 423us/step - loss: 6.2885 - predict_loss: 0.0000e+00 - loss_loss: 6.2885\n",
      "Epoch 237/500\n",
      "256/256 [==============================] - 0s 420us/step - loss: 5.1716 - predict_loss: 0.0000e+00 - loss_loss: 5.1716\n",
      "Epoch 238/500\n",
      "256/256 [==============================] - 0s 440us/step - loss: 6.9401 - predict_loss: 0.0000e+00 - loss_loss: 6.9401\n",
      "Epoch 239/500\n",
      "256/256 [==============================] - 0s 398us/step - loss: 7.8286 - predict_loss: 0.0000e+00 - loss_loss: 7.8286\n",
      "Epoch 240/500\n",
      "256/256 [==============================] - 0s 449us/step - loss: 5.3487 - predict_loss: 0.0000e+00 - loss_loss: 5.3487\n",
      "Epoch 241/500\n",
      "256/256 [==============================] - 0s 426us/step - loss: 8.4492 - predict_loss: 0.0000e+00 - loss_loss: 8.4492\n",
      "Epoch 242/500\n",
      "256/256 [==============================] - 0s 353us/step - loss: 5.7536 - predict_loss: 0.0000e+00 - loss_loss: 5.7536\n",
      "Epoch 243/500\n",
      "256/256 [==============================] - 0s 456us/step - loss: 5.7034 - predict_loss: 0.0000e+00 - loss_loss: 5.7034\n",
      "Epoch 244/500\n",
      "256/256 [==============================] - 0s 445us/step - loss: 6.2319 - predict_loss: 0.0000e+00 - loss_loss: 6.2319\n",
      "Epoch 245/500\n",
      "256/256 [==============================] - 0s 483us/step - loss: 7.2343 - predict_loss: 0.0000e+00 - loss_loss: 7.2343\n",
      "Epoch 246/500\n",
      "256/256 [==============================] - 0s 436us/step - loss: 6.2928 - predict_loss: 0.0000e+00 - loss_loss: 6.2928\n",
      "Epoch 247/500\n",
      "256/256 [==============================] - 0s 454us/step - loss: 6.2348 - predict_loss: 0.0000e+00 - loss_loss: 6.2348\n",
      "Epoch 248/500\n",
      "256/256 [==============================] - 0s 445us/step - loss: 5.8757 - predict_loss: 0.0000e+00 - loss_loss: 5.8757\n",
      "Epoch 249/500\n",
      "256/256 [==============================] - 0s 462us/step - loss: 7.3465 - predict_loss: 0.0000e+00 - loss_loss: 7.3465\n",
      "Epoch 250/500\n",
      "256/256 [==============================] - 0s 443us/step - loss: 6.0453 - predict_loss: 0.0000e+00 - loss_loss: 6.0453\n",
      "Epoch 251/500\n",
      "256/256 [==============================] - 0s 427us/step - loss: 6.6929 - predict_loss: 0.0000e+00 - loss_loss: 6.6929\n",
      "Epoch 252/500\n",
      "256/256 [==============================] - 0s 422us/step - loss: 5.5273 - predict_loss: 0.0000e+00 - loss_loss: 5.5273\n",
      "Epoch 253/500\n",
      "256/256 [==============================] - 0s 413us/step - loss: 6.9197 - predict_loss: 0.0000e+00 - loss_loss: 6.9197\n",
      "Epoch 254/500\n",
      "256/256 [==============================] - 0s 447us/step - loss: 4.9998 - predict_loss: 0.0000e+00 - loss_loss: 4.9998\n",
      "Epoch 255/500\n",
      "256/256 [==============================] - 0s 402us/step - loss: 6.8503 - predict_loss: 0.0000e+00 - loss_loss: 6.8503\n",
      "Epoch 256/500\n",
      "256/256 [==============================] - 0s 497us/step - loss: 8.3349 - predict_loss: 0.0000e+00 - loss_loss: 8.3349\n",
      "Epoch 257/500\n",
      "256/256 [==============================] - 0s 479us/step - loss: 5.2612 - predict_loss: 0.0000e+00 - loss_loss: 5.2612\n",
      "Epoch 258/500\n",
      "256/256 [==============================] - 0s 458us/step - loss: 5.5949 - predict_loss: 0.0000e+00 - loss_loss: 5.5949\n",
      "Epoch 259/500\n",
      "256/256 [==============================] - 0s 424us/step - loss: 6.1790 - predict_loss: 0.0000e+00 - loss_loss: 6.1790\n",
      "Epoch 260/500\n",
      "256/256 [==============================] - 0s 431us/step - loss: 7.3170 - predict_loss: 0.0000e+00 - loss_loss: 7.3170\n",
      "Epoch 261/500\n",
      "256/256 [==============================] - 0s 443us/step - loss: 5.6270 - predict_loss: 0.0000e+00 - loss_loss: 5.6270\n",
      "Epoch 262/500\n",
      "256/256 [==============================] - 0s 451us/step - loss: 6.0056 - predict_loss: 0.0000e+00 - loss_loss: 6.0056\n",
      "Epoch 263/500\n",
      "256/256 [==============================] - 0s 419us/step - loss: 5.3867 - predict_loss: 0.0000e+00 - loss_loss: 5.3867\n",
      "Epoch 264/500\n",
      "256/256 [==============================] - 0s 416us/step - loss: 7.3934 - predict_loss: 0.0000e+00 - loss_loss: 7.3934\n",
      "Epoch 265/500\n",
      "256/256 [==============================] - 0s 449us/step - loss: 5.6814 - predict_loss: 0.0000e+00 - loss_loss: 5.6814\n",
      "Epoch 266/500\n",
      "256/256 [==============================] - 0s 433us/step - loss: 4.8101 - predict_loss: 0.0000e+00 - loss_loss: 4.8101\n",
      "Epoch 267/500\n",
      "256/256 [==============================] - 0s 439us/step - loss: 6.7419 - predict_loss: 0.0000e+00 - loss_loss: 6.7419\n",
      "Epoch 268/500\n",
      "256/256 [==============================] - 0s 429us/step - loss: 6.0629 - predict_loss: 0.0000e+00 - loss_loss: 6.0629\n",
      "Epoch 269/500\n",
      "256/256 [==============================] - 0s 425us/step - loss: 5.3019 - predict_loss: 0.0000e+00 - loss_loss: 5.3019\n",
      "Epoch 270/500\n",
      "256/256 [==============================] - 0s 425us/step - loss: 6.2803 - predict_loss: 0.0000e+00 - loss_loss: 6.2803\n",
      "Epoch 271/500\n",
      "256/256 [==============================] - 0s 409us/step - loss: 5.9552 - predict_loss: 0.0000e+00 - loss_loss: 5.9552\n",
      "Epoch 272/500\n",
      "256/256 [==============================] - 0s 415us/step - loss: 6.4689 - predict_loss: 0.0000e+00 - loss_loss: 6.4689\n",
      "Epoch 273/500\n",
      "256/256 [==============================] - 0s 439us/step - loss: 5.7626 - predict_loss: 0.0000e+00 - loss_loss: 5.7626\n",
      "Epoch 274/500\n",
      "256/256 [==============================] - 0s 437us/step - loss: 5.6335 - predict_loss: 0.0000e+00 - loss_loss: 5.6335\n",
      "Epoch 275/500\n",
      "256/256 [==============================] - 0s 413us/step - loss: 5.6859 - predict_loss: 0.0000e+00 - loss_loss: 5.6859\n",
      "Epoch 276/500\n",
      "256/256 [==============================] - 0s 438us/step - loss: 4.9255 - predict_loss: 0.0000e+00 - loss_loss: 4.9255\n",
      "Epoch 277/500\n",
      "256/256 [==============================] - 0s 421us/step - loss: 6.4352 - predict_loss: 0.0000e+00 - loss_loss: 6.4352\n",
      "Epoch 278/500\n",
      "256/256 [==============================] - 0s 383us/step - loss: 5.8386 - predict_loss: 0.0000e+00 - loss_loss: 5.8386\n",
      "Epoch 279/500\n",
      "256/256 [==============================] - 0s 441us/step - loss: 6.2809 - predict_loss: 0.0000e+00 - loss_loss: 6.2809\n",
      "Epoch 280/500\n",
      "256/256 [==============================] - 0s 479us/step - loss: 6.9837 - predict_loss: 0.0000e+00 - loss_loss: 6.9837\n",
      "Epoch 281/500\n",
      "256/256 [==============================] - 0s 487us/step - loss: 4.7062 - predict_loss: 0.0000e+00 - loss_loss: 4.7062\n",
      "Epoch 282/500\n",
      "256/256 [==============================] - 0s 463us/step - loss: 6.5838 - predict_loss: 0.0000e+00 - loss_loss: 6.5838\n",
      "Epoch 283/500\n",
      "256/256 [==============================] - 0s 462us/step - loss: 5.3406 - predict_loss: 0.0000e+00 - loss_loss: 5.3406\n",
      "Epoch 284/500\n",
      "256/256 [==============================] - 0s 450us/step - loss: 4.7537 - predict_loss: 0.0000e+00 - loss_loss: 4.7537\n",
      "Epoch 285/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256/256 [==============================] - 0s 483us/step - loss: 6.2929 - predict_loss: 0.0000e+00 - loss_loss: 6.2929\n",
      "Epoch 286/500\n",
      "256/256 [==============================] - 0s 405us/step - loss: 5.1188 - predict_loss: 0.0000e+00 - loss_loss: 5.1188\n",
      "Epoch 287/500\n",
      "256/256 [==============================] - 0s 417us/step - loss: 7.0248 - predict_loss: 0.0000e+00 - loss_loss: 7.0248\n",
      "Epoch 288/500\n",
      "256/256 [==============================] - 0s 410us/step - loss: 5.0436 - predict_loss: 0.0000e+00 - loss_loss: 5.0436\n",
      "Epoch 289/500\n",
      "256/256 [==============================] - 0s 411us/step - loss: 4.6308 - predict_loss: 0.0000e+00 - loss_loss: 4.6308\n",
      "Epoch 290/500\n",
      "256/256 [==============================] - 0s 455us/step - loss: 5.4040 - predict_loss: 0.0000e+00 - loss_loss: 5.4040\n",
      "Epoch 291/500\n",
      "256/256 [==============================] - 0s 410us/step - loss: 6.8393 - predict_loss: 0.0000e+00 - loss_loss: 6.8393\n",
      "Epoch 292/500\n",
      "256/256 [==============================] - 0s 410us/step - loss: 4.6752 - predict_loss: 0.0000e+00 - loss_loss: 4.6752\n",
      "Epoch 293/500\n",
      "256/256 [==============================] - 0s 413us/step - loss: 5.3704 - predict_loss: 0.0000e+00 - loss_loss: 5.3704\n",
      "Epoch 294/500\n",
      "256/256 [==============================] - 0s 423us/step - loss: 4.9865 - predict_loss: 0.0000e+00 - loss_loss: 4.9865\n",
      "Epoch 295/500\n",
      "256/256 [==============================] - 0s 407us/step - loss: 7.7774 - predict_loss: 0.0000e+00 - loss_loss: 7.7774\n",
      "Epoch 296/500\n",
      "256/256 [==============================] - 0s 433us/step - loss: 4.1622 - predict_loss: 0.0000e+00 - loss_loss: 4.1622\n",
      "Epoch 297/500\n",
      "256/256 [==============================] - 0s 431us/step - loss: 4.6927 - predict_loss: 0.0000e+00 - loss_loss: 4.6927\n",
      "Epoch 298/500\n",
      "256/256 [==============================] - 0s 495us/step - loss: 5.8428 - predict_loss: 0.0000e+00 - loss_loss: 5.8428\n",
      "Epoch 299/500\n",
      "256/256 [==============================] - 0s 429us/step - loss: 4.9554 - predict_loss: 0.0000e+00 - loss_loss: 4.9554\n",
      "Epoch 300/500\n",
      "256/256 [==============================] - 0s 419us/step - loss: 4.3882 - predict_loss: 0.0000e+00 - loss_loss: 4.3882\n",
      "Epoch 301/500\n",
      "256/256 [==============================] - 0s 466us/step - loss: 7.3270 - predict_loss: 0.0000e+00 - loss_loss: 7.3270\n",
      "Epoch 302/500\n",
      "256/256 [==============================] - 0s 457us/step - loss: 6.2451 - predict_loss: 0.0000e+00 - loss_loss: 6.2451\n",
      "Epoch 303/500\n",
      "256/256 [==============================] - 0s 449us/step - loss: 4.8469 - predict_loss: 0.0000e+00 - loss_loss: 4.8469\n",
      "Epoch 304/500\n",
      "256/256 [==============================] - 0s 403us/step - loss: 5.2877 - predict_loss: 0.0000e+00 - loss_loss: 5.2877\n",
      "Epoch 305/500\n",
      "256/256 [==============================] - 0s 435us/step - loss: 4.3241 - predict_loss: 0.0000e+00 - loss_loss: 4.3241\n",
      "Epoch 306/500\n",
      "256/256 [==============================] - 0s 406us/step - loss: 6.0023 - predict_loss: 0.0000e+00 - loss_loss: 6.0023\n",
      "Epoch 307/500\n",
      "256/256 [==============================] - 0s 460us/step - loss: 5.8067 - predict_loss: 0.0000e+00 - loss_loss: 5.8067\n",
      "Epoch 308/500\n",
      "256/256 [==============================] - 0s 438us/step - loss: 5.8440 - predict_loss: 0.0000e+00 - loss_loss: 5.8440\n",
      "Epoch 309/500\n",
      "256/256 [==============================] - 0s 424us/step - loss: 3.7106 - predict_loss: 0.0000e+00 - loss_loss: 3.7106\n",
      "Epoch 310/500\n",
      "256/256 [==============================] - 0s 450us/step - loss: 6.3588 - predict_loss: 0.0000e+00 - loss_loss: 6.3588\n",
      "Epoch 311/500\n",
      "256/256 [==============================] - 0s 440us/step - loss: 4.7775 - predict_loss: 0.0000e+00 - loss_loss: 4.7775\n",
      "Epoch 312/500\n",
      "256/256 [==============================] - 0s 525us/step - loss: 5.8460 - predict_loss: 0.0000e+00 - loss_loss: 5.8460\n",
      "Epoch 313/500\n",
      "256/256 [==============================] - 0s 449us/step - loss: 5.1999 - predict_loss: 0.0000e+00 - loss_loss: 5.1999\n",
      "Epoch 314/500\n",
      "256/256 [==============================] - 0s 446us/step - loss: 5.1685 - predict_loss: 0.0000e+00 - loss_loss: 5.1685\n",
      "Epoch 315/500\n",
      "256/256 [==============================] - 0s 415us/step - loss: 6.0690 - predict_loss: 0.0000e+00 - loss_loss: 6.0690\n",
      "Epoch 316/500\n",
      "256/256 [==============================] - 0s 414us/step - loss: 3.6945 - predict_loss: 0.0000e+00 - loss_loss: 3.6945\n",
      "Epoch 317/500\n",
      "256/256 [==============================] - 0s 463us/step - loss: 5.8890 - predict_loss: 0.0000e+00 - loss_loss: 5.8890\n",
      "Epoch 318/500\n",
      "256/256 [==============================] - 0s 453us/step - loss: 4.9497 - predict_loss: 0.0000e+00 - loss_loss: 4.9497\n",
      "Epoch 319/500\n",
      "256/256 [==============================] - 0s 497us/step - loss: 5.4800 - predict_loss: 0.0000e+00 - loss_loss: 5.4800\n",
      "Epoch 320/500\n",
      "256/256 [==============================] - 0s 477us/step - loss: 4.4442 - predict_loss: 0.0000e+00 - loss_loss: 4.4442\n",
      "Epoch 321/500\n",
      "256/256 [==============================] - 0s 417us/step - loss: 5.3817 - predict_loss: 0.0000e+00 - loss_loss: 5.3817\n",
      "Epoch 322/500\n",
      "256/256 [==============================] - 0s 463us/step - loss: 5.0215 - predict_loss: 0.0000e+00 - loss_loss: 5.0215\n",
      "Epoch 323/500\n",
      "256/256 [==============================] - 0s 452us/step - loss: 6.5857 - predict_loss: 0.0000e+00 - loss_loss: 6.5857\n",
      "Epoch 324/500\n",
      "256/256 [==============================] - 0s 490us/step - loss: 4.3763 - predict_loss: 0.0000e+00 - loss_loss: 4.3763\n",
      "Epoch 325/500\n",
      "256/256 [==============================] - 0s 486us/step - loss: 4.5272 - predict_loss: 0.0000e+00 - loss_loss: 4.5272\n",
      "Epoch 326/500\n",
      "256/256 [==============================] - 0s 534us/step - loss: 6.0584 - predict_loss: 0.0000e+00 - loss_loss: 6.0584\n",
      "Epoch 327/500\n",
      "256/256 [==============================] - 0s 473us/step - loss: 4.3176 - predict_loss: 0.0000e+00 - loss_loss: 4.3176\n",
      "Epoch 328/500\n",
      "256/256 [==============================] - 0s 507us/step - loss: 5.5235 - predict_loss: 0.0000e+00 - loss_loss: 5.5235\n",
      "Epoch 329/500\n",
      "256/256 [==============================] - 0s 457us/step - loss: 4.2982 - predict_loss: 0.0000e+00 - loss_loss: 4.2982\n",
      "Epoch 330/500\n",
      "256/256 [==============================] - 0s 456us/step - loss: 5.2788 - predict_loss: 0.0000e+00 - loss_loss: 5.2788\n",
      "Epoch 331/500\n",
      "256/256 [==============================] - 0s 432us/step - loss: 5.9895 - predict_loss: 0.0000e+00 - loss_loss: 5.9895\n",
      "Epoch 332/500\n",
      "256/256 [==============================] - 0s 510us/step - loss: 4.6168 - predict_loss: 0.0000e+00 - loss_loss: 4.6168\n",
      "Epoch 333/500\n",
      "256/256 [==============================] - 0s 552us/step - loss: 4.5783 - predict_loss: 0.0000e+00 - loss_loss: 4.5783\n",
      "Epoch 334/500\n",
      "256/256 [==============================] - 0s 435us/step - loss: 5.8257 - predict_loss: 0.0000e+00 - loss_loss: 5.8257\n",
      "Epoch 335/500\n",
      "256/256 [==============================] - 0s 481us/step - loss: 4.3451 - predict_loss: 0.0000e+00 - loss_loss: 4.3451\n",
      "Epoch 336/500\n",
      "256/256 [==============================] - 0s 527us/step - loss: 5.3663 - predict_loss: 0.0000e+00 - loss_loss: 5.3663\n",
      "Epoch 337/500\n",
      "256/256 [==============================] - 0s 522us/step - loss: 3.7902 - predict_loss: 0.0000e+00 - loss_loss: 3.7902\n",
      "Epoch 338/500\n",
      "256/256 [==============================] - 0s 397us/step - loss: 5.8542 - predict_loss: 0.0000e+00 - loss_loss: 5.8542\n",
      "Epoch 339/500\n",
      "256/256 [==============================] - 0s 436us/step - loss: 4.4481 - predict_loss: 0.0000e+00 - loss_loss: 4.4481\n",
      "Epoch 340/500\n",
      "256/256 [==============================] - 0s 485us/step - loss: 4.3484 - predict_loss: 0.0000e+00 - loss_loss: 4.3484\n",
      "Epoch 341/500\n",
      "256/256 [==============================] - 0s 507us/step - loss: 4.4398 - predict_loss: 0.0000e+00 - loss_loss: 4.4398\n",
      "Epoch 342/500\n",
      "256/256 [==============================] - 0s 509us/step - loss: 4.4910 - predict_loss: 0.0000e+00 - loss_loss: 4.4910\n",
      "Epoch 343/500\n",
      "256/256 [==============================] - 0s 469us/step - loss: 5.4623 - predict_loss: 0.0000e+00 - loss_loss: 5.4623\n",
      "Epoch 344/500\n",
      "256/256 [==============================] - 0s 514us/step - loss: 4.8175 - predict_loss: 0.0000e+00 - loss_loss: 4.8175\n",
      "Epoch 345/500\n",
      "256/256 [==============================] - 0s 520us/step - loss: 5.3693 - predict_loss: 0.0000e+00 - loss_loss: 5.3693\n",
      "Epoch 346/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256/256 [==============================] - 0s 464us/step - loss: 4.1215 - predict_loss: 0.0000e+00 - loss_loss: 4.1215\n",
      "Epoch 347/500\n",
      "256/256 [==============================] - 0s 434us/step - loss: 3.9234 - predict_loss: 0.0000e+00 - loss_loss: 3.9234\n",
      "Epoch 348/500\n",
      "256/256 [==============================] - 0s 489us/step - loss: 5.6424 - predict_loss: 0.0000e+00 - loss_loss: 5.6424\n",
      "Epoch 349/500\n",
      "256/256 [==============================] - 0s 524us/step - loss: 4.4254 - predict_loss: 0.0000e+00 - loss_loss: 4.4254\n",
      "Epoch 350/500\n",
      "256/256 [==============================] - 0s 443us/step - loss: 4.5451 - predict_loss: 0.0000e+00 - loss_loss: 4.5451\n",
      "Epoch 351/500\n",
      "256/256 [==============================] - 0s 389us/step - loss: 4.3670 - predict_loss: 0.0000e+00 - loss_loss: 4.3670\n",
      "Epoch 352/500\n",
      "256/256 [==============================] - 0s 374us/step - loss: 4.7824 - predict_loss: 0.0000e+00 - loss_loss: 4.7824\n",
      "Epoch 353/500\n",
      "256/256 [==============================] - 0s 438us/step - loss: 5.3681 - predict_loss: 0.0000e+00 - loss_loss: 5.3681\n",
      "Epoch 354/500\n",
      "256/256 [==============================] - 0s 397us/step - loss: 4.1107 - predict_loss: 0.0000e+00 - loss_loss: 4.1107\n",
      "Epoch 355/500\n",
      "256/256 [==============================] - 0s 439us/step - loss: 5.2421 - predict_loss: 0.0000e+00 - loss_loss: 5.2421\n",
      "Epoch 356/500\n",
      "256/256 [==============================] - 0s 454us/step - loss: 4.1460 - predict_loss: 0.0000e+00 - loss_loss: 4.1460\n",
      "Epoch 357/500\n",
      "256/256 [==============================] - 0s 441us/step - loss: 4.1589 - predict_loss: 0.0000e+00 - loss_loss: 4.1589\n",
      "Epoch 358/500\n",
      "256/256 [==============================] - 0s 446us/step - loss: 5.0883 - predict_loss: 0.0000e+00 - loss_loss: 5.0883\n",
      "Epoch 359/500\n",
      "256/256 [==============================] - 0s 453us/step - loss: 3.7777 - predict_loss: 0.0000e+00 - loss_loss: 3.7777\n",
      "Epoch 360/500\n",
      "256/256 [==============================] - 0s 486us/step - loss: 5.4860 - predict_loss: 0.0000e+00 - loss_loss: 5.4860\n",
      "Epoch 361/500\n",
      "256/256 [==============================] - 0s 415us/step - loss: 4.5511 - predict_loss: 0.0000e+00 - loss_loss: 4.5511\n",
      "Epoch 362/500\n",
      "256/256 [==============================] - 0s 417us/step - loss: 5.0803 - predict_loss: 0.0000e+00 - loss_loss: 5.0803\n",
      "Epoch 363/500\n",
      "256/256 [==============================] - 0s 445us/step - loss: 4.3772 - predict_loss: 0.0000e+00 - loss_loss: 4.3772\n",
      "Epoch 364/500\n",
      "256/256 [==============================] - 0s 421us/step - loss: 4.0749 - predict_loss: 0.0000e+00 - loss_loss: 4.0749\n",
      "Epoch 365/500\n",
      "256/256 [==============================] - 0s 430us/step - loss: 5.2344 - predict_loss: 0.0000e+00 - loss_loss: 5.2344\n",
      "Epoch 366/500\n",
      "256/256 [==============================] - 0s 406us/step - loss: 5.2741 - predict_loss: 0.0000e+00 - loss_loss: 5.2741\n",
      "Epoch 367/500\n",
      "256/256 [==============================] - 0s 724us/step - loss: 3.9810 - predict_loss: 0.0000e+00 - loss_loss: 3.9810\n",
      "Epoch 368/500\n",
      "256/256 [==============================] - 0s 431us/step - loss: 1.9564 - predict_loss: 0.0000e+00 - loss_loss: 1.9564\n",
      "Epoch 369/500\n",
      "256/256 [==============================] - 0s 380us/step - loss: 0.6294 - predict_loss: 0.0000e+00 - loss_loss: 0.6294\n",
      "Epoch 370/500\n",
      "256/256 [==============================] - 0s 433us/step - loss: 0.5662 - predict_loss: 0.0000e+00 - loss_loss: 0.5662\n",
      "Epoch 371/500\n",
      "256/256 [==============================] - 0s 407us/step - loss: 0.5219 - predict_loss: 0.0000e+00 - loss_loss: 0.5219\n",
      "Epoch 372/500\n",
      "256/256 [==============================] - 0s 387us/step - loss: 0.4711 - predict_loss: 0.0000e+00 - loss_loss: 0.4711\n",
      "Epoch 373/500\n",
      "256/256 [==============================] - 0s 437us/step - loss: 0.4489 - predict_loss: 0.0000e+00 - loss_loss: 0.4489\n",
      "Epoch 374/500\n",
      "256/256 [==============================] - 0s 424us/step - loss: 0.4533 - predict_loss: 0.0000e+00 - loss_loss: 0.4533\n",
      "Epoch 375/500\n",
      "256/256 [==============================] - 0s 394us/step - loss: 0.3672 - predict_loss: 0.0000e+00 - loss_loss: 0.3672\n",
      "Epoch 376/500\n",
      "256/256 [==============================] - 0s 426us/step - loss: 0.3724 - predict_loss: 0.0000e+00 - loss_loss: 0.3724\n",
      "Epoch 377/500\n",
      "256/256 [==============================] - 0s 418us/step - loss: 0.3148 - predict_loss: 0.0000e+00 - loss_loss: 0.3148\n",
      "Epoch 378/500\n",
      "256/256 [==============================] - 0s 373us/step - loss: 0.3355 - predict_loss: 0.0000e+00 - loss_loss: 0.3355\n",
      "Epoch 379/500\n",
      "256/256 [==============================] - 0s 449us/step - loss: 0.3203 - predict_loss: 0.0000e+00 - loss_loss: 0.3203\n",
      "Epoch 380/500\n",
      "256/256 [==============================] - 0s 421us/step - loss: 0.3006 - predict_loss: 0.0000e+00 - loss_loss: 0.3006\n",
      "Epoch 381/500\n",
      "256/256 [==============================] - 0s 412us/step - loss: 0.3194 - predict_loss: 0.0000e+00 - loss_loss: 0.3194\n",
      "Epoch 382/500\n",
      "256/256 [==============================] - 0s 482us/step - loss: 0.2274 - predict_loss: 0.0000e+00 - loss_loss: 0.2274\n",
      "Epoch 383/500\n",
      "256/256 [==============================] - 0s 496us/step - loss: 0.3470 - predict_loss: 0.0000e+00 - loss_loss: 0.3470\n",
      "Epoch 384/500\n",
      "256/256 [==============================] - 0s 475us/step - loss: 0.2430 - predict_loss: 0.0000e+00 - loss_loss: 0.2430\n",
      "Epoch 385/500\n",
      "256/256 [==============================] - 0s 414us/step - loss: 0.2608 - predict_loss: 0.0000e+00 - loss_loss: 0.2608\n",
      "Epoch 386/500\n",
      "256/256 [==============================] - 0s 394us/step - loss: 0.2097 - predict_loss: 0.0000e+00 - loss_loss: 0.2097\n",
      "Epoch 387/500\n",
      "256/256 [==============================] - 0s 395us/step - loss: 0.2796 - predict_loss: 0.0000e+00 - loss_loss: 0.2796\n",
      "Epoch 388/500\n",
      "256/256 [==============================] - 0s 444us/step - loss: 0.2307 - predict_loss: 0.0000e+00 - loss_loss: 0.2307\n",
      "Epoch 389/500\n",
      "256/256 [==============================] - 0s 402us/step - loss: 0.2233 - predict_loss: 0.0000e+00 - loss_loss: 0.2233\n",
      "Epoch 390/500\n",
      "256/256 [==============================] - 0s 417us/step - loss: 0.2838 - predict_loss: 0.0000e+00 - loss_loss: 0.2838\n",
      "Epoch 391/500\n",
      "256/256 [==============================] - 0s 383us/step - loss: 0.2188 - predict_loss: 0.0000e+00 - loss_loss: 0.2188\n",
      "Epoch 392/500\n",
      "256/256 [==============================] - 0s 428us/step - loss: 0.2085 - predict_loss: 0.0000e+00 - loss_loss: 0.2085\n",
      "Epoch 393/500\n",
      "256/256 [==============================] - 0s 433us/step - loss: 0.2436 - predict_loss: 0.0000e+00 - loss_loss: 0.2436\n",
      "Epoch 394/500\n",
      "256/256 [==============================] - 0s 438us/step - loss: 0.1881 - predict_loss: 0.0000e+00 - loss_loss: 0.1881\n",
      "Epoch 395/500\n",
      "256/256 [==============================] - 0s 433us/step - loss: 0.2864 - predict_loss: 0.0000e+00 - loss_loss: 0.2864\n",
      "Epoch 396/500\n",
      "256/256 [==============================] - 0s 444us/step - loss: 0.2138 - predict_loss: 0.0000e+00 - loss_loss: 0.2138\n",
      "Epoch 397/500\n",
      "256/256 [==============================] - 0s 451us/step - loss: 0.2352 - predict_loss: 0.0000e+00 - loss_loss: 0.2352\n",
      "Epoch 398/500\n",
      "256/256 [==============================] - 0s 442us/step - loss: 0.1830 - predict_loss: 0.0000e+00 - loss_loss: 0.1830\n",
      "Epoch 399/500\n",
      "256/256 [==============================] - 0s 460us/step - loss: 0.2117 - predict_loss: 0.0000e+00 - loss_loss: 0.2117\n",
      "Epoch 400/500\n",
      "256/256 [==============================] - 0s 443us/step - loss: 0.2270 - predict_loss: 0.0000e+00 - loss_loss: 0.2270\n",
      "Epoch 401/500\n",
      "256/256 [==============================] - 0s 433us/step - loss: 0.1635 - predict_loss: 0.0000e+00 - loss_loss: 0.1635\n",
      "Epoch 402/500\n",
      "256/256 [==============================] - 0s 366us/step - loss: 0.2535 - predict_loss: 0.0000e+00 - loss_loss: 0.2535\n",
      "Epoch 403/500\n",
      "256/256 [==============================] - 0s 416us/step - loss: 0.1760 - predict_loss: 0.0000e+00 - loss_loss: 0.1760\n",
      "Epoch 404/500\n",
      "256/256 [==============================] - 0s 416us/step - loss: 0.2108 - predict_loss: 0.0000e+00 - loss_loss: 0.2108\n",
      "Epoch 405/500\n",
      "256/256 [==============================] - 0s 442us/step - loss: 0.1676 - predict_loss: 0.0000e+00 - loss_loss: 0.1676\n",
      "Epoch 406/500\n",
      "256/256 [==============================] - 0s 464us/step - loss: 0.2273 - predict_loss: 0.0000e+00 - loss_loss: 0.2273\n",
      "Epoch 407/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256/256 [==============================] - 0s 452us/step - loss: 0.1984 - predict_loss: 0.0000e+00 - loss_loss: 0.1984\n",
      "Epoch 408/500\n",
      "256/256 [==============================] - 0s 364us/step - loss: 0.2154 - predict_loss: 0.0000e+00 - loss_loss: 0.2154\n",
      "Epoch 409/500\n",
      "256/256 [==============================] - 0s 379us/step - loss: 0.1877 - predict_loss: 0.0000e+00 - loss_loss: 0.1877\n",
      "Epoch 410/500\n",
      "256/256 [==============================] - 0s 468us/step - loss: 0.2188 - predict_loss: 0.0000e+00 - loss_loss: 0.2188\n",
      "Epoch 411/500\n",
      "256/256 [==============================] - 0s 311us/step - loss: 0.1870 - predict_loss: 0.0000e+00 - loss_loss: 0.1870\n",
      "Epoch 412/500\n",
      "256/256 [==============================] - 0s 440us/step - loss: 0.2060 - predict_loss: 0.0000e+00 - loss_loss: 0.2060\n",
      "Epoch 413/500\n",
      "256/256 [==============================] - 0s 420us/step - loss: 0.2043 - predict_loss: 0.0000e+00 - loss_loss: 0.2043\n",
      "Epoch 414/500\n",
      "256/256 [==============================] - 0s 415us/step - loss: 0.2037 - predict_loss: 0.0000e+00 - loss_loss: 0.2037\n",
      "Epoch 415/500\n",
      "256/256 [==============================] - 0s 455us/step - loss: 0.2053 - predict_loss: 0.0000e+00 - loss_loss: 0.2053\n",
      "Epoch 416/500\n",
      "256/256 [==============================] - 0s 439us/step - loss: 0.1581 - predict_loss: 0.0000e+00 - loss_loss: 0.1581\n",
      "Epoch 417/500\n",
      "256/256 [==============================] - 0s 428us/step - loss: 0.2191 - predict_loss: 0.0000e+00 - loss_loss: 0.2191\n",
      "Epoch 418/500\n",
      "256/256 [==============================] - 0s 405us/step - loss: 0.1490 - predict_loss: 0.0000e+00 - loss_loss: 0.1490\n",
      "Epoch 419/500\n",
      "256/256 [==============================] - 0s 397us/step - loss: 0.2138 - predict_loss: 0.0000e+00 - loss_loss: 0.2138\n",
      "Epoch 420/500\n",
      "256/256 [==============================] - 0s 434us/step - loss: 0.2035 - predict_loss: 0.0000e+00 - loss_loss: 0.2035\n",
      "Epoch 421/500\n",
      "256/256 [==============================] - 0s 381us/step - loss: 0.1531 - predict_loss: 0.0000e+00 - loss_loss: 0.1531\n",
      "Epoch 422/500\n",
      "256/256 [==============================] - 0s 386us/step - loss: 0.2172 - predict_loss: 0.0000e+00 - loss_loss: 0.2172\n",
      "Epoch 423/500\n",
      "256/256 [==============================] - 0s 458us/step - loss: 0.1719 - predict_loss: 0.0000e+00 - loss_loss: 0.1719\n",
      "Epoch 424/500\n",
      "256/256 [==============================] - 0s 408us/step - loss: 0.1881 - predict_loss: 0.0000e+00 - loss_loss: 0.1881\n",
      "Epoch 425/500\n",
      "256/256 [==============================] - 0s 404us/step - loss: 0.1272 - predict_loss: 0.0000e+00 - loss_loss: 0.1272\n",
      "Epoch 426/500\n",
      "256/256 [==============================] - 0s 428us/step - loss: 0.2394 - predict_loss: 0.0000e+00 - loss_loss: 0.2394\n",
      "Epoch 427/500\n",
      "256/256 [==============================] - 0s 426us/step - loss: 0.1796 - predict_loss: 0.0000e+00 - loss_loss: 0.1796\n",
      "Epoch 428/500\n",
      "256/256 [==============================] - 0s 400us/step - loss: 0.1364 - predict_loss: 0.0000e+00 - loss_loss: 0.1364\n",
      "Epoch 429/500\n",
      "256/256 [==============================] - 0s 472us/step - loss: 0.2010 - predict_loss: 0.0000e+00 - loss_loss: 0.2010\n",
      "Epoch 430/500\n",
      "256/256 [==============================] - 0s 451us/step - loss: 0.1921 - predict_loss: 0.0000e+00 - loss_loss: 0.1921\n",
      "Epoch 431/500\n",
      "256/256 [==============================] - 0s 452us/step - loss: 0.1460 - predict_loss: 0.0000e+00 - loss_loss: 0.1460\n",
      "Epoch 432/500\n",
      "256/256 [==============================] - 0s 416us/step - loss: 0.1480 - predict_loss: 0.0000e+00 - loss_loss: 0.1480\n",
      "Epoch 433/500\n",
      "256/256 [==============================] - 0s 490us/step - loss: 0.2094 - predict_loss: 0.0000e+00 - loss_loss: 0.2094\n",
      "Epoch 434/500\n",
      "256/256 [==============================] - 0s 459us/step - loss: 0.1839 - predict_loss: 0.0000e+00 - loss_loss: 0.1839\n",
      "Epoch 435/500\n",
      "256/256 [==============================] - 0s 431us/step - loss: 0.2126 - predict_loss: 0.0000e+00 - loss_loss: 0.2126\n",
      "Epoch 436/500\n",
      "256/256 [==============================] - 0s 469us/step - loss: 0.1490 - predict_loss: 0.0000e+00 - loss_loss: 0.1490\n",
      "Epoch 437/500\n",
      "256/256 [==============================] - 0s 418us/step - loss: 0.2156 - predict_loss: 0.0000e+00 - loss_loss: 0.2156\n",
      "Epoch 438/500\n",
      "256/256 [==============================] - 0s 428us/step - loss: 0.1610 - predict_loss: 0.0000e+00 - loss_loss: 0.1610\n",
      "Epoch 439/500\n",
      "256/256 [==============================] - 0s 418us/step - loss: 0.1962 - predict_loss: 0.0000e+00 - loss_loss: 0.1962\n",
      "Epoch 440/500\n",
      "256/256 [==============================] - 0s 445us/step - loss: 0.1653 - predict_loss: 0.0000e+00 - loss_loss: 0.1653\n",
      "Epoch 441/500\n",
      "256/256 [==============================] - 0s 438us/step - loss: 0.1832 - predict_loss: 0.0000e+00 - loss_loss: 0.1832\n",
      "Epoch 442/500\n",
      "256/256 [==============================] - 0s 459us/step - loss: 0.1615 - predict_loss: 0.0000e+00 - loss_loss: 0.1615\n",
      "Epoch 443/500\n",
      "256/256 [==============================] - 0s 481us/step - loss: 0.2093 - predict_loss: 0.0000e+00 - loss_loss: 0.2093\n",
      "Epoch 444/500\n",
      "256/256 [==============================] - 0s 488us/step - loss: 0.1399 - predict_loss: 0.0000e+00 - loss_loss: 0.1399\n",
      "Epoch 445/500\n",
      "256/256 [==============================] - 0s 490us/step - loss: 0.2120 - predict_loss: 0.0000e+00 - loss_loss: 0.2120\n",
      "Epoch 446/500\n",
      "256/256 [==============================] - 0s 508us/step - loss: 0.1813 - predict_loss: 0.0000e+00 - loss_loss: 0.1813\n",
      "Epoch 447/500\n",
      "256/256 [==============================] - 0s 448us/step - loss: 0.1282 - predict_loss: 0.0000e+00 - loss_loss: 0.1282\n",
      "Epoch 448/500\n",
      "256/256 [==============================] - 0s 484us/step - loss: 0.1926 - predict_loss: 0.0000e+00 - loss_loss: 0.1926\n",
      "Epoch 449/500\n",
      "256/256 [==============================] - 0s 459us/step - loss: 0.1421 - predict_loss: 0.0000e+00 - loss_loss: 0.1421\n",
      "Epoch 450/500\n",
      "256/256 [==============================] - 0s 476us/step - loss: 0.2004 - predict_loss: 0.0000e+00 - loss_loss: 0.2004\n",
      "Epoch 451/500\n",
      "256/256 [==============================] - 0s 500us/step - loss: 0.1671 - predict_loss: 0.0000e+00 - loss_loss: 0.1671\n",
      "Epoch 452/500\n",
      "256/256 [==============================] - 0s 476us/step - loss: 0.1386 - predict_loss: 0.0000e+00 - loss_loss: 0.1386\n",
      "Epoch 453/500\n",
      "256/256 [==============================] - 0s 447us/step - loss: 0.2085 - predict_loss: 0.0000e+00 - loss_loss: 0.2085\n",
      "Epoch 454/500\n",
      "256/256 [==============================] - 0s 396us/step - loss: 0.1673 - predict_loss: 0.0000e+00 - loss_loss: 0.1673\n",
      "Epoch 455/500\n",
      "256/256 [==============================] - 0s 456us/step - loss: 0.1677 - predict_loss: 0.0000e+00 - loss_loss: 0.1677\n",
      "Epoch 456/500\n",
      "256/256 [==============================] - 0s 441us/step - loss: 0.1511 - predict_loss: 0.0000e+00 - loss_loss: 0.1511\n",
      "Epoch 457/500\n",
      "256/256 [==============================] - 0s 403us/step - loss: 0.1864 - predict_loss: 0.0000e+00 - loss_loss: 0.1864\n",
      "Epoch 458/500\n",
      "256/256 [==============================] - 0s 366us/step - loss: 0.1548 - predict_loss: 0.0000e+00 - loss_loss: 0.1548\n",
      "Epoch 459/500\n",
      "256/256 [==============================] - 0s 459us/step - loss: 0.1743 - predict_loss: 0.0000e+00 - loss_loss: 0.1743\n",
      "Epoch 460/500\n",
      "256/256 [==============================] - 0s 438us/step - loss: 0.1807 - predict_loss: 0.0000e+00 - loss_loss: 0.1807\n",
      "Epoch 461/500\n",
      "256/256 [==============================] - 0s 410us/step - loss: 0.1607 - predict_loss: 0.0000e+00 - loss_loss: 0.1607\n",
      "Epoch 462/500\n",
      "256/256 [==============================] - 0s 447us/step - loss: 0.1583 - predict_loss: 0.0000e+00 - loss_loss: 0.1583\n",
      "Epoch 463/500\n",
      "256/256 [==============================] - 0s 484us/step - loss: 0.1482 - predict_loss: 0.0000e+00 - loss_loss: 0.1482\n",
      "Epoch 464/500\n",
      "256/256 [==============================] - 0s 433us/step - loss: 0.2014 - predict_loss: 0.0000e+00 - loss_loss: 0.2014\n",
      "Epoch 465/500\n",
      "256/256 [==============================] - 0s 427us/step - loss: 0.1677 - predict_loss: 0.0000e+00 - loss_loss: 0.1677\n",
      "Epoch 466/500\n",
      "256/256 [==============================] - 0s 468us/step - loss: 0.1899 - predict_loss: 0.0000e+00 - loss_loss: 0.1899\n",
      "Epoch 467/500\n",
      "256/256 [==============================] - 0s 432us/step - loss: 0.1549 - predict_loss: 0.0000e+00 - loss_loss: 0.1549\n",
      "Epoch 468/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256/256 [==============================] - 0s 442us/step - loss: 0.1787 - predict_loss: 0.0000e+00 - loss_loss: 0.1787\n",
      "Epoch 469/500\n",
      "256/256 [==============================] - 0s 385us/step - loss: 0.1599 - predict_loss: 0.0000e+00 - loss_loss: 0.1599\n",
      "Epoch 470/500\n",
      "256/256 [==============================] - 0s 388us/step - loss: 0.1575 - predict_loss: 0.0000e+00 - loss_loss: 0.1575\n",
      "Epoch 471/500\n",
      "256/256 [==============================] - 0s 418us/step - loss: 0.1841 - predict_loss: 0.0000e+00 - loss_loss: 0.1841\n",
      "Epoch 472/500\n",
      "256/256 [==============================] - 0s 390us/step - loss: 0.1676 - predict_loss: 0.0000e+00 - loss_loss: 0.1676\n",
      "Epoch 473/500\n",
      "256/256 [==============================] - 0s 425us/step - loss: 0.1402 - predict_loss: 0.0000e+00 - loss_loss: 0.1402\n",
      "Epoch 474/500\n",
      "256/256 [==============================] - 0s 445us/step - loss: 0.1883 - predict_loss: 0.0000e+00 - loss_loss: 0.1883\n",
      "Epoch 475/500\n",
      "256/256 [==============================] - 0s 457us/step - loss: 0.1466 - predict_loss: 0.0000e+00 - loss_loss: 0.1466\n",
      "Epoch 476/500\n",
      "256/256 [==============================] - 0s 454us/step - loss: 0.1953 - predict_loss: 0.0000e+00 - loss_loss: 0.1953\n",
      "Epoch 477/500\n",
      "256/256 [==============================] - 0s 425us/step - loss: 0.1010 - predict_loss: 0.0000e+00 - loss_loss: 0.1010\n",
      "Epoch 478/500\n",
      "256/256 [==============================] - 0s 394us/step - loss: 0.0861 - predict_loss: 0.0000e+00 - loss_loss: 0.0861\n",
      "Epoch 479/500\n",
      "256/256 [==============================] - 0s 436us/step - loss: 0.0856 - predict_loss: 0.0000e+00 - loss_loss: 0.0856\n",
      "Epoch 480/500\n",
      "256/256 [==============================] - 0s 419us/step - loss: 0.0846 - predict_loss: 0.0000e+00 - loss_loss: 0.0846\n",
      "Epoch 481/500\n",
      "256/256 [==============================] - 0s 405us/step - loss: 0.0849 - predict_loss: 0.0000e+00 - loss_loss: 0.0849\n",
      "Epoch 482/500\n",
      "256/256 [==============================] - 0s 462us/step - loss: 0.0838 - predict_loss: 0.0000e+00 - loss_loss: 0.0838\n",
      "Epoch 483/500\n",
      "256/256 [==============================] - 0s 477us/step - loss: 0.0839 - predict_loss: 0.0000e+00 - loss_loss: 0.0839\n",
      "Epoch 484/500\n",
      "256/256 [==============================] - 0s 443us/step - loss: 0.0855 - predict_loss: 0.0000e+00 - loss_loss: 0.0855\n",
      "Epoch 485/500\n",
      "256/256 [==============================] - 0s 421us/step - loss: 0.0839 - predict_loss: 0.0000e+00 - loss_loss: 0.0839\n",
      "Epoch 486/500\n",
      "256/256 [==============================] - 0s 431us/step - loss: 0.0841 - predict_loss: 0.0000e+00 - loss_loss: 0.0841\n",
      "Epoch 487/500\n",
      "256/256 [==============================] - 0s 404us/step - loss: 0.0837 - predict_loss: 0.0000e+00 - loss_loss: 0.0837\n",
      "Epoch 488/500\n",
      "256/256 [==============================] - 0s 422us/step - loss: 0.0831 - predict_loss: 0.0000e+00 - loss_loss: 0.0831\n",
      "Epoch 489/500\n",
      "256/256 [==============================] - 0s 445us/step - loss: 0.0833 - predict_loss: 0.0000e+00 - loss_loss: 0.0833\n",
      "Epoch 490/500\n",
      "256/256 [==============================] - 0s 439us/step - loss: 0.0844 - predict_loss: 0.0000e+00 - loss_loss: 0.0844\n",
      "Epoch 491/500\n",
      "256/256 [==============================] - 0s 432us/step - loss: 0.0833 - predict_loss: 0.0000e+00 - loss_loss: 0.0833\n",
      "Epoch 492/500\n",
      "256/256 [==============================] - 0s 425us/step - loss: 0.0830 - predict_loss: 0.0000e+00 - loss_loss: 0.0830\n",
      "Epoch 493/500\n",
      "256/256 [==============================] - 0s 456us/step - loss: 0.0826 - predict_loss: 0.0000e+00 - loss_loss: 0.0826\n",
      "Epoch 494/500\n",
      "256/256 [==============================] - 0s 408us/step - loss: 0.0832 - predict_loss: 0.0000e+00 - loss_loss: 0.0832\n",
      "Epoch 495/500\n",
      "256/256 [==============================] - 0s 487us/step - loss: 0.0828 - predict_loss: 0.0000e+00 - loss_loss: 0.0828\n",
      "Epoch 496/500\n",
      "256/256 [==============================] - 0s 430us/step - loss: 0.0836 - predict_loss: 0.0000e+00 - loss_loss: 0.0836\n",
      "Epoch 497/500\n",
      "256/256 [==============================] - 0s 413us/step - loss: 0.0823 - predict_loss: 0.0000e+00 - loss_loss: 0.0823\n",
      "Epoch 498/500\n",
      "256/256 [==============================] - 0s 417us/step - loss: 0.0833 - predict_loss: 0.0000e+00 - loss_loss: 0.0833\n",
      "Epoch 499/500\n",
      "256/256 [==============================] - 0s 413us/step - loss: 0.0822 - predict_loss: 0.0000e+00 - loss_loss: 0.0822\n",
      "Epoch 500/500\n",
      "256/256 [==============================] - 0s 461us/step - loss: 0.0834 - predict_loss: 0.0000e+00 - loss_loss: 0.0834\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fdbe32811d0>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now run the training!\n",
    "# try to overfit a batch first...\n",
    "\n",
    "INPUT_LENGTH = 8\n",
    "OUTPUT_LENGTH = 4\n",
    "TOTAL_LENGTH = INPUT_LENGTH + OUTPUT_LENGTH\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "inp_ph,target_ph, model, params, loss = vanilla_lstm_model(128,\n",
    "                                                           INPUT_LENGTH,\n",
    "                                                           OUTPUT_LENGTH,\n",
    "                                                          1e-3)\n",
    "model.summary()\n",
    "indices, input_batch, target_batch = get_batch(normalized_data,BATCH_SIZE,INPUT_LENGTH,OUTPUT_LENGTH)\n",
    "# prepare data\n",
    "input_batch_padded = np.hstack([input_batch,np.zeros((BATCH_SIZE,OUTPUT_LENGTH,2))])\n",
    "target_batch_padded = np.hstack([np.zeros((BATCH_SIZE,INPUT_LENGTH,2)),target_batch])\n",
    "\n",
    "\n",
    "# and train\n",
    "model.fit(\n",
    "    [input_batch_padded,target_batch_padded],\n",
    "    [np.zeros((BATCH_SIZE,TOTAL_LENGTH,2)),np.zeros(BATCH_SIZE)],\n",
    "    epochs = 500,\n",
    "    callbacks = get_callbacks(input_batch_padded,target_batch_padded)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "256/256 [==============================] - 0s 2ms/step - loss: 0.0857 - predict_loss: 0.0000e+00 - loss_loss: 0.0857\n",
      "Epoch 2/500\n",
      "256/256 [==============================] - 0s 486us/step - loss: 0.0816 - predict_loss: 0.0000e+00 - loss_loss: 0.0816\n",
      "Epoch 3/500\n",
      "256/256 [==============================] - 0s 497us/step - loss: 0.0821 - predict_loss: 0.0000e+00 - loss_loss: 0.0821\n",
      "Epoch 4/500\n",
      "256/256 [==============================] - 0s 482us/step - loss: 0.0813 - predict_loss: 0.0000e+00 - loss_loss: 0.0813\n",
      "Epoch 5/500\n",
      "256/256 [==============================] - 0s 453us/step - loss: 0.0821 - predict_loss: 0.0000e+00 - loss_loss: 0.0821\n",
      "Epoch 6/500\n",
      "256/256 [==============================] - 0s 431us/step - loss: 0.0821 - predict_loss: 0.0000e+00 - loss_loss: 0.0821\n",
      "Epoch 7/500\n",
      "256/256 [==============================] - 0s 479us/step - loss: 0.0815 - predict_loss: 0.0000e+00 - loss_loss: 0.0815\n",
      "Epoch 8/500\n",
      "256/256 [==============================] - 0s 441us/step - loss: 0.0823 - predict_loss: 0.0000e+00 - loss_loss: 0.0823\n",
      "Epoch 9/500\n",
      "256/256 [==============================] - 0s 463us/step - loss: 0.0811 - predict_loss: 0.0000e+00 - loss_loss: 0.0811\n",
      "Epoch 10/500\n",
      "256/256 [==============================] - 0s 419us/step - loss: 0.0809 - predict_loss: 0.0000e+00 - loss_loss: 0.0809\n",
      "Epoch 11/500\n",
      "256/256 [==============================] - 0s 425us/step - loss: 0.0804 - predict_loss: 0.0000e+00 - loss_loss: 0.0804\n",
      "Epoch 12/500\n",
      "256/256 [==============================] - 0s 462us/step - loss: 0.0808 - predict_loss: 0.0000e+00 - loss_loss: 0.0808\n",
      "Epoch 13/500\n",
      "256/256 [==============================] - 0s 538us/step - loss: 0.0821 - predict_loss: 0.0000e+00 - loss_loss: 0.0821\n",
      "Epoch 14/500\n",
      "256/256 [==============================] - 0s 486us/step - loss: 0.0813 - predict_loss: 0.0000e+00 - loss_loss: 0.0813\n",
      "Epoch 15/500\n",
      "256/256 [==============================] - 0s 504us/step - loss: 0.0808 - predict_loss: 0.0000e+00 - loss_loss: 0.0808\n",
      "Epoch 16/500\n",
      "256/256 [==============================] - 0s 469us/step - loss: 0.0801 - predict_loss: 0.0000e+00 - loss_loss: 0.0801\n",
      "Epoch 17/500\n",
      "256/256 [==============================] - 0s 462us/step - loss: 0.0805 - predict_loss: 0.0000e+00 - loss_loss: 0.0805\n",
      "Epoch 18/500\n",
      "256/256 [==============================] - 0s 481us/step - loss: 0.0801 - predict_loss: 0.0000e+00 - loss_loss: 0.0801\n",
      "Epoch 19/500\n",
      "256/256 [==============================] - 0s 428us/step - loss: 0.0805 - predict_loss: 0.0000e+00 - loss_loss: 0.0805\n",
      "Epoch 20/500\n",
      "256/256 [==============================] - 0s 415us/step - loss: 0.0816 - predict_loss: 0.0000e+00 - loss_loss: 0.0816\n",
      "Epoch 21/500\n",
      "256/256 [==============================] - 0s 427us/step - loss: 0.0801 - predict_loss: 0.0000e+00 - loss_loss: 0.0801\n",
      "Epoch 22/500\n",
      "256/256 [==============================] - 0s 439us/step - loss: 0.0794 - predict_loss: 0.0000e+00 - loss_loss: 0.0794\n",
      "Epoch 23/500\n",
      "256/256 [==============================] - 0s 477us/step - loss: 0.0789 - predict_loss: 0.0000e+00 - loss_loss: 0.0789\n",
      "Epoch 24/500\n",
      "256/256 [==============================] - 0s 459us/step - loss: 0.0807 - predict_loss: 0.0000e+00 - loss_loss: 0.0807\n",
      "Epoch 25/500\n",
      "256/256 [==============================] - 0s 451us/step - loss: 0.0809 - predict_loss: 0.0000e+00 - loss_loss: 0.0809\n",
      "Epoch 26/500\n",
      "256/256 [==============================] - 0s 467us/step - loss: 0.0795 - predict_loss: 0.0000e+00 - loss_loss: 0.0795\n",
      "Epoch 27/500\n",
      "256/256 [==============================] - 0s 459us/step - loss: 0.0791 - predict_loss: 0.0000e+00 - loss_loss: 0.0791\n",
      "Epoch 28/500\n",
      "256/256 [==============================] - 0s 439us/step - loss: 0.0817 - predict_loss: 0.0000e+00 - loss_loss: 0.0817\n",
      "Epoch 29/500\n",
      "256/256 [==============================] - 0s 449us/step - loss: 0.0788 - predict_loss: 0.0000e+00 - loss_loss: 0.0788\n",
      "Epoch 30/500\n",
      "256/256 [==============================] - 0s 432us/step - loss: 0.0802 - predict_loss: 0.0000e+00 - loss_loss: 0.0802\n",
      "Epoch 31/500\n",
      "256/256 [==============================] - 0s 499us/step - loss: 0.0804 - predict_loss: 0.0000e+00 - loss_loss: 0.0804\n",
      "Epoch 32/500\n",
      "256/256 [==============================] - 0s 450us/step - loss: 0.0788 - predict_loss: 0.0000e+00 - loss_loss: 0.0788\n",
      "Epoch 33/500\n",
      "256/256 [==============================] - 0s 472us/step - loss: 0.0788 - predict_loss: 0.0000e+00 - loss_loss: 0.0788\n",
      "Epoch 34/500\n",
      "256/256 [==============================] - 0s 562us/step - loss: 0.0798 - predict_loss: 0.0000e+00 - loss_loss: 0.0798\n",
      "Epoch 35/500\n",
      "256/256 [==============================] - 0s 587us/step - loss: 0.0788 - predict_loss: 0.0000e+00 - loss_loss: 0.0788\n",
      "Epoch 36/500\n",
      "256/256 [==============================] - 0s 475us/step - loss: 0.0789 - predict_loss: 0.0000e+00 - loss_loss: 0.0789\n",
      "Epoch 37/500\n",
      "256/256 [==============================] - 0s 458us/step - loss: 0.0778 - predict_loss: 0.0000e+00 - loss_loss: 0.0778\n",
      "Epoch 38/500\n",
      "256/256 [==============================] - 0s 518us/step - loss: 0.0789 - predict_loss: 0.0000e+00 - loss_loss: 0.0789\n",
      "Epoch 39/500\n",
      "256/256 [==============================] - 0s 572us/step - loss: 0.0788 - predict_loss: 0.0000e+00 - loss_loss: 0.0788\n",
      "Epoch 40/500\n",
      "256/256 [==============================] - 0s 488us/step - loss: 0.0795 - predict_loss: 0.0000e+00 - loss_loss: 0.0795\n",
      "Epoch 41/500\n",
      "256/256 [==============================] - 0s 451us/step - loss: 0.0803 - predict_loss: 0.0000e+00 - loss_loss: 0.0803\n",
      "Epoch 42/500\n",
      "256/256 [==============================] - 0s 464us/step - loss: 0.0785 - predict_loss: 0.0000e+00 - loss_loss: 0.0785\n",
      "Epoch 43/500\n",
      "256/256 [==============================] - 0s 457us/step - loss: 0.0787 - predict_loss: 0.0000e+00 - loss_loss: 0.0787\n",
      "Epoch 44/500\n",
      "256/256 [==============================] - 0s 537us/step - loss: 0.0785 - predict_loss: 0.0000e+00 - loss_loss: 0.0785\n",
      "Epoch 45/500\n",
      "256/256 [==============================] - 0s 493us/step - loss: 0.0787 - predict_loss: 0.0000e+00 - loss_loss: 0.0787\n",
      "Epoch 46/500\n",
      "256/256 [==============================] - 0s 588us/step - loss: 0.0804 - predict_loss: 0.0000e+00 - loss_loss: 0.0804\n",
      "Epoch 47/500\n",
      "256/256 [==============================] - 0s 489us/step - loss: 0.0792 - predict_loss: 0.0000e+00 - loss_loss: 0.0792\n",
      "Epoch 48/500\n",
      "256/256 [==============================] - 0s 525us/step - loss: 0.0785 - predict_loss: 0.0000e+00 - loss_loss: 0.0785\n",
      "Epoch 49/500\n",
      "256/256 [==============================] - 0s 532us/step - loss: 0.0787 - predict_loss: 0.0000e+00 - loss_loss: 0.0787\n",
      "Epoch 50/500\n",
      "256/256 [==============================] - 0s 481us/step - loss: 0.0770 - predict_loss: 0.0000e+00 - loss_loss: 0.0770\n",
      "Epoch 51/500\n",
      "256/256 [==============================] - 0s 435us/step - loss: 0.0786 - predict_loss: 0.0000e+00 - loss_loss: 0.0786\n",
      "Epoch 52/500\n",
      "256/256 [==============================] - 0s 424us/step - loss: 0.0774 - predict_loss: 0.0000e+00 - loss_loss: 0.0774\n",
      "Epoch 53/500\n",
      "256/256 [==============================] - 0s 477us/step - loss: 0.0783 - predict_loss: 0.0000e+00 - loss_loss: 0.0783\n",
      "Epoch 54/500\n",
      "256/256 [==============================] - 0s 434us/step - loss: 0.0785 - predict_loss: 0.0000e+00 - loss_loss: 0.0785\n",
      "Epoch 55/500\n",
      "256/256 [==============================] - 0s 474us/step - loss: 0.0765 - predict_loss: 0.0000e+00 - loss_loss: 0.0765\n",
      "Epoch 56/500\n",
      "256/256 [==============================] - 0s 466us/step - loss: 0.0785 - predict_loss: 0.0000e+00 - loss_loss: 0.0785\n",
      "Epoch 57/500\n",
      "256/256 [==============================] - 0s 525us/step - loss: 0.0796 - predict_loss: 0.0000e+00 - loss_loss: 0.0796\n",
      "Epoch 58/500\n",
      "256/256 [==============================] - 0s 511us/step - loss: 0.0780 - predict_loss: 0.0000e+00 - loss_loss: 0.0780\n",
      "Epoch 59/500\n",
      "256/256 [==============================] - 0s 495us/step - loss: 0.0793 - predict_loss: 0.0000e+00 - loss_loss: 0.0793\n",
      "Epoch 60/500\n",
      "256/256 [==============================] - 0s 513us/step - loss: 0.0770 - predict_loss: 0.0000e+00 - loss_loss: 0.0770\n",
      "Epoch 61/500\n",
      "256/256 [==============================] - 0s 451us/step - loss: 0.0785 - predict_loss: 0.0000e+00 - loss_loss: 0.0785\n",
      "Epoch 62/500\n",
      "256/256 [==============================] - 0s 414us/step - loss: 0.0776 - predict_loss: 0.0000e+00 - loss_loss: 0.0776\n",
      "Epoch 63/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256/256 [==============================] - 0s 469us/step - loss: 0.0763 - predict_loss: 0.0000e+00 - loss_loss: 0.0763\n",
      "Epoch 64/500\n",
      "256/256 [==============================] - 0s 378us/step - loss: 0.0773 - predict_loss: 0.0000e+00 - loss_loss: 0.0773\n",
      "Epoch 65/500\n",
      "256/256 [==============================] - 0s 396us/step - loss: 0.0775 - predict_loss: 0.0000e+00 - loss_loss: 0.0775\n",
      "Epoch 66/500\n",
      "256/256 [==============================] - 0s 574us/step - loss: 0.0780 - predict_loss: 0.0000e+00 - loss_loss: 0.0780\n",
      "Epoch 67/500\n",
      "256/256 [==============================] - 0s 558us/step - loss: 0.0762 - predict_loss: 0.0000e+00 - loss_loss: 0.0762\n",
      "Epoch 68/500\n",
      "256/256 [==============================] - 0s 541us/step - loss: 0.0788 - predict_loss: 0.0000e+00 - loss_loss: 0.0788\n",
      "Epoch 69/500\n",
      "256/256 [==============================] - 0s 431us/step - loss: 0.0764 - predict_loss: 0.0000e+00 - loss_loss: 0.0764\n",
      "Epoch 70/500\n",
      "256/256 [==============================] - 0s 434us/step - loss: 0.0776 - predict_loss: 0.0000e+00 - loss_loss: 0.0776\n",
      "Epoch 71/500\n",
      "256/256 [==============================] - 0s 544us/step - loss: 0.0767 - predict_loss: 0.0000e+00 - loss_loss: 0.0767\n",
      "Epoch 72/500\n",
      "256/256 [==============================] - 0s 602us/step - loss: 0.0773 - predict_loss: 0.0000e+00 - loss_loss: 0.0773\n",
      "Epoch 73/500\n",
      "256/256 [==============================] - 0s 586us/step - loss: 0.0762 - predict_loss: 0.0000e+00 - loss_loss: 0.0762\n",
      "Epoch 74/500\n",
      "256/256 [==============================] - 0s 426us/step - loss: 0.0772 - predict_loss: 0.0000e+00 - loss_loss: 0.0772\n",
      "Epoch 75/500\n",
      "256/256 [==============================] - 0s 486us/step - loss: 0.0771 - predict_loss: 0.0000e+00 - loss_loss: 0.0771\n",
      "Epoch 76/500\n",
      "256/256 [==============================] - 0s 510us/step - loss: 0.0777 - predict_loss: 0.0000e+00 - loss_loss: 0.0777\n",
      "Epoch 77/500\n",
      "256/256 [==============================] - 0s 415us/step - loss: 0.0764 - predict_loss: 0.0000e+00 - loss_loss: 0.0764\n",
      "Epoch 78/500\n",
      "256/256 [==============================] - 0s 530us/step - loss: 0.0768 - predict_loss: 0.0000e+00 - loss_loss: 0.0768\n",
      "Epoch 79/500\n",
      "256/256 [==============================] - 0s 528us/step - loss: 0.0756 - predict_loss: 0.0000e+00 - loss_loss: 0.0756\n",
      "Epoch 80/500\n",
      "256/256 [==============================] - 0s 530us/step - loss: 0.0754 - predict_loss: 0.0000e+00 - loss_loss: 0.0754\n",
      "Epoch 81/500\n",
      "256/256 [==============================] - 0s 412us/step - loss: 0.0788 - predict_loss: 0.0000e+00 - loss_loss: 0.0788\n",
      "Epoch 82/500\n",
      "256/256 [==============================] - 0s 513us/step - loss: 0.0763 - predict_loss: 0.0000e+00 - loss_loss: 0.0763\n",
      "Epoch 83/500\n",
      "256/256 [==============================] - 0s 630us/step - loss: 0.0770 - predict_loss: 0.0000e+00 - loss_loss: 0.0770\n",
      "Epoch 84/500\n",
      "256/256 [==============================] - 0s 448us/step - loss: 0.0768 - predict_loss: 0.0000e+00 - loss_loss: 0.0768\n",
      "Epoch 85/500\n",
      "256/256 [==============================] - 0s 356us/step - loss: 0.0757 - predict_loss: 0.0000e+00 - loss_loss: 0.0757\n",
      "Epoch 86/500\n",
      "256/256 [==============================] - 0s 411us/step - loss: 0.0771 - predict_loss: 0.0000e+00 - loss_loss: 0.0771\n",
      "Epoch 87/500\n",
      "256/256 [==============================] - 0s 433us/step - loss: 0.0760 - predict_loss: 0.0000e+00 - loss_loss: 0.0760\n",
      "Epoch 88/500\n",
      "256/256 [==============================] - 0s 398us/step - loss: 0.0749 - predict_loss: 0.0000e+00 - loss_loss: 0.0749\n",
      "Epoch 89/500\n",
      "256/256 [==============================] - 0s 385us/step - loss: 0.0758 - predict_loss: 0.0000e+00 - loss_loss: 0.0758\n",
      "Epoch 90/500\n",
      "256/256 [==============================] - 0s 478us/step - loss: 0.0756 - predict_loss: 0.0000e+00 - loss_loss: 0.0756\n",
      "Epoch 91/500\n",
      "256/256 [==============================] - 0s 499us/step - loss: 0.0759 - predict_loss: 0.0000e+00 - loss_loss: 0.0759\n",
      "Epoch 92/500\n",
      "256/256 [==============================] - 0s 452us/step - loss: 0.0761 - predict_loss: 0.0000e+00 - loss_loss: 0.0761\n",
      "Epoch 93/500\n",
      "256/256 [==============================] - 0s 402us/step - loss: 0.0757 - predict_loss: 0.0000e+00 - loss_loss: 0.0757\n",
      "Epoch 94/500\n",
      "256/256 [==============================] - 0s 449us/step - loss: 0.0755 - predict_loss: 0.0000e+00 - loss_loss: 0.0755\n",
      "Epoch 95/500\n",
      "256/256 [==============================] - 0s 397us/step - loss: 0.0760 - predict_loss: 0.0000e+00 - loss_loss: 0.0760\n",
      "Epoch 96/500\n",
      "256/256 [==============================] - 0s 431us/step - loss: 0.0750 - predict_loss: 0.0000e+00 - loss_loss: 0.0750\n",
      "Epoch 97/500\n",
      "256/256 [==============================] - 0s 387us/step - loss: 0.0750 - predict_loss: 0.0000e+00 - loss_loss: 0.0750\n",
      "Epoch 98/500\n",
      "256/256 [==============================] - 0s 360us/step - loss: 0.0752 - predict_loss: 0.0000e+00 - loss_loss: 0.0752\n",
      "Epoch 99/500\n",
      "256/256 [==============================] - 0s 361us/step - loss: 0.0759 - predict_loss: 0.0000e+00 - loss_loss: 0.0759\n",
      "Epoch 100/500\n",
      "256/256 [==============================] - 0s 417us/step - loss: 0.0758 - predict_loss: 0.0000e+00 - loss_loss: 0.0758\n",
      "Epoch 101/500\n",
      "256/256 [==============================] - 0s 425us/step - loss: 0.0761 - predict_loss: 0.0000e+00 - loss_loss: 0.0761\n",
      "Epoch 102/500\n",
      "256/256 [==============================] - 0s 404us/step - loss: 0.0754 - predict_loss: 0.0000e+00 - loss_loss: 0.0754\n",
      "Epoch 103/500\n",
      "256/256 [==============================] - 0s 424us/step - loss: 0.0749 - predict_loss: 0.0000e+00 - loss_loss: 0.0749\n",
      "Epoch 104/500\n",
      "256/256 [==============================] - 0s 484us/step - loss: 0.0748 - predict_loss: 0.0000e+00 - loss_loss: 0.0748\n",
      "Epoch 105/500\n",
      "256/256 [==============================] - 0s 551us/step - loss: 0.0760 - predict_loss: 0.0000e+00 - loss_loss: 0.0760\n",
      "Epoch 106/500\n",
      "256/256 [==============================] - 0s 531us/step - loss: 0.0750 - predict_loss: 0.0000e+00 - loss_loss: 0.0750\n",
      "Epoch 107/500\n",
      "256/256 [==============================] - 0s 500us/step - loss: 0.0756 - predict_loss: 0.0000e+00 - loss_loss: 0.0756\n",
      "Epoch 108/500\n",
      "256/256 [==============================] - 0s 477us/step - loss: 0.0747 - predict_loss: 0.0000e+00 - loss_loss: 0.0747\n",
      "Epoch 109/500\n",
      "256/256 [==============================] - 0s 508us/step - loss: 0.0753 - predict_loss: 0.0000e+00 - loss_loss: 0.0753\n",
      "Epoch 110/500\n",
      "256/256 [==============================] - 0s 506us/step - loss: 0.0749 - predict_loss: 0.0000e+00 - loss_loss: 0.0749\n",
      "Epoch 111/500\n",
      "256/256 [==============================] - 0s 469us/step - loss: 0.0745 - predict_loss: 0.0000e+00 - loss_loss: 0.0745\n",
      "Epoch 112/500\n",
      "256/256 [==============================] - 0s 527us/step - loss: 0.0754 - predict_loss: 0.0000e+00 - loss_loss: 0.0754\n",
      "Epoch 113/500\n",
      "256/256 [==============================] - 0s 440us/step - loss: 0.0758 - predict_loss: 0.0000e+00 - loss_loss: 0.0758\n",
      "Epoch 114/500\n",
      "256/256 [==============================] - 0s 498us/step - loss: 0.0731 - predict_loss: 0.0000e+00 - loss_loss: 0.0731\n",
      "Epoch 115/500\n",
      "256/256 [==============================] - 0s 480us/step - loss: 0.0751 - predict_loss: 0.0000e+00 - loss_loss: 0.0751\n",
      "Epoch 116/500\n",
      "256/256 [==============================] - 0s 483us/step - loss: 0.0743 - predict_loss: 0.0000e+00 - loss_loss: 0.0743\n",
      "Epoch 117/500\n",
      "256/256 [==============================] - 0s 477us/step - loss: 0.0753 - predict_loss: 0.0000e+00 - loss_loss: 0.0753\n",
      "Epoch 118/500\n",
      "256/256 [==============================] - 0s 494us/step - loss: 0.0737 - predict_loss: 0.0000e+00 - loss_loss: 0.0737\n",
      "Epoch 119/500\n",
      "256/256 [==============================] - 0s 449us/step - loss: 0.0740 - predict_loss: 0.0000e+00 - loss_loss: 0.0740\n",
      "Epoch 120/500\n",
      "256/256 [==============================] - 0s 483us/step - loss: 0.0755 - predict_loss: 0.0000e+00 - loss_loss: 0.0755\n",
      "Epoch 121/500\n",
      "256/256 [==============================] - 0s 442us/step - loss: 0.0739 - predict_loss: 0.0000e+00 - loss_loss: 0.0739\n",
      "Epoch 122/500\n",
      "256/256 [==============================] - 0s 491us/step - loss: 0.0746 - predict_loss: 0.0000e+00 - loss_loss: 0.0746\n",
      "Epoch 123/500\n",
      "256/256 [==============================] - 0s 462us/step - loss: 0.0738 - predict_loss: 0.0000e+00 - loss_loss: 0.0738\n",
      "Epoch 124/500\n",
      "256/256 [==============================] - 0s 412us/step - loss: 0.0740 - predict_loss: 0.0000e+00 - loss_loss: 0.0740\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 125/500\n",
      "256/256 [==============================] - 0s 397us/step - loss: 0.0741 - predict_loss: 0.0000e+00 - loss_loss: 0.0741\n",
      "Epoch 126/500\n",
      "256/256 [==============================] - 0s 408us/step - loss: 0.0758 - predict_loss: 0.0000e+00 - loss_loss: 0.0758\n",
      "Epoch 127/500\n",
      "256/256 [==============================] - 0s 355us/step - loss: 0.0730 - predict_loss: 0.0000e+00 - loss_loss: 0.0730\n",
      "Epoch 128/500\n",
      "256/256 [==============================] - 0s 412us/step - loss: 0.0740 - predict_loss: 0.0000e+00 - loss_loss: 0.0740\n",
      "Epoch 129/500\n",
      "256/256 [==============================] - 0s 416us/step - loss: 0.0738 - predict_loss: 0.0000e+00 - loss_loss: 0.0738\n",
      "Epoch 130/500\n",
      "256/256 [==============================] - 0s 407us/step - loss: 0.0751 - predict_loss: 0.0000e+00 - loss_loss: 0.0751\n",
      "Epoch 131/500\n",
      "256/256 [==============================] - 0s 456us/step - loss: 0.0736 - predict_loss: 0.0000e+00 - loss_loss: 0.0736\n",
      "Epoch 132/500\n",
      "256/256 [==============================] - 0s 437us/step - loss: 0.0747 - predict_loss: 0.0000e+00 - loss_loss: 0.0747\n",
      "Epoch 133/500\n",
      "256/256 [==============================] - 0s 414us/step - loss: 0.0723 - predict_loss: 0.0000e+00 - loss_loss: 0.0723\n",
      "Epoch 134/500\n",
      "256/256 [==============================] - 0s 404us/step - loss: 0.0753 - predict_loss: 0.0000e+00 - loss_loss: 0.0753\n",
      "Epoch 135/500\n",
      "256/256 [==============================] - 0s 413us/step - loss: 0.0734 - predict_loss: 0.0000e+00 - loss_loss: 0.0734\n",
      "Epoch 136/500\n",
      "256/256 [==============================] - 0s 426us/step - loss: 0.0734 - predict_loss: 0.0000e+00 - loss_loss: 0.0734\n",
      "Epoch 137/500\n",
      "256/256 [==============================] - 0s 409us/step - loss: 0.0730 - predict_loss: 0.0000e+00 - loss_loss: 0.0730\n",
      "Epoch 138/500\n",
      "256/256 [==============================] - 0s 389us/step - loss: 0.0745 - predict_loss: 0.0000e+00 - loss_loss: 0.0745\n",
      "Epoch 139/500\n",
      "256/256 [==============================] - 0s 439us/step - loss: 0.0734 - predict_loss: 0.0000e+00 - loss_loss: 0.0734\n",
      "Epoch 140/500\n",
      "256/256 [==============================] - 0s 446us/step - loss: 0.0742 - predict_loss: 0.0000e+00 - loss_loss: 0.0742\n",
      "Epoch 141/500\n",
      "256/256 [==============================] - 0s 451us/step - loss: 0.0737 - predict_loss: 0.0000e+00 - loss_loss: 0.0737\n",
      "Epoch 142/500\n",
      "256/256 [==============================] - 0s 436us/step - loss: 0.0727 - predict_loss: 0.0000e+00 - loss_loss: 0.0727\n",
      "Epoch 143/500\n",
      "256/256 [==============================] - 0s 449us/step - loss: 0.0731 - predict_loss: 0.0000e+00 - loss_loss: 0.0731\n",
      "Epoch 144/500\n",
      "256/256 [==============================] - 0s 395us/step - loss: 0.0733 - predict_loss: 0.0000e+00 - loss_loss: 0.0733\n",
      "Epoch 145/500\n",
      "256/256 [==============================] - 0s 430us/step - loss: 0.0730 - predict_loss: 0.0000e+00 - loss_loss: 0.0730\n",
      "Epoch 146/500\n",
      "256/256 [==============================] - 0s 419us/step - loss: 0.0730 - predict_loss: 0.0000e+00 - loss_loss: 0.0730\n",
      "Epoch 147/500\n",
      "256/256 [==============================] - 0s 441us/step - loss: 0.0736 - predict_loss: 0.0000e+00 - loss_loss: 0.0736\n",
      "Epoch 148/500\n",
      "256/256 [==============================] - 0s 439us/step - loss: 0.0732 - predict_loss: 0.0000e+00 - loss_loss: 0.0732\n",
      "Epoch 149/500\n",
      "256/256 [==============================] - 0s 394us/step - loss: 0.0744 - predict_loss: 0.0000e+00 - loss_loss: 0.0744\n",
      "Epoch 150/500\n",
      "256/256 [==============================] - 0s 427us/step - loss: 0.0724 - predict_loss: 0.0000e+00 - loss_loss: 0.0724\n",
      "Epoch 151/500\n",
      "256/256 [==============================] - 0s 464us/step - loss: 0.0740 - predict_loss: 0.0000e+00 - loss_loss: 0.0740\n",
      "Epoch 152/500\n",
      "256/256 [==============================] - 0s 464us/step - loss: 0.0736 - predict_loss: 0.0000e+00 - loss_loss: 0.0736\n",
      "Epoch 153/500\n",
      "256/256 [==============================] - 0s 433us/step - loss: 0.0724 - predict_loss: 0.0000e+00 - loss_loss: 0.0724\n",
      "Epoch 154/500\n",
      "256/256 [==============================] - 0s 426us/step - loss: 0.0727 - predict_loss: 0.0000e+00 - loss_loss: 0.0727\n",
      "Epoch 155/500\n",
      "256/256 [==============================] - 0s 422us/step - loss: 0.0724 - predict_loss: 0.0000e+00 - loss_loss: 0.0724\n",
      "Epoch 156/500\n",
      "256/256 [==============================] - 0s 439us/step - loss: 0.0731 - predict_loss: 0.0000e+00 - loss_loss: 0.0731\n",
      "Epoch 157/500\n",
      "256/256 [==============================] - 0s 451us/step - loss: 0.0735 - predict_loss: 0.0000e+00 - loss_loss: 0.0735\n",
      "Epoch 158/500\n",
      "256/256 [==============================] - 0s 443us/step - loss: 0.0739 - predict_loss: 0.0000e+00 - loss_loss: 0.0739\n",
      "Epoch 159/500\n",
      "256/256 [==============================] - 0s 419us/step - loss: 0.0721 - predict_loss: 0.0000e+00 - loss_loss: 0.0721\n",
      "Epoch 160/500\n",
      "256/256 [==============================] - 0s 459us/step - loss: 0.0718 - predict_loss: 0.0000e+00 - loss_loss: 0.0718\n",
      "Epoch 161/500\n",
      "256/256 [==============================] - 0s 426us/step - loss: 0.0727 - predict_loss: 0.0000e+00 - loss_loss: 0.0727\n",
      "Epoch 162/500\n",
      "256/256 [==============================] - 0s 436us/step - loss: 0.0753 - predict_loss: 0.0000e+00 - loss_loss: 0.0753\n",
      "Epoch 163/500\n",
      "256/256 [==============================] - 0s 425us/step - loss: 0.0723 - predict_loss: 0.0000e+00 - loss_loss: 0.0723\n",
      "Epoch 164/500\n",
      "256/256 [==============================] - 0s 452us/step - loss: 0.0723 - predict_loss: 0.0000e+00 - loss_loss: 0.0723\n",
      "Epoch 165/500\n",
      "256/256 [==============================] - 0s 463us/step - loss: 0.0711 - predict_loss: 0.0000e+00 - loss_loss: 0.0711\n",
      "Epoch 166/500\n",
      "256/256 [==============================] - 0s 394us/step - loss: 0.0736 - predict_loss: 0.0000e+00 - loss_loss: 0.0736\n",
      "Epoch 167/500\n",
      "256/256 [==============================] - 0s 423us/step - loss: 0.0719 - predict_loss: 0.0000e+00 - loss_loss: 0.0719\n",
      "Epoch 168/500\n",
      "256/256 [==============================] - 0s 400us/step - loss: 0.0725 - predict_loss: 0.0000e+00 - loss_loss: 0.0725\n",
      "Epoch 169/500\n",
      "256/256 [==============================] - 0s 427us/step - loss: 0.0717 - predict_loss: 0.0000e+00 - loss_loss: 0.0717\n",
      "Epoch 170/500\n",
      "256/256 [==============================] - 0s 439us/step - loss: 0.0733 - predict_loss: 0.0000e+00 - loss_loss: 0.0733\n",
      "Epoch 171/500\n",
      "256/256 [==============================] - 0s 442us/step - loss: 0.0725 - predict_loss: 0.0000e+00 - loss_loss: 0.0725\n",
      "Epoch 172/500\n",
      "256/256 [==============================] - 0s 448us/step - loss: 0.0719 - predict_loss: 0.0000e+00 - loss_loss: 0.0719\n",
      "Epoch 173/500\n",
      "256/256 [==============================] - 0s 416us/step - loss: 0.0721 - predict_loss: 0.0000e+00 - loss_loss: 0.0721\n",
      "Epoch 174/500\n",
      "256/256 [==============================] - 0s 428us/step - loss: 0.0719 - predict_loss: 0.0000e+00 - loss_loss: 0.0719\n",
      "Epoch 175/500\n",
      "256/256 [==============================] - 0s 431us/step - loss: 0.0727 - predict_loss: 0.0000e+00 - loss_loss: 0.0727\n",
      "Epoch 176/500\n",
      "256/256 [==============================] - 0s 413us/step - loss: 0.0727 - predict_loss: 0.0000e+00 - loss_loss: 0.0727\n",
      "Epoch 177/500\n",
      "256/256 [==============================] - 0s 471us/step - loss: 0.0716 - predict_loss: 0.0000e+00 - loss_loss: 0.0716\n",
      "Epoch 178/500\n",
      "256/256 [==============================] - 0s 423us/step - loss: 0.0720 - predict_loss: 0.0000e+00 - loss_loss: 0.0720\n",
      "Epoch 179/500\n",
      "256/256 [==============================] - 0s 445us/step - loss: 0.0742 - predict_loss: 0.0000e+00 - loss_loss: 0.0742\n",
      "Epoch 180/500\n",
      "256/256 [==============================] - 0s 430us/step - loss: 0.0718 - predict_loss: 0.0000e+00 - loss_loss: 0.0718\n",
      "Epoch 181/500\n",
      "256/256 [==============================] - 0s 421us/step - loss: 0.0707 - predict_loss: 0.0000e+00 - loss_loss: 0.0707\n",
      "Epoch 182/500\n",
      "256/256 [==============================] - 0s 482us/step - loss: 0.0717 - predict_loss: 0.0000e+00 - loss_loss: 0.0717\n",
      "Epoch 183/500\n",
      "256/256 [==============================] - 0s 433us/step - loss: 0.0728 - predict_loss: 0.0000e+00 - loss_loss: 0.0728\n",
      "Epoch 184/500\n",
      "256/256 [==============================] - 0s 452us/step - loss: 0.0718 - predict_loss: 0.0000e+00 - loss_loss: 0.0718\n",
      "Epoch 185/500\n",
      "256/256 [==============================] - 0s 420us/step - loss: 0.0728 - predict_loss: 0.0000e+00 - loss_loss: 0.0728\n",
      "Epoch 186/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256/256 [==============================] - 0s 462us/step - loss: 0.0715 - predict_loss: 0.0000e+00 - loss_loss: 0.0715\n",
      "Epoch 187/500\n",
      "256/256 [==============================] - 0s 398us/step - loss: 0.0713 - predict_loss: 0.0000e+00 - loss_loss: 0.0713\n",
      "Epoch 188/500\n",
      "256/256 [==============================] - 0s 405us/step - loss: 0.0727 - predict_loss: 0.0000e+00 - loss_loss: 0.0727\n",
      "Epoch 189/500\n",
      "256/256 [==============================] - 0s 415us/step - loss: 0.0709 - predict_loss: 0.0000e+00 - loss_loss: 0.0709\n",
      "Epoch 190/500\n",
      "256/256 [==============================] - 0s 416us/step - loss: 0.0724 - predict_loss: 0.0000e+00 - loss_loss: 0.0724\n",
      "Epoch 191/500\n",
      "256/256 [==============================] - 0s 394us/step - loss: 0.0720 - predict_loss: 0.0000e+00 - loss_loss: 0.0720\n",
      "Epoch 192/500\n",
      "256/256 [==============================] - 0s 440us/step - loss: 0.0718 - predict_loss: 0.0000e+00 - loss_loss: 0.0718\n",
      "Epoch 193/500\n",
      "256/256 [==============================] - 0s 423us/step - loss: 0.0715 - predict_loss: 0.0000e+00 - loss_loss: 0.0715\n",
      "Epoch 194/500\n",
      "256/256 [==============================] - 0s 389us/step - loss: 0.0717 - predict_loss: 0.0000e+00 - loss_loss: 0.0717\n",
      "Epoch 195/500\n",
      "256/256 [==============================] - 0s 389us/step - loss: 0.0704 - predict_loss: 0.0000e+00 - loss_loss: 0.0704\n",
      "Epoch 196/500\n",
      "256/256 [==============================] - 0s 386us/step - loss: 0.0719 - predict_loss: 0.0000e+00 - loss_loss: 0.0719\n",
      "Epoch 197/500\n",
      "256/256 [==============================] - 0s 418us/step - loss: 0.0708 - predict_loss: 0.0000e+00 - loss_loss: 0.0708\n",
      "Epoch 198/500\n",
      "256/256 [==============================] - 0s 428us/step - loss: 0.0721 - predict_loss: 0.0000e+00 - loss_loss: 0.0721\n",
      "Epoch 199/500\n",
      "256/256 [==============================] - 0s 436us/step - loss: 0.0711 - predict_loss: 0.0000e+00 - loss_loss: 0.0711\n",
      "Epoch 200/500\n",
      "256/256 [==============================] - 0s 405us/step - loss: 0.0727 - predict_loss: 0.0000e+00 - loss_loss: 0.0727\n",
      "Epoch 201/500\n",
      "256/256 [==============================] - 0s 418us/step - loss: 0.0707 - predict_loss: 0.0000e+00 - loss_loss: 0.0707\n",
      "Epoch 202/500\n",
      "256/256 [==============================] - 0s 467us/step - loss: 0.0728 - predict_loss: 0.0000e+00 - loss_loss: 0.0728\n",
      "Epoch 203/500\n",
      "256/256 [==============================] - 0s 423us/step - loss: 0.0722 - predict_loss: 0.0000e+00 - loss_loss: 0.0722\n",
      "Epoch 204/500\n",
      "256/256 [==============================] - 0s 403us/step - loss: 0.0708 - predict_loss: 0.0000e+00 - loss_loss: 0.0708\n",
      "Epoch 205/500\n",
      "256/256 [==============================] - 0s 416us/step - loss: 0.0708 - predict_loss: 0.0000e+00 - loss_loss: 0.0708\n",
      "Epoch 206/500\n",
      "256/256 [==============================] - 0s 431us/step - loss: 0.0716 - predict_loss: 0.0000e+00 - loss_loss: 0.0716\n",
      "Epoch 207/500\n",
      "256/256 [==============================] - 0s 426us/step - loss: 0.0711 - predict_loss: 0.0000e+00 - loss_loss: 0.0711\n",
      "Epoch 208/500\n",
      "256/256 [==============================] - 0s 460us/step - loss: 0.0710 - predict_loss: 0.0000e+00 - loss_loss: 0.0710\n",
      "Epoch 209/500\n",
      "256/256 [==============================] - 0s 407us/step - loss: 0.0715 - predict_loss: 0.0000e+00 - loss_loss: 0.0715\n",
      "Epoch 210/500\n",
      "256/256 [==============================] - 0s 400us/step - loss: 0.0705 - predict_loss: 0.0000e+00 - loss_loss: 0.0705\n",
      "Epoch 211/500\n",
      "256/256 [==============================] - 0s 403us/step - loss: 0.0713 - predict_loss: 0.0000e+00 - loss_loss: 0.0713\n",
      "Epoch 212/500\n",
      "256/256 [==============================] - 0s 462us/step - loss: 0.0710 - predict_loss: 0.0000e+00 - loss_loss: 0.0710\n",
      "Epoch 213/500\n",
      "256/256 [==============================] - 0s 391us/step - loss: 0.0701 - predict_loss: 0.0000e+00 - loss_loss: 0.0701\n",
      "Epoch 214/500\n",
      "256/256 [==============================] - 0s 427us/step - loss: 0.0716 - predict_loss: 0.0000e+00 - loss_loss: 0.0716\n",
      "Epoch 215/500\n",
      "256/256 [==============================] - 0s 407us/step - loss: 0.0716 - predict_loss: 0.0000e+00 - loss_loss: 0.0716\n",
      "Epoch 216/500\n",
      "256/256 [==============================] - 0s 408us/step - loss: 0.0714 - predict_loss: 0.0000e+00 - loss_loss: 0.0714\n",
      "Epoch 217/500\n",
      "256/256 [==============================] - 0s 431us/step - loss: 0.0698 - predict_loss: 0.0000e+00 - loss_loss: 0.0698\n",
      "Epoch 218/500\n",
      "256/256 [==============================] - 0s 461us/step - loss: 0.0701 - predict_loss: 0.0000e+00 - loss_loss: 0.0701\n",
      "Epoch 219/500\n",
      "256/256 [==============================] - 0s 440us/step - loss: 0.0739 - predict_loss: 0.0000e+00 - loss_loss: 0.0739\n",
      "Epoch 220/500\n",
      "256/256 [==============================] - 0s 408us/step - loss: 0.0702 - predict_loss: 0.0000e+00 - loss_loss: 0.0702\n",
      "Epoch 221/500\n",
      "256/256 [==============================] - 0s 441us/step - loss: 0.0700 - predict_loss: 0.0000e+00 - loss_loss: 0.0700\n",
      "Epoch 222/500\n",
      "256/256 [==============================] - 0s 396us/step - loss: 0.0713 - predict_loss: 0.0000e+00 - loss_loss: 0.0713\n",
      "Epoch 223/500\n",
      "256/256 [==============================] - 0s 465us/step - loss: 0.0703 - predict_loss: 0.0000e+00 - loss_loss: 0.0703\n",
      "Epoch 224/500\n",
      "256/256 [==============================] - 0s 410us/step - loss: 0.0700 - predict_loss: 0.0000e+00 - loss_loss: 0.0700\n",
      "Epoch 225/500\n",
      "256/256 [==============================] - 0s 391us/step - loss: 0.0701 - predict_loss: 0.0000e+00 - loss_loss: 0.0701\n",
      "Epoch 226/500\n",
      "256/256 [==============================] - 0s 394us/step - loss: 0.0702 - predict_loss: 0.0000e+00 - loss_loss: 0.0702\n",
      "Epoch 227/500\n",
      "256/256 [==============================] - 0s 406us/step - loss: 0.0703 - predict_loss: 0.0000e+00 - loss_loss: 0.0703\n",
      "Epoch 228/500\n",
      "256/256 [==============================] - 0s 478us/step - loss: 0.0703 - predict_loss: 0.0000e+00 - loss_loss: 0.0703\n",
      "Epoch 229/500\n",
      "256/256 [==============================] - 0s 349us/step - loss: 0.0712 - predict_loss: 0.0000e+00 - loss_loss: 0.0712\n",
      "Epoch 230/500\n",
      "256/256 [==============================] - 0s 370us/step - loss: 0.0707 - predict_loss: 0.0000e+00 - loss_loss: 0.0707\n",
      "Epoch 231/500\n",
      "256/256 [==============================] - 0s 401us/step - loss: 0.0699 - predict_loss: 0.0000e+00 - loss_loss: 0.0699\n",
      "Epoch 232/500\n",
      "256/256 [==============================] - 0s 432us/step - loss: 0.0695 - predict_loss: 0.0000e+00 - loss_loss: 0.0695\n",
      "Epoch 233/500\n",
      "256/256 [==============================] - 0s 396us/step - loss: 0.0702 - predict_loss: 0.0000e+00 - loss_loss: 0.0702\n",
      "Epoch 234/500\n",
      "256/256 [==============================] - 0s 446us/step - loss: 0.0691 - predict_loss: 0.0000e+00 - loss_loss: 0.0691\n",
      "Epoch 235/500\n",
      "256/256 [==============================] - 0s 430us/step - loss: 0.0699 - predict_loss: 0.0000e+00 - loss_loss: 0.0699\n",
      "Epoch 236/500\n",
      "256/256 [==============================] - 0s 435us/step - loss: 0.0711 - predict_loss: 0.0000e+00 - loss_loss: 0.0711\n",
      "Epoch 237/500\n",
      "256/256 [==============================] - 0s 459us/step - loss: 0.0700 - predict_loss: 0.0000e+00 - loss_loss: 0.0700\n",
      "Epoch 238/500\n",
      "256/256 [==============================] - 0s 451us/step - loss: 0.0715 - predict_loss: 0.0000e+00 - loss_loss: 0.0715\n",
      "Epoch 239/500\n",
      "256/256 [==============================] - 0s 429us/step - loss: 0.0709 - predict_loss: 0.0000e+00 - loss_loss: 0.0709\n",
      "Epoch 240/500\n",
      "256/256 [==============================] - 0s 459us/step - loss: 0.0701 - predict_loss: 0.0000e+00 - loss_loss: 0.0701\n",
      "Epoch 241/500\n",
      "256/256 [==============================] - 0s 458us/step - loss: 0.0699 - predict_loss: 0.0000e+00 - loss_loss: 0.0699\n",
      "Epoch 242/500\n",
      "256/256 [==============================] - 0s 432us/step - loss: 0.0689 - predict_loss: 0.0000e+00 - loss_loss: 0.0689\n",
      "Epoch 243/500\n",
      "256/256 [==============================] - 0s 425us/step - loss: 0.0690 - predict_loss: 0.0000e+00 - loss_loss: 0.0690\n",
      "Epoch 244/500\n",
      "256/256 [==============================] - 0s 414us/step - loss: 0.0691 - predict_loss: 0.0000e+00 - loss_loss: 0.0691\n",
      "Epoch 245/500\n",
      "256/256 [==============================] - 0s 430us/step - loss: 0.0704 - predict_loss: 0.0000e+00 - loss_loss: 0.0704\n",
      "Epoch 246/500\n",
      "256/256 [==============================] - 0s 444us/step - loss: 0.0709 - predict_loss: 0.0000e+00 - loss_loss: 0.0709\n",
      "Epoch 247/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256/256 [==============================] - 0s 424us/step - loss: 0.0711 - predict_loss: 0.0000e+00 - loss_loss: 0.0711\n",
      "Epoch 248/500\n",
      "256/256 [==============================] - 0s 425us/step - loss: 0.0688 - predict_loss: 0.0000e+00 - loss_loss: 0.0688\n",
      "Epoch 249/500\n",
      "256/256 [==============================] - 0s 404us/step - loss: 0.0703 - predict_loss: 0.0000e+00 - loss_loss: 0.0703\n",
      "Epoch 250/500\n",
      "256/256 [==============================] - 0s 443us/step - loss: 0.0699 - predict_loss: 0.0000e+00 - loss_loss: 0.0699\n",
      "Epoch 251/500\n",
      "256/256 [==============================] - 0s 432us/step - loss: 0.0693 - predict_loss: 0.0000e+00 - loss_loss: 0.0693\n",
      "Epoch 252/500\n",
      "256/256 [==============================] - 0s 343us/step - loss: 0.0703 - predict_loss: 0.0000e+00 - loss_loss: 0.0703\n",
      "Epoch 253/500\n",
      "256/256 [==============================] - 0s 388us/step - loss: 0.0707 - predict_loss: 0.0000e+00 - loss_loss: 0.0707\n",
      "Epoch 254/500\n",
      "256/256 [==============================] - 0s 387us/step - loss: 0.0703 - predict_loss: 0.0000e+00 - loss_loss: 0.0703\n",
      "Epoch 255/500\n",
      "256/256 [==============================] - 0s 389us/step - loss: 0.0698 - predict_loss: 0.0000e+00 - loss_loss: 0.0698\n",
      "Epoch 256/500\n",
      "256/256 [==============================] - 0s 470us/step - loss: 0.0691 - predict_loss: 0.0000e+00 - loss_loss: 0.0691\n",
      "Epoch 257/500\n",
      "256/256 [==============================] - 0s 431us/step - loss: 0.0700 - predict_loss: 0.0000e+00 - loss_loss: 0.0700\n",
      "Epoch 258/500\n",
      "256/256 [==============================] - 0s 424us/step - loss: 0.0703 - predict_loss: 0.0000e+00 - loss_loss: 0.0703\n",
      "Epoch 259/500\n",
      "256/256 [==============================] - 0s 374us/step - loss: 0.0699 - predict_loss: 0.0000e+00 - loss_loss: 0.0699\n",
      "Epoch 260/500\n",
      "256/256 [==============================] - 0s 432us/step - loss: 0.0696 - predict_loss: 0.0000e+00 - loss_loss: 0.0696\n",
      "Epoch 261/500\n",
      "256/256 [==============================] - 0s 419us/step - loss: 0.0692 - predict_loss: 0.0000e+00 - loss_loss: 0.0692\n",
      "Epoch 262/500\n",
      "256/256 [==============================] - 0s 443us/step - loss: 0.0701 - predict_loss: 0.0000e+00 - loss_loss: 0.0701\n",
      "Epoch 263/500\n",
      "256/256 [==============================] - 0s 411us/step - loss: 0.0706 - predict_loss: 0.0000e+00 - loss_loss: 0.0706\n",
      "Epoch 264/500\n",
      "256/256 [==============================] - 0s 462us/step - loss: 0.0696 - predict_loss: 0.0000e+00 - loss_loss: 0.0696\n",
      "Epoch 265/500\n",
      "256/256 [==============================] - 0s 425us/step - loss: 0.0690 - predict_loss: 0.0000e+00 - loss_loss: 0.0690\n",
      "Epoch 266/500\n",
      "256/256 [==============================] - 0s 416us/step - loss: 0.0687 - predict_loss: 0.0000e+00 - loss_loss: 0.0687\n",
      "Epoch 267/500\n",
      "256/256 [==============================] - 0s 390us/step - loss: 0.0700 - predict_loss: 0.0000e+00 - loss_loss: 0.0700\n",
      "Epoch 268/500\n",
      "256/256 [==============================] - 0s 463us/step - loss: 0.0701 - predict_loss: 0.0000e+00 - loss_loss: 0.0701\n",
      "Epoch 269/500\n",
      "256/256 [==============================] - 0s 453us/step - loss: 0.0684 - predict_loss: 0.0000e+00 - loss_loss: 0.0684\n",
      "Epoch 270/500\n",
      "256/256 [==============================] - 0s 425us/step - loss: 0.0685 - predict_loss: 0.0000e+00 - loss_loss: 0.0685\n",
      "Epoch 271/500\n",
      "256/256 [==============================] - 0s 390us/step - loss: 0.0690 - predict_loss: 0.0000e+00 - loss_loss: 0.0690\n",
      "Epoch 272/500\n",
      "256/256 [==============================] - 0s 405us/step - loss: 0.0686 - predict_loss: 0.0000e+00 - loss_loss: 0.0686\n",
      "Epoch 273/500\n",
      "256/256 [==============================] - 0s 417us/step - loss: 0.0714 - predict_loss: 0.0000e+00 - loss_loss: 0.0714\n",
      "Epoch 274/500\n",
      "256/256 [==============================] - 0s 498us/step - loss: 0.0687 - predict_loss: 0.0000e+00 - loss_loss: 0.0687\n",
      "Epoch 275/500\n",
      "256/256 [==============================] - 0s 396us/step - loss: 0.0688 - predict_loss: 0.0000e+00 - loss_loss: 0.0688\n",
      "Epoch 276/500\n",
      "256/256 [==============================] - 0s 401us/step - loss: 0.0684 - predict_loss: 0.0000e+00 - loss_loss: 0.0684\n",
      "Epoch 277/500\n",
      "256/256 [==============================] - 0s 421us/step - loss: 0.0686 - predict_loss: 0.0000e+00 - loss_loss: 0.0686\n",
      "Epoch 278/500\n",
      "256/256 [==============================] - 0s 416us/step - loss: 0.0691 - predict_loss: 0.0000e+00 - loss_loss: 0.0691\n",
      "Epoch 279/500\n",
      "256/256 [==============================] - 0s 407us/step - loss: 0.0681 - predict_loss: 0.0000e+00 - loss_loss: 0.0681\n",
      "Epoch 280/500\n",
      "256/256 [==============================] - 0s 401us/step - loss: 0.0694 - predict_loss: 0.0000e+00 - loss_loss: 0.0694\n",
      "Epoch 281/500\n",
      "256/256 [==============================] - 0s 426us/step - loss: 0.0709 - predict_loss: 0.0000e+00 - loss_loss: 0.0709\n",
      "Epoch 282/500\n",
      "256/256 [==============================] - 0s 450us/step - loss: 0.0682 - predict_loss: 0.0000e+00 - loss_loss: 0.0682\n",
      "Epoch 283/500\n",
      "256/256 [==============================] - 0s 407us/step - loss: 0.0679 - predict_loss: 0.0000e+00 - loss_loss: 0.0679\n",
      "Epoch 284/500\n",
      "256/256 [==============================] - 0s 435us/step - loss: 0.0709 - predict_loss: 0.0000e+00 - loss_loss: 0.0709\n",
      "Epoch 285/500\n",
      "256/256 [==============================] - 0s 451us/step - loss: 0.0691 - predict_loss: 0.0000e+00 - loss_loss: 0.0691\n",
      "Epoch 286/500\n",
      "256/256 [==============================] - 0s 390us/step - loss: 0.0689 - predict_loss: 0.0000e+00 - loss_loss: 0.0689\n",
      "Epoch 287/500\n",
      "256/256 [==============================] - 0s 417us/step - loss: 0.0688 - predict_loss: 0.0000e+00 - loss_loss: 0.0688\n",
      "Epoch 288/500\n",
      "256/256 [==============================] - 0s 399us/step - loss: 0.0689 - predict_loss: 0.0000e+00 - loss_loss: 0.0689\n",
      "Epoch 289/500\n",
      "256/256 [==============================] - 0s 386us/step - loss: 0.0686 - predict_loss: 0.0000e+00 - loss_loss: 0.0686\n",
      "Epoch 290/500\n",
      "256/256 [==============================] - 0s 431us/step - loss: 0.0693 - predict_loss: 0.0000e+00 - loss_loss: 0.0693\n",
      "Epoch 291/500\n",
      "256/256 [==============================] - 0s 491us/step - loss: 0.0695 - predict_loss: 0.0000e+00 - loss_loss: 0.0695\n",
      "Epoch 292/500\n",
      "256/256 [==============================] - 0s 460us/step - loss: 0.0690 - predict_loss: 0.0000e+00 - loss_loss: 0.0690\n",
      "Epoch 293/500\n",
      "256/256 [==============================] - 0s 442us/step - loss: 0.0686 - predict_loss: 0.0000e+00 - loss_loss: 0.0686\n",
      "Epoch 294/500\n",
      "256/256 [==============================] - 0s 420us/step - loss: 0.0686 - predict_loss: 0.0000e+00 - loss_loss: 0.0686\n",
      "Epoch 295/500\n",
      "256/256 [==============================] - 0s 430us/step - loss: 0.0688 - predict_loss: 0.0000e+00 - loss_loss: 0.0688\n",
      "Epoch 296/500\n",
      "256/256 [==============================] - 0s 431us/step - loss: 0.0685 - predict_loss: 0.0000e+00 - loss_loss: 0.0685\n",
      "Epoch 297/500\n",
      "256/256 [==============================] - 0s 427us/step - loss: 0.0680 - predict_loss: 0.0000e+00 - loss_loss: 0.0680\n",
      "Epoch 298/500\n",
      "256/256 [==============================] - 0s 449us/step - loss: 0.0679 - predict_loss: 0.0000e+00 - loss_loss: 0.0679\n",
      "Epoch 299/500\n",
      "256/256 [==============================] - 0s 416us/step - loss: 0.0672 - predict_loss: 0.0000e+00 - loss_loss: 0.0672\n",
      "Epoch 300/500\n",
      "256/256 [==============================] - 0s 430us/step - loss: 0.0690 - predict_loss: 0.0000e+00 - loss_loss: 0.0690\n",
      "Epoch 301/500\n",
      "256/256 [==============================] - 0s 449us/step - loss: 0.0702 - predict_loss: 0.0000e+00 - loss_loss: 0.0702\n",
      "Epoch 302/500\n",
      "256/256 [==============================] - 0s 459us/step - loss: 0.0684 - predict_loss: 0.0000e+00 - loss_loss: 0.0684\n",
      "Epoch 303/500\n",
      "256/256 [==============================] - 0s 422us/step - loss: 0.0691 - predict_loss: 0.0000e+00 - loss_loss: 0.0691\n",
      "Epoch 304/500\n",
      "256/256 [==============================] - 0s 403us/step - loss: 0.0676 - predict_loss: 0.0000e+00 - loss_loss: 0.0676\n",
      "Epoch 305/500\n",
      "256/256 [==============================] - 0s 443us/step - loss: 0.0691 - predict_loss: 0.0000e+00 - loss_loss: 0.0691\n",
      "Epoch 306/500\n",
      "256/256 [==============================] - 0s 439us/step - loss: 0.0679 - predict_loss: 0.0000e+00 - loss_loss: 0.0679\n",
      "Epoch 307/500\n",
      "256/256 [==============================] - 0s 431us/step - loss: 0.0692 - predict_loss: 0.0000e+00 - loss_loss: 0.0692\n",
      "Epoch 308/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256/256 [==============================] - 0s 440us/step - loss: 0.0670 - predict_loss: 0.0000e+00 - loss_loss: 0.0670\n",
      "Epoch 309/500\n",
      "256/256 [==============================] - 0s 427us/step - loss: 0.0683 - predict_loss: 0.0000e+00 - loss_loss: 0.0683\n",
      "Epoch 310/500\n",
      "256/256 [==============================] - 0s 465us/step - loss: 0.0675 - predict_loss: 0.0000e+00 - loss_loss: 0.0675\n",
      "Epoch 311/500\n",
      "256/256 [==============================] - 0s 387us/step - loss: 0.0693 - predict_loss: 0.0000e+00 - loss_loss: 0.0693\n",
      "Epoch 312/500\n",
      "256/256 [==============================] - 0s 438us/step - loss: 0.0669 - predict_loss: 0.0000e+00 - loss_loss: 0.0669\n",
      "Epoch 313/500\n",
      "256/256 [==============================] - 0s 442us/step - loss: 0.0692 - predict_loss: 0.0000e+00 - loss_loss: 0.0692\n",
      "Epoch 314/500\n",
      "256/256 [==============================] - 0s 406us/step - loss: 0.0686 - predict_loss: 0.0000e+00 - loss_loss: 0.0686\n",
      "Epoch 315/500\n",
      "256/256 [==============================] - 0s 408us/step - loss: 0.0674 - predict_loss: 0.0000e+00 - loss_loss: 0.0674\n",
      "Epoch 316/500\n",
      "256/256 [==============================] - 0s 390us/step - loss: 0.0687 - predict_loss: 0.0000e+00 - loss_loss: 0.0687\n",
      "Epoch 317/500\n",
      "256/256 [==============================] - 0s 415us/step - loss: 0.0680 - predict_loss: 0.0000e+00 - loss_loss: 0.0680\n",
      "Epoch 318/500\n",
      "256/256 [==============================] - 0s 402us/step - loss: 0.0671 - predict_loss: 0.0000e+00 - loss_loss: 0.0671\n",
      "Epoch 319/500\n",
      "256/256 [==============================] - 0s 398us/step - loss: 0.0681 - predict_loss: 0.0000e+00 - loss_loss: 0.0681\n",
      "Epoch 320/500\n",
      "256/256 [==============================] - 0s 418us/step - loss: 0.0691 - predict_loss: 0.0000e+00 - loss_loss: 0.0691\n",
      "Epoch 321/500\n",
      "256/256 [==============================] - 0s 414us/step - loss: 0.0680 - predict_loss: 0.0000e+00 - loss_loss: 0.0680\n",
      "Epoch 322/500\n",
      "256/256 [==============================] - 0s 414us/step - loss: 0.0692 - predict_loss: 0.0000e+00 - loss_loss: 0.0692\n",
      "Epoch 323/500\n",
      "256/256 [==============================] - 0s 469us/step - loss: 0.0672 - predict_loss: 0.0000e+00 - loss_loss: 0.0672\n",
      "Epoch 324/500\n",
      "256/256 [==============================] - 0s 400us/step - loss: 0.0685 - predict_loss: 0.0000e+00 - loss_loss: 0.0685\n",
      "Epoch 325/500\n",
      "256/256 [==============================] - 0s 420us/step - loss: 0.0672 - predict_loss: 0.0000e+00 - loss_loss: 0.0672\n",
      "Epoch 326/500\n",
      "256/256 [==============================] - 0s 419us/step - loss: 0.0678 - predict_loss: 0.0000e+00 - loss_loss: 0.0678\n",
      "Epoch 327/500\n",
      "256/256 [==============================] - 0s 440us/step - loss: 0.0673 - predict_loss: 0.0000e+00 - loss_loss: 0.0673\n",
      "Epoch 328/500\n",
      "256/256 [==============================] - 0s 469us/step - loss: 0.0680 - predict_loss: 0.0000e+00 - loss_loss: 0.0680\n",
      "Epoch 329/500\n",
      "256/256 [==============================] - 0s 432us/step - loss: 0.0682 - predict_loss: 0.0000e+00 - loss_loss: 0.0682\n",
      "Epoch 330/500\n",
      "256/256 [==============================] - 0s 410us/step - loss: 0.0679 - predict_loss: 0.0000e+00 - loss_loss: 0.0679\n",
      "Epoch 331/500\n",
      "256/256 [==============================] - 0s 393us/step - loss: 0.0673 - predict_loss: 0.0000e+00 - loss_loss: 0.0673\n",
      "Epoch 332/500\n",
      "256/256 [==============================] - 0s 415us/step - loss: 0.0679 - predict_loss: 0.0000e+00 - loss_loss: 0.0679\n",
      "Epoch 333/500\n",
      "256/256 [==============================] - 0s 436us/step - loss: 0.0695 - predict_loss: 0.0000e+00 - loss_loss: 0.0695\n",
      "Epoch 334/500\n",
      "256/256 [==============================] - 0s 455us/step - loss: 0.0680 - predict_loss: 0.0000e+00 - loss_loss: 0.0680\n",
      "Epoch 335/500\n",
      "256/256 [==============================] - 0s 440us/step - loss: 0.0698 - predict_loss: 0.0000e+00 - loss_loss: 0.0698\n",
      "Epoch 336/500\n",
      "256/256 [==============================] - 0s 426us/step - loss: 0.0671 - predict_loss: 0.0000e+00 - loss_loss: 0.0671\n",
      "Epoch 337/500\n",
      "256/256 [==============================] - 0s 445us/step - loss: 0.0666 - predict_loss: 0.0000e+00 - loss_loss: 0.0666\n",
      "Epoch 338/500\n",
      "256/256 [==============================] - 0s 370us/step - loss: 0.0680 - predict_loss: 0.0000e+00 - loss_loss: 0.0680\n",
      "Epoch 339/500\n",
      "256/256 [==============================] - 0s 435us/step - loss: 0.0673 - predict_loss: 0.0000e+00 - loss_loss: 0.0673\n",
      "Epoch 340/500\n",
      "256/256 [==============================] - 0s 425us/step - loss: 0.0681 - predict_loss: 0.0000e+00 - loss_loss: 0.0681\n",
      "Epoch 341/500\n",
      "256/256 [==============================] - 0s 395us/step - loss: 0.0676 - predict_loss: 0.0000e+00 - loss_loss: 0.0676\n",
      "Epoch 342/500\n",
      "256/256 [==============================] - 0s 451us/step - loss: 0.0691 - predict_loss: 0.0000e+00 - loss_loss: 0.0691\n",
      "Epoch 343/500\n",
      "256/256 [==============================] - 0s 425us/step - loss: 0.0665 - predict_loss: 0.0000e+00 - loss_loss: 0.0665\n",
      "Epoch 344/500\n",
      "256/256 [==============================] - 0s 450us/step - loss: 0.0671 - predict_loss: 0.0000e+00 - loss_loss: 0.0671\n",
      "Epoch 345/500\n",
      "256/256 [==============================] - 0s 416us/step - loss: 0.0669 - predict_loss: 0.0000e+00 - loss_loss: 0.0669\n",
      "Epoch 346/500\n",
      "256/256 [==============================] - 0s 437us/step - loss: 0.0678 - predict_loss: 0.0000e+00 - loss_loss: 0.0678\n",
      "Epoch 347/500\n",
      "256/256 [==============================] - 0s 425us/step - loss: 0.0669 - predict_loss: 0.0000e+00 - loss_loss: 0.0669\n",
      "Epoch 348/500\n",
      "256/256 [==============================] - 0s 452us/step - loss: 0.0666 - predict_loss: 0.0000e+00 - loss_loss: 0.0666\n",
      "Epoch 349/500\n",
      "256/256 [==============================] - 0s 402us/step - loss: 0.0667 - predict_loss: 0.0000e+00 - loss_loss: 0.0667\n",
      "Epoch 350/500\n",
      "256/256 [==============================] - 0s 430us/step - loss: 0.0672 - predict_loss: 0.0000e+00 - loss_loss: 0.0672\n",
      "Epoch 351/500\n",
      "256/256 [==============================] - 0s 430us/step - loss: 0.0673 - predict_loss: 0.0000e+00 - loss_loss: 0.0673\n",
      "Epoch 352/500\n",
      "256/256 [==============================] - 0s 459us/step - loss: 0.0684 - predict_loss: 0.0000e+00 - loss_loss: 0.0684\n",
      "Epoch 353/500\n",
      "256/256 [==============================] - 0s 466us/step - loss: 0.0683 - predict_loss: 0.0000e+00 - loss_loss: 0.0683\n",
      "Epoch 354/500\n",
      "256/256 [==============================] - 0s 474us/step - loss: 0.0666 - predict_loss: 0.0000e+00 - loss_loss: 0.0666\n",
      "Epoch 355/500\n",
      "256/256 [==============================] - 0s 445us/step - loss: 0.0666 - predict_loss: 0.0000e+00 - loss_loss: 0.0666\n",
      "Epoch 356/500\n",
      "256/256 [==============================] - 0s 441us/step - loss: 0.0678 - predict_loss: 0.0000e+00 - loss_loss: 0.0678\n",
      "Epoch 357/500\n",
      "256/256 [==============================] - 0s 419us/step - loss: 0.0676 - predict_loss: 0.0000e+00 - loss_loss: 0.0676\n",
      "Epoch 358/500\n",
      "256/256 [==============================] - 0s 417us/step - loss: 0.0665 - predict_loss: 0.0000e+00 - loss_loss: 0.0665\n",
      "Epoch 359/500\n",
      "256/256 [==============================] - 0s 449us/step - loss: 0.0684 - predict_loss: 0.0000e+00 - loss_loss: 0.0684\n",
      "Epoch 360/500\n",
      "256/256 [==============================] - 0s 469us/step - loss: 0.0673 - predict_loss: 0.0000e+00 - loss_loss: 0.0673\n",
      "Epoch 361/500\n",
      "256/256 [==============================] - 0s 440us/step - loss: 0.0677 - predict_loss: 0.0000e+00 - loss_loss: 0.0677\n",
      "Epoch 362/500\n",
      "256/256 [==============================] - 0s 443us/step - loss: 0.0661 - predict_loss: 0.0000e+00 - loss_loss: 0.0661\n",
      "Epoch 363/500\n",
      "256/256 [==============================] - 0s 537us/step - loss: 0.0678 - predict_loss: 0.0000e+00 - loss_loss: 0.0678\n",
      "Epoch 364/500\n",
      "256/256 [==============================] - 0s 419us/step - loss: 0.0683 - predict_loss: 0.0000e+00 - loss_loss: 0.0683\n",
      "Epoch 365/500\n",
      "256/256 [==============================] - 0s 405us/step - loss: 0.0665 - predict_loss: 0.0000e+00 - loss_loss: 0.0665\n",
      "Epoch 366/500\n",
      "256/256 [==============================] - 0s 423us/step - loss: 0.0661 - predict_loss: 0.0000e+00 - loss_loss: 0.0661\n",
      "Epoch 367/500\n",
      "256/256 [==============================] - 0s 407us/step - loss: 0.0672 - predict_loss: 0.0000e+00 - loss_loss: 0.0672\n",
      "Epoch 368/500\n",
      "256/256 [==============================] - 0s 361us/step - loss: 0.0677 - predict_loss: 0.0000e+00 - loss_loss: 0.0677\n",
      "Epoch 369/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256/256 [==============================] - 0s 449us/step - loss: 0.0667 - predict_loss: 0.0000e+00 - loss_loss: 0.0667\n",
      "Epoch 370/500\n",
      "256/256 [==============================] - 0s 397us/step - loss: 0.0656 - predict_loss: 0.0000e+00 - loss_loss: 0.0656\n",
      "Epoch 371/500\n",
      "256/256 [==============================] - 0s 306us/step - loss: 0.0668 - predict_loss: 0.0000e+00 - loss_loss: 0.0668\n",
      "Epoch 372/500\n",
      "256/256 [==============================] - 0s 350us/step - loss: 0.0667 - predict_loss: 0.0000e+00 - loss_loss: 0.0667\n",
      "Epoch 373/500\n",
      "256/256 [==============================] - 0s 325us/step - loss: 0.0667 - predict_loss: 0.0000e+00 - loss_loss: 0.0667\n",
      "Epoch 374/500\n",
      "256/256 [==============================] - 0s 329us/step - loss: 0.0668 - predict_loss: 0.0000e+00 - loss_loss: 0.0668\n",
      "Epoch 375/500\n",
      "256/256 [==============================] - 0s 344us/step - loss: 0.0665 - predict_loss: 0.0000e+00 - loss_loss: 0.0665\n",
      "Epoch 376/500\n",
      "256/256 [==============================] - 0s 343us/step - loss: 0.0670 - predict_loss: 0.0000e+00 - loss_loss: 0.0670\n",
      "Epoch 377/500\n",
      "256/256 [==============================] - 0s 354us/step - loss: 0.0665 - predict_loss: 0.0000e+00 - loss_loss: 0.0665\n",
      "Epoch 378/500\n",
      "256/256 [==============================] - 0s 386us/step - loss: 0.0658 - predict_loss: 0.0000e+00 - loss_loss: 0.0658\n",
      "Epoch 379/500\n",
      "256/256 [==============================] - 0s 318us/step - loss: 0.0661 - predict_loss: 0.0000e+00 - loss_loss: 0.0661\n",
      "Epoch 380/500\n",
      "256/256 [==============================] - 0s 377us/step - loss: 0.0688 - predict_loss: 0.0000e+00 - loss_loss: 0.0688\n",
      "Epoch 381/500\n",
      "256/256 [==============================] - 0s 272us/step - loss: 0.0668 - predict_loss: 0.0000e+00 - loss_loss: 0.0668\n",
      "Epoch 382/500\n",
      "256/256 [==============================] - 0s 257us/step - loss: 0.0656 - predict_loss: 0.0000e+00 - loss_loss: 0.0656\n",
      "Epoch 383/500\n",
      "256/256 [==============================] - 0s 369us/step - loss: 0.0681 - predict_loss: 0.0000e+00 - loss_loss: 0.0681\n",
      "Epoch 384/500\n",
      "256/256 [==============================] - 0s 326us/step - loss: 0.0671 - predict_loss: 0.0000e+00 - loss_loss: 0.0671\n",
      "Epoch 385/500\n",
      "256/256 [==============================] - 0s 359us/step - loss: 0.0651 - predict_loss: 0.0000e+00 - loss_loss: 0.0651\n",
      "Epoch 386/500\n",
      "256/256 [==============================] - 0s 401us/step - loss: 0.0681 - predict_loss: 0.0000e+00 - loss_loss: 0.0681\n",
      "Epoch 387/500\n",
      "256/256 [==============================] - 0s 358us/step - loss: 0.0671 - predict_loss: 0.0000e+00 - loss_loss: 0.0671\n",
      "Epoch 388/500\n",
      "256/256 [==============================] - 0s 321us/step - loss: 0.0656 - predict_loss: 0.0000e+00 - loss_loss: 0.0656\n",
      "Epoch 389/500\n",
      "256/256 [==============================] - 0s 330us/step - loss: 0.0662 - predict_loss: 0.0000e+00 - loss_loss: 0.0662\n",
      "Epoch 390/500\n",
      "256/256 [==============================] - 0s 331us/step - loss: 0.0661 - predict_loss: 0.0000e+00 - loss_loss: 0.0661\n",
      "Epoch 391/500\n",
      "256/256 [==============================] - 0s 316us/step - loss: 0.0666 - predict_loss: 0.0000e+00 - loss_loss: 0.0666\n",
      "Epoch 392/500\n",
      "256/256 [==============================] - 0s 347us/step - loss: 0.0679 - predict_loss: 0.0000e+00 - loss_loss: 0.0679\n",
      "Epoch 393/500\n",
      "256/256 [==============================] - 0s 346us/step - loss: 0.0654 - predict_loss: 0.0000e+00 - loss_loss: 0.0654\n",
      "Epoch 394/500\n",
      "256/256 [==============================] - 0s 349us/step - loss: 0.0680 - predict_loss: 0.0000e+00 - loss_loss: 0.0680\n",
      "Epoch 395/500\n",
      "256/256 [==============================] - 0s 311us/step - loss: 0.0654 - predict_loss: 0.0000e+00 - loss_loss: 0.0654\n",
      "Epoch 396/500\n",
      "256/256 [==============================] - 0s 286us/step - loss: 0.0651 - predict_loss: 0.0000e+00 - loss_loss: 0.0651\n",
      "Epoch 397/500\n",
      "256/256 [==============================] - 0s 257us/step - loss: 0.0673 - predict_loss: 0.0000e+00 - loss_loss: 0.0673\n",
      "Epoch 398/500\n",
      "256/256 [==============================] - 0s 252us/step - loss: 0.0661 - predict_loss: 0.0000e+00 - loss_loss: 0.0661\n",
      "Epoch 399/500\n",
      "256/256 [==============================] - 0s 255us/step - loss: 0.0672 - predict_loss: 0.0000e+00 - loss_loss: 0.0672\n",
      "Epoch 400/500\n",
      "256/256 [==============================] - 0s 258us/step - loss: 0.0672 - predict_loss: 0.0000e+00 - loss_loss: 0.0672\n",
      "Epoch 401/500\n",
      "256/256 [==============================] - 0s 329us/step - loss: 0.0655 - predict_loss: 0.0000e+00 - loss_loss: 0.0655\n",
      "Epoch 402/500\n",
      "256/256 [==============================] - 0s 357us/step - loss: 0.0677 - predict_loss: 0.0000e+00 - loss_loss: 0.0677\n",
      "Epoch 403/500\n",
      "256/256 [==============================] - 0s 351us/step - loss: 0.0651 - predict_loss: 0.0000e+00 - loss_loss: 0.0651\n",
      "Epoch 404/500\n",
      "256/256 [==============================] - 0s 281us/step - loss: 0.0659 - predict_loss: 0.0000e+00 - loss_loss: 0.0659\n",
      "Epoch 405/500\n",
      "256/256 [==============================] - 0s 297us/step - loss: 0.0666 - predict_loss: 0.0000e+00 - loss_loss: 0.0666\n",
      "Epoch 406/500\n",
      "256/256 [==============================] - 0s 252us/step - loss: 0.0673 - predict_loss: 0.0000e+00 - loss_loss: 0.0673\n",
      "Epoch 407/500\n",
      "256/256 [==============================] - 0s 284us/step - loss: 0.0653 - predict_loss: 0.0000e+00 - loss_loss: 0.0653\n",
      "Epoch 408/500\n",
      "256/256 [==============================] - 0s 285us/step - loss: 0.0653 - predict_loss: 0.0000e+00 - loss_loss: 0.0653\n",
      "Epoch 409/500\n",
      "256/256 [==============================] - 0s 276us/step - loss: 0.0659 - predict_loss: 0.0000e+00 - loss_loss: 0.0659\n",
      "Epoch 410/500\n",
      "256/256 [==============================] - 0s 270us/step - loss: 0.0667 - predict_loss: 0.0000e+00 - loss_loss: 0.0667\n",
      "Epoch 411/500\n",
      "256/256 [==============================] - 0s 290us/step - loss: 0.0647 - predict_loss: 0.0000e+00 - loss_loss: 0.0647\n",
      "Epoch 412/500\n",
      "256/256 [==============================] - 0s 288us/step - loss: 0.0671 - predict_loss: 0.0000e+00 - loss_loss: 0.0671\n",
      "Epoch 413/500\n",
      "256/256 [==============================] - 0s 263us/step - loss: 0.0655 - predict_loss: 0.0000e+00 - loss_loss: 0.0655\n",
      "Epoch 414/500\n",
      "256/256 [==============================] - 0s 276us/step - loss: 0.0667 - predict_loss: 0.0000e+00 - loss_loss: 0.0667\n",
      "Epoch 415/500\n",
      "256/256 [==============================] - 0s 291us/step - loss: 0.0652 - predict_loss: 0.0000e+00 - loss_loss: 0.0652\n",
      "Epoch 416/500\n",
      "256/256 [==============================] - 0s 279us/step - loss: 0.0662 - predict_loss: 0.0000e+00 - loss_loss: 0.0662\n",
      "Epoch 417/500\n",
      "256/256 [==============================] - 0s 264us/step - loss: 0.0656 - predict_loss: 0.0000e+00 - loss_loss: 0.0656\n",
      "Epoch 418/500\n",
      "256/256 [==============================] - 0s 259us/step - loss: 0.0657 - predict_loss: 0.0000e+00 - loss_loss: 0.0657\n",
      "Epoch 419/500\n",
      "256/256 [==============================] - 0s 272us/step - loss: 0.0672 - predict_loss: 0.0000e+00 - loss_loss: 0.0672\n",
      "Epoch 420/500\n",
      "256/256 [==============================] - 0s 255us/step - loss: 0.0645 - predict_loss: 0.0000e+00 - loss_loss: 0.0645\n",
      "Epoch 421/500\n",
      "256/256 [==============================] - 0s 261us/step - loss: 0.0655 - predict_loss: 0.0000e+00 - loss_loss: 0.0655\n",
      "Epoch 422/500\n",
      "256/256 [==============================] - 0s 270us/step - loss: 0.0672 - predict_loss: 0.0000e+00 - loss_loss: 0.0672\n",
      "Epoch 423/500\n",
      "256/256 [==============================] - 0s 271us/step - loss: 0.0649 - predict_loss: 0.0000e+00 - loss_loss: 0.0649\n",
      "Epoch 424/500\n",
      "256/256 [==============================] - 0s 273us/step - loss: 0.0661 - predict_loss: 0.0000e+00 - loss_loss: 0.0661\n",
      "Epoch 425/500\n",
      "256/256 [==============================] - 0s 257us/step - loss: 0.0667 - predict_loss: 0.0000e+00 - loss_loss: 0.0667\n",
      "Epoch 426/500\n",
      "256/256 [==============================] - 0s 256us/step - loss: 0.0651 - predict_loss: 0.0000e+00 - loss_loss: 0.0651\n",
      "Epoch 427/500\n",
      "256/256 [==============================] - 0s 258us/step - loss: 0.0662 - predict_loss: 0.0000e+00 - loss_loss: 0.0662\n",
      "Epoch 428/500\n",
      "256/256 [==============================] - 0s 273us/step - loss: 0.0655 - predict_loss: 0.0000e+00 - loss_loss: 0.0655\n",
      "Epoch 429/500\n",
      "256/256 [==============================] - 0s 285us/step - loss: 0.0664 - predict_loss: 0.0000e+00 - loss_loss: 0.0664\n",
      "Epoch 430/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256/256 [==============================] - 0s 308us/step - loss: 0.0653 - predict_loss: 0.0000e+00 - loss_loss: 0.0653\n",
      "Epoch 431/500\n",
      "256/256 [==============================] - 0s 260us/step - loss: 0.0653 - predict_loss: 0.0000e+00 - loss_loss: 0.0653\n",
      "Epoch 432/500\n",
      "256/256 [==============================] - 0s 263us/step - loss: 0.0659 - predict_loss: 0.0000e+00 - loss_loss: 0.0659\n",
      "Epoch 433/500\n",
      "256/256 [==============================] - 0s 263us/step - loss: 0.0652 - predict_loss: 0.0000e+00 - loss_loss: 0.0652\n",
      "Epoch 434/500\n",
      "256/256 [==============================] - 0s 267us/step - loss: 0.0666 - predict_loss: 0.0000e+00 - loss_loss: 0.0666\n",
      "Epoch 435/500\n",
      "256/256 [==============================] - 0s 284us/step - loss: 0.0650 - predict_loss: 0.0000e+00 - loss_loss: 0.0650\n",
      "Epoch 436/500\n",
      "256/256 [==============================] - 0s 259us/step - loss: 0.0656 - predict_loss: 0.0000e+00 - loss_loss: 0.0656\n",
      "Epoch 437/500\n",
      "256/256 [==============================] - 0s 278us/step - loss: 0.0675 - predict_loss: 0.0000e+00 - loss_loss: 0.0675\n",
      "Epoch 438/500\n",
      "256/256 [==============================] - 0s 286us/step - loss: 0.0649 - predict_loss: 0.0000e+00 - loss_loss: 0.0649\n",
      "Epoch 439/500\n",
      "256/256 [==============================] - 0s 248us/step - loss: 0.0648 - predict_loss: 0.0000e+00 - loss_loss: 0.0648\n",
      "Epoch 440/500\n",
      "256/256 [==============================] - 0s 294us/step - loss: 0.0651 - predict_loss: 0.0000e+00 - loss_loss: 0.0651\n",
      "Epoch 441/500\n",
      "256/256 [==============================] - 0s 253us/step - loss: 0.0646 - predict_loss: 0.0000e+00 - loss_loss: 0.0646\n",
      "Epoch 442/500\n",
      "256/256 [==============================] - 0s 283us/step - loss: 0.0641 - predict_loss: 0.0000e+00 - loss_loss: 0.0641\n",
      "Epoch 443/500\n",
      "256/256 [==============================] - 0s 256us/step - loss: 0.0661 - predict_loss: 0.0000e+00 - loss_loss: 0.0661\n",
      "Epoch 444/500\n",
      "256/256 [==============================] - 0s 256us/step - loss: 0.0644 - predict_loss: 0.0000e+00 - loss_loss: 0.0644\n",
      "Epoch 445/500\n",
      "256/256 [==============================] - 0s 291us/step - loss: 0.0645 - predict_loss: 0.0000e+00 - loss_loss: 0.0645\n",
      "Epoch 446/500\n",
      "256/256 [==============================] - 0s 306us/step - loss: 0.0653 - predict_loss: 0.0000e+00 - loss_loss: 0.0653\n",
      "Epoch 447/500\n",
      "256/256 [==============================] - 0s 296us/step - loss: 0.0651 - predict_loss: 0.0000e+00 - loss_loss: 0.0651\n",
      "Epoch 448/500\n",
      "256/256 [==============================] - 0s 291us/step - loss: 0.0638 - predict_loss: 0.0000e+00 - loss_loss: 0.0638\n",
      "Epoch 449/500\n",
      "256/256 [==============================] - 0s 313us/step - loss: 0.0641 - predict_loss: 0.0000e+00 - loss_loss: 0.0641\n",
      "Epoch 450/500\n",
      "256/256 [==============================] - 0s 768us/step - loss: 0.0648 - predict_loss: 0.0000e+00 - loss_loss: 0.0648\n",
      "Epoch 451/500\n",
      "256/256 [==============================] - 0s 636us/step - loss: 0.0655 - predict_loss: 0.0000e+00 - loss_loss: 0.0655\n",
      "Epoch 452/500\n",
      "256/256 [==============================] - 0s 433us/step - loss: 0.0659 - predict_loss: 0.0000e+00 - loss_loss: 0.0659\n",
      "Epoch 453/500\n",
      "256/256 [==============================] - 0s 355us/step - loss: 0.0649 - predict_loss: 0.0000e+00 - loss_loss: 0.0649\n",
      "Epoch 454/500\n",
      "256/256 [==============================] - 0s 350us/step - loss: 0.0649 - predict_loss: 0.0000e+00 - loss_loss: 0.0649\n",
      "Epoch 455/500\n",
      "256/256 [==============================] - 0s 361us/step - loss: 0.0651 - predict_loss: 0.0000e+00 - loss_loss: 0.0651\n",
      "Epoch 456/500\n",
      "256/256 [==============================] - 0s 351us/step - loss: 0.0649 - predict_loss: 0.0000e+00 - loss_loss: 0.0649\n",
      "Epoch 457/500\n",
      "256/256 [==============================] - 0s 381us/step - loss: 0.0674 - predict_loss: 0.0000e+00 - loss_loss: 0.0674\n",
      "Epoch 458/500\n",
      "256/256 [==============================] - 0s 515us/step - loss: 0.0649 - predict_loss: 0.0000e+00 - loss_loss: 0.0649\n",
      "Epoch 459/500\n",
      "256/256 [==============================] - 0s 451us/step - loss: 0.0640 - predict_loss: 0.0000e+00 - loss_loss: 0.0640\n",
      "Epoch 460/500\n",
      "256/256 [==============================] - 0s 400us/step - loss: 0.0647 - predict_loss: 0.0000e+00 - loss_loss: 0.0647\n",
      "Epoch 461/500\n",
      "256/256 [==============================] - 0s 501us/step - loss: 0.0665 - predict_loss: 0.0000e+00 - loss_loss: 0.0665\n",
      "Epoch 462/500\n",
      "256/256 [==============================] - 0s 488us/step - loss: 0.0645 - predict_loss: 0.0000e+00 - loss_loss: 0.0645\n",
      "Epoch 463/500\n",
      "256/256 [==============================] - 0s 396us/step - loss: 0.0642 - predict_loss: 0.0000e+00 - loss_loss: 0.0642\n",
      "Epoch 464/500\n",
      "256/256 [==============================] - 0s 374us/step - loss: 0.0643 - predict_loss: 0.0000e+00 - loss_loss: 0.0643\n",
      "Epoch 465/500\n",
      "256/256 [==============================] - 0s 359us/step - loss: 0.0649 - predict_loss: 0.0000e+00 - loss_loss: 0.0649\n",
      "Epoch 466/500\n",
      "256/256 [==============================] - 0s 374us/step - loss: 0.0644 - predict_loss: 0.0000e+00 - loss_loss: 0.0644\n",
      "Epoch 467/500\n",
      "256/256 [==============================] - 0s 393us/step - loss: 0.0640 - predict_loss: 0.0000e+00 - loss_loss: 0.0640\n",
      "Epoch 468/500\n",
      "256/256 [==============================] - 0s 376us/step - loss: 0.0640 - predict_loss: 0.0000e+00 - loss_loss: 0.0640\n",
      "Epoch 469/500\n",
      "256/256 [==============================] - 0s 356us/step - loss: 0.0661 - predict_loss: 0.0000e+00 - loss_loss: 0.0661\n",
      "Epoch 470/500\n",
      "256/256 [==============================] - 0s 313us/step - loss: 0.0646 - predict_loss: 0.0000e+00 - loss_loss: 0.0646\n",
      "Epoch 471/500\n",
      "256/256 [==============================] - 0s 410us/step - loss: 0.0645 - predict_loss: 0.0000e+00 - loss_loss: 0.0645\n",
      "Epoch 472/500\n",
      "256/256 [==============================] - 0s 356us/step - loss: 0.0638 - predict_loss: 0.0000e+00 - loss_loss: 0.0638\n",
      "Epoch 473/500\n",
      "256/256 [==============================] - 0s 347us/step - loss: 0.0664 - predict_loss: 0.0000e+00 - loss_loss: 0.0664\n",
      "Epoch 474/500\n",
      "256/256 [==============================] - 0s 269us/step - loss: 0.0654 - predict_loss: 0.0000e+00 - loss_loss: 0.0654\n",
      "Epoch 475/500\n",
      "256/256 [==============================] - 0s 255us/step - loss: 0.0638 - predict_loss: 0.0000e+00 - loss_loss: 0.0638\n",
      "Epoch 476/500\n",
      "256/256 [==============================] - 0s 288us/step - loss: 0.0656 - predict_loss: 0.0000e+00 - loss_loss: 0.0656\n",
      "Epoch 477/500\n",
      "256/256 [==============================] - 0s 305us/step - loss: 0.0644 - predict_loss: 0.0000e+00 - loss_loss: 0.0644\n",
      "Epoch 478/500\n",
      "256/256 [==============================] - 0s 270us/step - loss: 0.0637 - predict_loss: 0.0000e+00 - loss_loss: 0.0637\n",
      "Epoch 479/500\n",
      "256/256 [==============================] - 0s 284us/step - loss: 0.0637 - predict_loss: 0.0000e+00 - loss_loss: 0.0637\n",
      "Epoch 480/500\n",
      "256/256 [==============================] - 0s 289us/step - loss: 0.0646 - predict_loss: 0.0000e+00 - loss_loss: 0.0646\n",
      "Epoch 481/500\n",
      "256/256 [==============================] - 0s 329us/step - loss: 0.0646 - predict_loss: 0.0000e+00 - loss_loss: 0.0646\n",
      "Epoch 482/500\n",
      "256/256 [==============================] - 0s 331us/step - loss: 0.0643 - predict_loss: 0.0000e+00 - loss_loss: 0.0643\n",
      "Epoch 483/500\n",
      "256/256 [==============================] - 0s 328us/step - loss: 0.0638 - predict_loss: 0.0000e+00 - loss_loss: 0.0638\n",
      "Epoch 484/500\n",
      "256/256 [==============================] - 0s 328us/step - loss: 0.0641 - predict_loss: 0.0000e+00 - loss_loss: 0.0641\n",
      "Epoch 485/500\n",
      "256/256 [==============================] - 0s 330us/step - loss: 0.0645 - predict_loss: 0.0000e+00 - loss_loss: 0.0645\n",
      "Epoch 486/500\n",
      "256/256 [==============================] - 0s 354us/step - loss: 0.0637 - predict_loss: 0.0000e+00 - loss_loss: 0.0637\n",
      "Epoch 487/500\n",
      "256/256 [==============================] - 0s 374us/step - loss: 0.0642 - predict_loss: 0.0000e+00 - loss_loss: 0.0642\n",
      "Epoch 488/500\n",
      "256/256 [==============================] - 0s 406us/step - loss: 0.0655 - predict_loss: 0.0000e+00 - loss_loss: 0.0655\n",
      "Epoch 489/500\n",
      "256/256 [==============================] - 0s 420us/step - loss: 0.0634 - predict_loss: 0.0000e+00 - loss_loss: 0.0634\n",
      "Epoch 490/500\n",
      "256/256 [==============================] - 0s 360us/step - loss: 0.0634 - predict_loss: 0.0000e+00 - loss_loss: 0.0634\n",
      "Epoch 491/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256/256 [==============================] - 0s 276us/step - loss: 0.0651 - predict_loss: 0.0000e+00 - loss_loss: 0.0651\n",
      "Epoch 492/500\n",
      "256/256 [==============================] - 0s 301us/step - loss: 0.0636 - predict_loss: 0.0000e+00 - loss_loss: 0.0636\n",
      "Epoch 493/500\n",
      "256/256 [==============================] - 0s 309us/step - loss: 0.0625 - predict_loss: 0.0000e+00 - loss_loss: 0.0625\n",
      "Epoch 494/500\n",
      "256/256 [==============================] - 0s 349us/step - loss: 0.0656 - predict_loss: 0.0000e+00 - loss_loss: 0.0656\n",
      "Epoch 495/500\n",
      "256/256 [==============================] - 0s 443us/step - loss: 0.0640 - predict_loss: 0.0000e+00 - loss_loss: 0.0640\n",
      "Epoch 496/500\n",
      "256/256 [==============================] - 0s 545us/step - loss: 0.0645 - predict_loss: 0.0000e+00 - loss_loss: 0.0645\n",
      "Epoch 497/500\n",
      "256/256 [==============================] - 0s 512us/step - loss: 0.0632 - predict_loss: 0.0000e+00 - loss_loss: 0.0632\n",
      "Epoch 498/500\n",
      "256/256 [==============================] - 0s 544us/step - loss: 0.0635 - predict_loss: 0.0000e+00 - loss_loss: 0.0635\n",
      "Epoch 499/500\n",
      "256/256 [==============================] - 0s 597us/step - loss: 0.0633 - predict_loss: 0.0000e+00 - loss_loss: 0.0633\n",
      "Epoch 500/500\n",
      "256/256 [==============================] - 0s 478us/step - loss: 0.0658 - predict_loss: 0.0000e+00 - loss_loss: 0.0658\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fdbe1a97cc0>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# finetuning\n",
    "model.compile(optimizer=optimizers.RMSprop(lr = 1e-5, clipvalue = 10.),\n",
    "              loss= {\n",
    "                    'predict': lambda _, loss: loss - loss,\n",
    "                      'loss': lambda _, loss: loss\n",
    "                })\n",
    "# and train\n",
    "model.fit(\n",
    "    [input_batch_padded,target_batch_padded],\n",
    "    [np.zeros((BATCH_SIZE,TOTAL_LENGTH,2)),np.zeros(BATCH_SIZE)],\n",
    "    epochs = 500,\n",
    "    callbacks = get_callbacks(input_batch_padded,target_batch_padded, True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Function that takes a \"close\" look of each tragectory\n",
    "    Input:\n",
    "        model: trained Keras model\n",
    "        input: the batch of input of shape (N, input_length + target_length, 2)\n",
    "        target: the batch of input of the same shape with input\n",
    "        input_length: self-explanatory\n",
    "    Output:\n",
    "        Nothing, but generate N pictures of the plot with:\n",
    "            target tragectories (should be input_length + target_length long) in black,\n",
    "            predicted tragectories (shoudl be target_length long) in blue\n",
    "'''\n",
    "def close_visualize(model, input_batch,target_batch,input_length):\n",
    "    if not input_batch.shape == target_batch.shape:\n",
    "        raise ValueError(\"input batch and target batch should have the same size\")\n",
    "    batch_size,_,__ = input_batch.shape\n",
    "    \n",
    "    prediction, loss = model.predict([input_batch,target_batch])\n",
    "    for batch_id in range(batch_size):\n",
    "        # first clear the plot...\n",
    "        plt.gcf().clear()\n",
    "        \n",
    "        # then retrieve the tragectories\n",
    "        target_tragectories = target_batch[batch_id][input_length:]\n",
    "        predicted_tragectories = prediction[batch_id]\n",
    "        # evaluate the boundary to plot\n",
    "        x_min = np.min(target_tragectories[:,0] - .1)\n",
    "        x_max = np.max(target_tragectories[:,0] + .1)\n",
    "        \n",
    "        y_min = np.min(target_tragectories[:,1] - .1)\n",
    "        y_max = np.max(target_tragectories[:,1] + .1)\n",
    "        \n",
    "        x_min_predicted = np.min(predicted_tragectories[:,0] - .1)\n",
    "        x_max_predicted = np.max(predicted_tragectories[:,0] + .1)\n",
    "        \n",
    "        y_min_predicted = np.min(predicted_tragectories[:,1] - .1)\n",
    "        y_max_predicted = np.max(predicted_tragectories[:,1] + .1)\n",
    "        \n",
    "        # set the boundary\n",
    "        x_min = min(x_min,x_min_predicted)\n",
    "        x_max = max(x_max,x_max_predicted)\n",
    "        y_min = min(y_min,y_min_predicted)\n",
    "        y_max = max(y_max,y_max_predicted)\n",
    "        \n",
    "        plt.xlim(x_min,x_max)\n",
    "        plt.ylim(y_min,y_max)\n",
    "        # plot line...\n",
    "        # first the target tragectories\n",
    "#         plt.plot(target_tragectories[:,0],target_tragectories[:,1],c = 'black')\n",
    "        # then the prediction\n",
    "        for i in range(len(target_tragectories) - 1):\n",
    "            cur_point = target_tragectories[i,:]\n",
    "            next_point = target_tragectories[i + 1,:]\n",
    "            plt.plot([cur_point[0],next_point[0]],[cur_point[1],next_point[1]],c = 'black')\n",
    "            # and predicted...\n",
    "            cur_point = predicted_tragectories[i,:]\n",
    "            next_point = predicted_tragectories[i + 1,:]\n",
    "            print(cur_point,next_point)\n",
    "            plt.plot([cur_point[0],next_point[0]],[cur_point[1],next_point[1]],\n",
    "                     c = 'blue',\n",
    "                     linestyle = ':'\n",
    "            )\n",
    "#         plt.plot(predicted_tragectories[:,0],\n",
    "#                   predicted_tragectories[:,1],\n",
    "#                   linestyle = ':',\n",
    "#                   c = 'blue')\n",
    "        # then save the plot...\n",
    "        plt.savefig('close-{}.png'.format(batch_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8.963909 2.314864] [8.941953  2.3164215]\n",
      "[8.941953  2.3164215] [8.919731 2.321545]\n",
      "[8.919731 2.321545] [8.898853  2.3254669]\n",
      "[8.345926  2.4634602] [8.3236685 2.475243 ]\n",
      "[8.3236685 2.475243 ] [8.303656  2.4831889]\n",
      "[8.303656  2.4831889] [8.282155  2.4929714]\n",
      "[8.740453  2.3639095] [8.719084  2.3701067]\n",
      "[8.719084  2.3701067] [8.697542 2.376031]\n",
      "[8.697542 2.376031] [8.675773  2.3817744]\n",
      "[5.952652  4.4132686] [5.9346237 4.4306126]\n",
      "[5.9346237 4.4306126] [5.9157014 4.452982 ]\n",
      "[5.9157014 4.452982 ] [5.8997164 4.4718027]\n",
      "[8.809243  2.3319643] [8.788126  2.3365371]\n",
      "[8.788126  2.3365371] [8.765906  2.3423848]\n",
      "[8.765906  2.3423848] [8.7442665 2.34775  ]\n",
      "[8.903897  2.5688508] [8.884957  2.5681255]\n",
      "[8.884957  2.5681255] [8.866587  2.5692308]\n",
      "[8.866587  2.5692308] [8.849105 2.568795]\n",
      "[8.819116  1.5311345] [8.793456  1.5491201]\n",
      "[8.793456  1.5491201] [8.768486 1.569653]\n",
      "[8.768486 1.569653] [8.7474375 1.5934211]\n",
      "[8.999324  1.0004897] [8.975976  1.0283252]\n",
      "[8.975976  1.0283252] [8.950018  1.0562538]\n",
      "[8.950018  1.0562538] [8.925822  1.0869837]\n",
      "[8.785452 2.401425] [8.765268  2.4060495]\n",
      "[8.765268  2.4060495] [8.744173  2.4106786]\n",
      "[8.744173  2.4106786] [8.723431 2.414798]\n",
      "[9.406969  0.6188053] [9.379547  0.6489869]\n",
      "[9.379547  0.6489869] [9.352114  0.6781765]\n",
      "[9.352114  0.6781765] [9.324389   0.70802397]\n",
      "[9.31433   1.0564805] [9.289259  1.0777599]\n",
      "[9.289259  1.0777599] [9.264764 1.099223]\n",
      "[9.264764 1.099223] [9.239191 1.118938]\n",
      "[8.986001  1.2470981] [8.962977  1.2679694]\n",
      "[8.962977  1.2679694] [8.937722  1.2902459]\n",
      "[8.937722  1.2902459] [8.915157  1.3142467]\n",
      "[5.5447006 4.92338  ] [5.531354 4.942711]\n",
      "[5.531354 4.942711] [5.5165052 4.9621797]\n",
      "[5.5165052 4.9621797] [5.499767  4.9765487]\n",
      "[8.841544 2.560868] [8.822635  2.5615273]\n",
      "[8.822635  2.5615273] [8.803888 2.563013]\n",
      "[8.803888 2.563013] [8.785555  2.5635362]\n",
      "[9.395418   0.61508256] [9.36812   0.6450595]\n",
      "[9.36812   0.6450595] [9.340619 0.674479]\n",
      "[9.340619 0.674479] [9.312833  0.7046399]\n",
      "[8.860204  2.3437278] [8.839212 2.347093]\n",
      "[8.839212 2.347093] [8.817147  2.3522866]\n",
      "[8.817147  2.3522866] [8.795764  2.3567216]\n",
      "[8.233338  1.8479298] [8.202609  1.8724673]\n",
      "[8.202609  1.8724673] [8.17834   1.8963041]\n",
      "[8.17834   1.8963041] [8.158396  1.9264884]\n",
      "[9.537272   0.70754695] [9.508212   0.73326975]\n",
      "[9.508212   0.73326975] [9.482581  0.7596961]\n",
      "[9.482581  0.7596961] [9.454162  0.7831177]\n",
      "[8.233205  2.5065472] [8.211161 2.519479]\n",
      "[8.211161 2.519479] [8.191677  2.5277708]\n",
      "[8.191677  2.5277708] [8.170523  2.5385969]\n",
      "[8.863594  2.3589685] [8.842775  2.3622665]\n",
      "[8.842775  2.3622665] [8.820973  2.3671288]\n",
      "[8.820973  2.3671288] [8.799855 2.371194]\n",
      "[9.477606   0.91675276] [9.450158   0.93892956]\n",
      "[9.450158   0.93892956] [9.426091   0.96171904]\n",
      "[9.426091   0.96171904] [9.3992815  0.98069733]\n",
      "[9.533972 0.46702 ] [9.505495   0.49677137]\n",
      "[9.505495   0.49677137] [9.476563  0.5263064]\n",
      "[9.476563  0.5263064] [9.448213  0.5566322]\n",
      "[9.242326  1.0870081] [9.217919  1.1082155]\n",
      "[9.217919  1.1082155] [9.192979  1.1298501]\n",
      "[9.192979  1.1298501] [9.167617 1.150708]\n",
      "[8.839286 2.548735] [8.820196  2.5497725]\n",
      "[8.820196  2.5497725] [8.801276  2.5514784]\n",
      "[8.801276  2.5514784] [8.782702 2.552269]\n",
      "[6.5592165 4.280659 ] [6.5402727 4.293461 ]\n",
      "[6.5402727 4.293461 ] [6.522261 4.311362]\n",
      "[6.522261 4.311362] [6.50716   4.3270946]\n",
      "[8.321985  2.4193683] [8.29885   2.4323215]\n",
      "[8.29885   2.4323215] [8.278382  2.4413996]\n",
      "[8.278382  2.4413996] [8.256703  2.4524782]\n",
      "[8.915689  2.5618575] [8.896774  2.5608444]\n",
      "[8.896774  2.5608444] [8.878317 2.561991]\n",
      "[8.878317 2.561991] [8.860787  2.5614314]\n",
      "[8.973319 1.037648] [8.950426  1.0636152]\n",
      "[8.950426  1.0636152] [8.924665  1.0910009]\n",
      "[8.924665  1.0910009] [8.900766  1.1208447]\n",
      "[2.6240942 9.763621 ] [2.6229806 9.753704 ]\n",
      "[2.6229806 9.753704 ] [2.618894 9.746378]\n",
      "[2.618894 9.746378] [2.613624 9.738106]\n",
      "[8.575473  2.4709272] [8.554679  2.4790542]\n",
      "[8.554679  2.4790542] [8.534745  2.4845555]\n",
      "[8.534745  2.4845555] [8.513865  2.4907353]\n",
      "[9.3251     0.73748034] [9.298683  0.7661015]\n",
      "[9.298683  0.7661015] [9.272092  0.7942984]\n",
      "[9.272092  0.7942984] [9.244842   0.82289505]\n",
      "[6.4297276 3.8740487] [6.4083643 3.8890443]\n",
      "[6.4083643 3.8890443] [6.3858767 3.9125853]\n",
      "[6.3858767 3.9125853] [6.36871  3.935134]\n",
      "[9.426233  0.8696569] [9.399399   0.89384425]\n",
      "[9.399399   0.89384425] [9.374594  0.9183634]\n",
      "[9.374594  0.9183634] [9.347293   0.94029284]\n",
      "[9.453377   0.83222896] [9.426039  0.8567518]\n",
      "[9.426039  0.8567518] [9.401102   0.88167536]\n",
      "[9.401102   0.88167536] [9.373618 0.903805]\n",
      "[9.300927   0.72343796] [9.274574   0.75291383]\n",
      "[9.274574   0.75291383] [9.24769    0.78166753]\n",
      "[9.24769    0.78166753] [9.220555  0.8114923]\n",
      "[9.505117   0.79570484] [9.476768   0.81964463]\n",
      "[9.476768   0.81964463] [9.451864  0.8447878]\n",
      "[9.451864  0.8447878] [9.423937  0.8663909]\n",
      "[9.092967  1.1949936] [9.069615  1.2150158]\n",
      "[9.069615  1.2150158] [9.04414   1.2362981]\n",
      "[9.04414   1.2362981] [9.020237  1.2581891]\n",
      "[9.654288   0.35179684] [9.624516   0.37965706]\n",
      "[9.624516   0.37965706] [9.593749  0.4078962]\n",
      "[9.593749  0.4078962] [9.565245   0.43759817]\n",
      "[8.906259  2.5537627] [8.887152  2.5532162]\n",
      "[8.887152  2.5532162] [8.868561 2.554557]\n",
      "[8.868561 2.554557] [8.850714 2.554305]\n",
      "[2.675256 9.822646] [2.6699364 9.811687 ]\n",
      "[2.6699364 9.811687 ] [2.6671772 9.804576 ]\n",
      "[2.6671772 9.804576 ] [2.664853 9.795686]\n",
      "[8.539914 2.469877] [8.51852   2.4790423]\n",
      "[8.51852   2.4790423] [8.498662 2.485011]\n",
      "[8.498662 2.485011] [8.477633  2.4919534]\n",
      "[2.6433907 9.768354 ] [2.6408534 9.75861  ]\n",
      "[2.6408534 9.75861  ] [2.6373775 9.751264 ]\n",
      "[2.6373775 9.751264 ] [2.633255 9.742846]\n",
      "[8.873108  2.3149014] [8.85159   2.3185885]\n",
      "[8.85159   2.3185885] [8.829155  2.3242915]\n",
      "[8.829155  2.3242915] [8.807582  2.3293564]\n",
      "[9.156264  1.1734389] [9.132242  1.1939781]\n",
      "[9.132242  1.1939781] [9.106922  1.2147433]\n",
      "[9.106922  1.2147433] [9.082604  1.2358528]\n",
      "[8.471334 2.451986] [8.449527  2.4624443]\n",
      "[8.449527  2.4624443] [8.429416  2.4693902]\n",
      "[8.429416  2.4693902] [8.408093  2.4776347]\n",
      "[9.77703    0.26125258] [9.745405  0.2864518]\n",
      "[9.745405  0.2864518] [9.7131    0.3123596]\n",
      "[9.7131    0.3123596] [9.685246   0.34055257]\n",
      "[9.5537    0.7470657] [9.524368   0.77129525]\n",
      "[9.524368   0.77129525] [9.499285   0.79686385]\n",
      "[9.499285   0.79686385] [9.470924  0.8185856]\n",
      "[9.622753  0.4332594] [9.592879  0.4621653]\n",
      "[9.592879  0.4621653] [9.563621   0.49090517]\n",
      "[9.563621   0.49090517] [9.534798   0.51988333]\n",
      "[8.348452  2.7347019] [8.329225  2.7417495]\n",
      "[8.329225  2.7417495] [8.31149   2.7457094]\n",
      "[8.31149   2.7457094] [8.291586  2.7527018]\n",
      "[7.91416 2.7049 ] [7.894209  2.7187717]\n",
      "[7.894209  2.7187717] [7.8760715 2.7274144]\n",
      "[7.8760715 2.7274144] [7.855459  2.7410076]\n",
      "[7.593318  2.7296598] [7.572686  2.7488594]\n",
      "[7.572686  2.7488594] [7.5545964 2.7616262]\n",
      "[7.5545964 2.7616262] [7.535404  2.7808483]\n",
      "[8.8889885 2.286906 ] [8.866831  2.2905326]\n",
      "[8.866831  2.2905326] [8.844055  2.2967234]\n",
      "[8.844055  2.2967234] [8.822235 2.302333]\n",
      "[9.635367   0.41353583] [9.60537    0.44190794]\n",
      "[9.60537    0.44190794] [9.575791   0.47053868]\n",
      "[9.575791   0.47053868] [9.546882   0.49951178]\n",
      "[5.6787024 4.442924 ] [5.660782  4.4624615]\n",
      "[5.660782  4.4624615] [5.641357 4.486912]\n",
      "[5.641357 4.486912] [5.6257954 4.507559 ]\n",
      "[8.30054   2.5200512] [8.279199  2.5313387]\n",
      "[8.279199  2.5313387] [8.259764  2.5386505]\n",
      "[8.259764  2.5386505] [8.238516  2.5480316]\n",
      "[8.864253  2.5813942] [8.845242  2.5815141]\n",
      "[8.845242  2.5815141] [8.82694   2.5826318]\n",
      "[8.82694   2.5826318] [8.809242 2.582671]\n",
      "[7.1797433 2.8943808] [7.159171  2.9137707]\n",
      "[7.159171  2.9137707] [7.1394777 2.9322755]\n",
      "[7.1394777 2.9322755] [7.121427  2.9569097]\n",
      "[9.001175  2.3355725] [8.979325 2.335831]\n",
      "[8.979325 2.335831] [8.957594  2.3404238]\n",
      "[8.957594  2.3404238] [8.937341 2.343313]\n",
      "[8.917455 2.332053] [8.896045 2.334376]\n",
      "[8.896045 2.334376] [8.8738785 2.339399 ]\n",
      "[8.8738785 2.339399 ] [8.852703  2.3434556]\n",
      "[9.768475   0.25433543] [9.737272  0.2792382]\n",
      "[9.737272  0.2792382] [9.70484    0.30516842]\n",
      "[9.70484    0.30516842] [9.677031   0.33352253]\n",
      "[3.2719967 6.8932614] [3.2493174 6.915946 ]\n",
      "[3.2493174 6.915946 ] [3.2271962 6.937395 ]\n",
      "[3.2271962 6.937395 ] [3.2141378 6.9614043]\n",
      "[9.044496  1.2679714] [9.021144  1.2871917]\n",
      "[9.021144  1.2871917] [8.995924  1.3078656]\n",
      "[8.995924  1.3078656] [8.972931 1.329489]\n",
      "[9.234519   0.82149476] [9.209334  0.8494473]\n",
      "[9.209334  0.8494473] [9.182937  0.8771355]\n",
      "[9.182937  0.8771355] [9.156228 0.905886]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.7294106 9.833407 ] [2.7191424 9.822831 ]\n",
      "[2.7191424 9.822831 ] [2.7182167 9.815828 ]\n",
      "[2.7182167 9.815828 ] [2.7190282 9.806581 ]\n",
      "[8.904262  2.3256128] [8.882549 2.328102]\n",
      "[8.882549 2.328102] [8.860271  2.3334053]\n",
      "[8.860271  2.3334053] [8.838659  2.3377838]\n",
      "[6.009138  4.5636725] [5.9926505 4.580005 ]\n",
      "[5.9926505 4.580005 ] [5.9756393 4.599977 ]\n",
      "[5.9756393 4.599977 ] [5.9595914 4.6159825]\n",
      "[2.9070408 8.132476 ] [2.9025228 8.144393 ]\n",
      "[2.9025228 8.144393 ] [2.8922577 8.152995 ]\n",
      "[2.8922577 8.152995 ] [2.8758101 8.160357 ]\n",
      "[9.43947   0.6013277] [9.411708  0.6312239]\n",
      "[9.411708  0.6312239] [9.384249  0.6603501]\n",
      "[9.384249  0.6603501] [9.356215  0.6897732]\n",
      "[8.798264  2.5894258] [8.779214 2.590837]\n",
      "[8.779214 2.590837] [8.760828 2.592215]\n",
      "[8.760828 2.592215] [8.742644  2.5931268]\n",
      "[9.768081   0.25682485] [9.736798   0.28210673]\n",
      "[9.736798   0.28210673] [9.704389  0.3080523]\n",
      "[9.704389  0.3080523] [9.676673   0.33651942]\n",
      "[1.5830098 9.664341 ] [1.5827913 9.670332 ]\n",
      "[1.5827913 9.670332 ] [1.5709686 9.677387 ]\n",
      "[1.5709686 9.677387 ] [1.5616431 9.684293 ]\n",
      "[8.864657  2.5546916] [8.845745  2.5549939]\n",
      "[8.845745  2.5549939] [8.8270035 2.5564651]\n",
      "[8.8270035 2.5564651] [8.808853  2.5567517]\n",
      "[9.200095 0.845484] [9.1752205 0.8738082]\n",
      "[9.1752205 0.8738082] [9.148748  0.9014373]\n",
      "[9.148748  0.9014373] [9.1223755 0.9306489]\n",
      "[9.7179575 0.3217199] [9.686874   0.34892574]\n",
      "[9.686874   0.34892574] [9.655605   0.37616855]\n",
      "[9.655605   0.37616855] [9.627273  0.4050176]\n",
      "[3.8642194 6.6586046] [3.847745 6.674101]\n",
      "[3.847745 6.674101] [3.8300228 6.690527 ]\n",
      "[3.8300228 6.690527 ] [3.8175359 6.704635 ]\n",
      "[9.274135  0.7377897] [9.248183   0.76681465]\n",
      "[9.248183   0.76681465] [9.221329  0.7956105]\n",
      "[9.221329  0.7956105] [9.194263   0.82555217]\n",
      "[9.207419  1.0656446] [9.183392  1.0879793]\n",
      "[9.183392  1.0879793] [9.158099  1.1104867]\n",
      "[9.158099  1.1104867] [9.132772  1.1329287]\n",
      "[8.503606  2.3865168] [8.480956 2.397022]\n",
      "[8.480956 2.397022] [8.459951  2.4049344]\n",
      "[8.459951  2.4049344] [8.438007 2.413923]\n",
      "[9.563342   0.72749335] [9.53381    0.75235295]\n",
      "[9.53381    0.75235295] [9.508558   0.77815574]\n",
      "[9.508558   0.77815574] [9.480154  0.8003068]\n",
      "[9.494351   0.54748577] [9.466061  0.5771108]\n",
      "[9.466061  0.5771108] [9.438185 0.606333]\n",
      "[9.438185 0.606333] [9.40977   0.6356324]\n",
      "[2.6756394 9.842225 ] [2.6707072 9.830324 ]\n",
      "[2.6707072 9.830324 ] [2.6676455 9.823217 ]\n",
      "[2.6676455 9.823217 ] [2.6647446 9.814002 ]\n",
      "[9.730614  0.2945572] [9.699633  0.3205535]\n",
      "[9.699633  0.3205535] [9.66792   0.3474485]\n",
      "[9.66792   0.3474485] [9.639625   0.37615398]\n",
      "[9.417046  0.9221814] [9.390344  0.9459423]\n",
      "[9.390344  0.9459423] [9.365837  0.9694731]\n",
      "[9.365837  0.9694731] [9.338953   0.99049383]\n",
      "[8.235805 2.525462] [8.214046 2.538324]\n",
      "[8.214046 2.538324] [8.194738 2.546338]\n",
      "[8.194738 2.546338] [8.173698  2.5569825]\n",
      "[8.950084  2.2830753] [8.927333 2.285519]\n",
      "[8.927333 2.285519] [8.904635  2.2913525]\n",
      "[8.904635  2.2913525] [8.883084  2.2963471]\n",
      "[9.444514  0.8462247] [9.417347  0.8702736]\n",
      "[9.417347  0.8702736] [9.392499   0.89508444]\n",
      "[9.392499   0.89508444] [9.364961 0.917031]\n",
      "[8.838641  2.5742314] [8.819783  2.5747495]\n",
      "[8.819783  2.5747495] [8.8012295 2.57605  ]\n",
      "[8.8012295 2.57605  ] [8.783112  2.5764413]\n",
      "[9.654237   0.34382194] [9.624564   0.37142536]\n",
      "[9.624564   0.37142536] [9.593666   0.39963004]\n",
      "[9.593666   0.39963004] [9.565186   0.42940167]\n",
      "[2.787638 7.368777] [2.7642956 7.391435 ]\n",
      "[2.7642956 7.391435 ] [2.7446032 7.409064 ]\n",
      "[2.7446032 7.409064 ] [2.7253637 7.4313316]\n",
      "[0.85881436 9.578933  ] [0.8354383 9.60455  ]\n",
      "[0.8354383 9.60455  ] [0.80154353 9.625689  ]\n",
      "[0.80154353 9.625689  ] [0.77077925 9.654818  ]\n",
      "[9.514457   0.50146514] [9.486096  0.5313299]\n",
      "[9.486096  0.5313299] [9.45762  0.560763]\n",
      "[9.45762  0.560763] [9.429252 0.590721]\n",
      "[9.652051  0.6388056] [9.620822  0.6643607]\n",
      "[9.620822  0.6643607] [9.595027  0.6906534]\n",
      "[9.595027  0.6906534] [9.566088   0.71317816]\n",
      "[9.708392   0.32366267] [9.677559  0.3505824]\n",
      "[9.677559  0.3505824] [9.646354   0.37800634]\n",
      "[9.646354   0.37800634] [9.617857 0.406908]\n",
      "[8.758645  1.5565261] [8.733359  1.5733702]\n",
      "[8.733359  1.5733702] [8.708287  1.5943632]\n",
      "[8.708287  1.5943632] [8.687251 1.618534]\n",
      "[8.813626  2.5798032] [8.794873 2.58063 ]\n",
      "[8.794873 2.58063 ] [8.776311 2.581969]\n",
      "[8.776311 2.581969] [8.758063  2.5825944]\n",
      "[2.6822112 9.79147  ] [2.676516 9.781608]\n",
      "[2.676516 9.781608] [2.6742148 9.774388 ]\n",
      "[2.6742148 9.774388 ] [2.6723354 9.765651 ]\n",
      "[9.725312   0.29792824] [9.694438   0.32419032]\n",
      "[9.694438   0.32419032] [9.662765   0.35114387]\n",
      "[9.662765   0.35114387] [9.63454   0.3799501]\n",
      "[1.9412361 8.176709 ] [1.9102359 8.202281 ]\n",
      "[1.9102359 8.202281 ] [1.8934288 8.218115 ]\n",
      "[1.8934288 8.218115 ] [1.8696635 8.242553 ]\n",
      "[9.256914   0.73933095] [9.230972  0.7692124]\n",
      "[9.230972  0.7692124] [9.203998  0.7981971]\n",
      "[9.203998  0.7981971] [9.177151  0.8288861]\n",
      "[9.545319   0.72953874] [9.516062  0.7548346]\n",
      "[9.516062  0.7548346] [9.490701 0.780821]\n",
      "[9.490701 0.780821] [9.462323 0.803491]\n",
      "[9.451598   0.58160275] [9.423725  0.6118066]\n",
      "[9.423725  0.6118066] [9.396062  0.6410835]\n",
      "[9.396062  0.6410835] [9.367944  0.6708019]\n",
      "[8.931914  2.5580428] [8.912675  2.5568557]\n",
      "[8.912675  2.5568557] [8.894251  2.5580454]\n",
      "[8.894251  2.5580454] [8.876682  2.5573924]\n",
      "[9.06578   1.2136317] [9.042406  1.2336907]\n",
      "[9.042406  1.2336907] [9.017002  1.2551787]\n",
      "[9.017002  1.2551787] [8.993275  1.2775028]\n",
      "[9.456695  0.8378545] [9.429234  0.8622875]\n",
      "[9.429234  0.8622875] [9.404368  0.8871598]\n",
      "[9.404368  0.8871598] [9.376775  0.9091813]\n",
      "[8.909962  2.3493552] [8.888868  2.3519316]\n",
      "[8.888868  2.3519316] [8.86696   2.3566504]\n",
      "[8.86696   2.3566504] [8.846039 2.36043 ]\n",
      "[9.428762  0.5929193] [9.401183  0.6229873]\n",
      "[9.401183  0.6229873] [9.373567  0.6523247]\n",
      "[9.373567  0.6523247] [9.345609   0.68221414]\n",
      "[8.632626 2.665718] [8.61351   2.6695254]\n",
      "[8.61351   2.6695254] [8.595829  2.6711667]\n",
      "[8.595829  2.6711667] [8.577672 2.673857]\n",
      "[8.889974 2.570932] [8.870917  2.5704393]\n",
      "[8.870917  2.5704393] [8.852537  2.5715938]\n",
      "[8.852537  2.5715938] [8.834776  2.5713239]\n",
      "[9.023204  0.9770276] [9.000355  1.0041839]\n",
      "[9.000355  1.0041839] [8.974177  1.0318915]\n",
      "[8.974177  1.0318915] [8.949685  1.0619402]\n",
      "[8.333613  2.5174348] [8.3122635 2.5289137]\n",
      "[8.3122635 2.5289137] [8.292854 2.536016]\n",
      "[8.292854 2.536016] [8.2717285 2.5451684]\n",
      "[8.847523  2.5748718] [8.828468  2.5753686]\n",
      "[8.828468  2.5753686] [8.809997  2.5766585]\n",
      "[8.809997  2.5766585] [8.791936  2.5769994]\n",
      "[8.919311  1.0808483] [8.896181  1.1067328]\n",
      "[8.896181  1.1067328] [8.870736  1.1342831]\n",
      "[8.870736  1.1342831] [8.847613  1.1647804]\n",
      "[8.902387  2.3537266] [8.881531  2.3561018]\n",
      "[8.881531  2.3561018] [8.859626  2.3607621]\n",
      "[8.859626  2.3607621] [8.838679  2.3644392]\n",
      "[9.260243  0.7139795] [9.23406    0.74448895]\n",
      "[9.23406    0.74448895] [9.206869   0.77388424]\n",
      "[9.206869   0.77388424] [9.180053   0.80530614]\n",
      "[8.970539  1.0574049] [8.947376  1.0839019]\n",
      "[8.947376  1.0839019] [8.921692  1.1109668]\n",
      "[8.921692  1.1109668] [8.898111  1.1408013]\n",
      "[2.467339 9.276076] [2.4707112 9.2779045]\n",
      "[2.4707112 9.2779045] [2.4657297 9.277047 ]\n",
      "[2.4657297 9.277047 ] [2.452327 9.276707]\n",
      "[8.507574  2.4144638] [8.485446  2.4243453]\n",
      "[8.485446  2.4243453] [8.464777  2.4316401]\n",
      "[8.464777  2.4316401] [8.442978  2.4399073]\n",
      "[9.286807  0.6952499] [9.260408  0.7252957]\n",
      "[9.260408  0.7252957] [9.233186   0.75469136]\n",
      "[9.233186   0.75469136] [9.206104  0.7856776]\n",
      "[9.165231  0.8307165] [9.140392  0.8600376]\n",
      "[9.140392  0.8600376] [9.113665   0.88853335]\n",
      "[9.113665   0.88853335] [9.087512  0.9192658]\n",
      "[9.614688  0.6668541] [9.584282  0.6920876]\n",
      "[9.584282  0.6920876] [9.558649 0.71832 ]\n",
      "[9.558649 0.71832 ] [9.5299015 0.7408034]\n",
      "[8.778814  2.5061505] [8.759574 2.509342]\n",
      "[8.759574 2.509342] [8.73998   2.5121343]\n",
      "[8.73998   2.5121343] [8.720624  2.5144122]\n",
      "[8.529411  2.6208243] [8.509529  2.6269815]\n",
      "[8.509529  2.6269815] [8.491315  2.6304135]\n",
      "[8.491315  2.6304135] [8.471536 2.63515 ]\n",
      "[8.873387 2.339866] [8.852258  2.3431814]\n",
      "[8.852258  2.3431814] [8.830162  2.3483663]\n",
      "[8.830162  2.3483663] [8.808817  2.3527792]\n",
      "[8.694054  2.3711386] [8.672719  2.3780665]\n",
      "[8.672719  2.3780665] [8.651301  2.3842804]\n",
      "[8.651301  2.3842804] [8.629478  2.3904986]\n",
      "[8.817237  2.3489215] [8.796217 2.353478]\n",
      "[8.796217 2.353478] [8.77432  2.358924]\n",
      "[8.77432  2.358924] [8.752909 2.363856]\n",
      "[9.126881   0.87408584] [9.102694  0.9027838]\n",
      "[9.102694  0.9027838] [9.076163  0.9309814]\n",
      "[9.076163  0.9309814] [9.050481  0.9614155]\n",
      "[8.958987 2.302811] [8.936855 2.304489]\n",
      "[8.936855 2.304489] [8.914419  2.3098516]\n",
      "[8.914419  2.3098516] [8.893307  2.3141077]\n",
      "[9.0318    0.9990361] [9.008875  1.0255651]\n",
      "[9.008875  1.0255651] [8.982767  1.0526364]\n",
      "[8.982767  1.0526364] [8.958261  1.0819145]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.3323234 9.347001 ] [1.3153262 9.361366 ]\n",
      "[1.3153262 9.361366 ] [1.3010783 9.374717 ]\n",
      "[1.3010783 9.374717 ] [1.2931248 9.388701 ]\n",
      "[9.39667   0.9761093] [9.370075  1.0002114]\n",
      "[9.370075  1.0002114] [9.345685  1.0227548]\n",
      "[9.345685  1.0227548] [9.319459  1.0433725]\n",
      "[9.307696   0.72242725] [9.281169  0.7523581]\n",
      "[9.281169  0.7523581] [9.2542715 0.7810925]\n",
      "[9.2542715 0.7810925] [9.227143   0.81101227]\n",
      "[9.63539   0.4120759] [9.605387  0.4406624]\n",
      "[9.605387  0.4406624] [9.575774  0.4693062]\n",
      "[9.575774  0.4693062] [9.54691    0.49841085]\n",
      "[7.9123216 2.0956984] [7.8810863 2.125173 ]\n",
      "[7.8810863 2.125173 ] [7.8594246 2.146489 ]\n",
      "[7.8594246 2.146489 ] [7.842164  2.1744628]\n",
      "[9.3371525 0.6608862] [9.310359   0.69081223]\n",
      "[9.310359   0.69081223] [9.283061   0.72019243]\n",
      "[9.283061   0.72019243] [9.255633   0.75067466]\n",
      "[9.354477   0.95777404] [9.328973 0.9808  ]\n",
      "[9.328973 0.9808  ] [9.304291  1.0041761]\n",
      "[9.304291  1.0041761] [9.277742  1.0255419]\n",
      "[9.117878  1.1937264] [9.094401  1.2133685]\n",
      "[9.094401  1.2133685] [9.068955  1.2341686]\n",
      "[9.068955  1.2341686] [9.044946  1.2553406]\n",
      "[8.871579  2.3011255] [8.849387  2.3054857]\n",
      "[8.849387  2.3054857] [8.826853  2.3115711]\n",
      "[8.826853  2.3115711] [8.804947  2.3171775]\n",
      "[9.619124  0.3760799] [9.589985   0.40394482]\n",
      "[9.589985   0.40394482] [9.559669   0.43268672]\n",
      "[9.559669   0.43268672] [9.531001   0.46251655]\n",
      "[7.5721903 2.4735582] [7.5479183 2.500724 ]\n",
      "[7.5479183 2.500724 ] [7.5299835 2.5170677]\n",
      "[7.5299835 2.5170677] [7.515361  2.5400288]\n",
      "[9.2759905 1.0421607] [9.251174  1.0649751]\n",
      "[9.251174  1.0649751] [9.226275  1.0872804]\n",
      "[9.226275  1.0872804] [9.200675  1.1088129]\n",
      "[7.8110094 2.9771254] [7.7943277 2.985707 ]\n",
      "[7.7943277 2.985707 ] [7.7760124 2.993072 ]\n",
      "[7.7760124 2.993072 ] [7.7528477 3.0065255]\n",
      "[9.594953   0.46843144] [9.5654545  0.49744788]\n",
      "[9.5654545  0.49744788] [9.536786  0.5263994]\n",
      "[9.536786  0.5263994] [9.507966   0.55520946]\n",
      "[6.24      4.5355506] [6.222849 4.549952]\n",
      "[6.222849 4.549952] [6.206563  4.5680118]\n",
      "[6.206563  4.5680118] [6.1914945 4.582446 ]\n",
      "[6.741742  3.8506362] [6.722019 3.862622]\n",
      "[6.722019 3.862622] [6.700196  3.8823001]\n",
      "[6.700196  3.8823001] [6.6819267 3.9021208]\n",
      "[8.82859  2.355137] [8.807417 2.359402]\n",
      "[8.807417 2.359402] [8.785626  2.3646662]\n",
      "[8.785626  2.3646662] [8.764127  2.3693402]\n",
      "[8.831913  2.4070327] [8.811855  2.4105573]\n",
      "[8.811855  2.4105573] [8.790803  2.4146924]\n",
      "[8.790803  2.4146924] [8.770297  2.4180562]\n",
      "[8.592358  1.6950544] [8.564841  1.7122449]\n",
      "[8.564841  1.7122449] [8.539514  1.7336606]\n",
      "[8.539514  1.7336606] [8.518067  1.7590989]\n",
      "[9.621928  0.6512452] [9.591362   0.67678297]\n",
      "[9.591362   0.67678297] [9.565564   0.70319134]\n",
      "[9.565564   0.70319134] [9.536737   0.72599643]\n",
      "[8.904811  2.5423071] [8.885672  2.5418892]\n",
      "[8.885672  2.5418892] [8.866888  2.5434046]\n",
      "[8.866888  2.5434046] [8.848774 2.543308]\n",
      "[8.945573  1.0427219] [8.922477  1.0691825]\n",
      "[8.922477  1.0691825] [8.89684   1.0971023]\n",
      "[8.89684   1.0971023] [8.873203  1.1278002]\n",
      "[9.306762   0.69037956] [9.280212  0.7199962]\n",
      "[9.280212  0.7199962] [9.253057   0.74925655]\n",
      "[9.253057   0.74925655] [9.225765  0.7797164]\n",
      "[8.943704  2.3217585] [8.92236  2.323418]\n",
      "[8.92236  2.323418] [8.900096 2.32843 ]\n",
      "[8.900096 2.32843 ] [8.8793   2.332321]\n",
      "[9.312877   0.69482434] [9.286267  0.7244219]\n",
      "[9.286267  0.7244219] [9.259178   0.75353765]\n",
      "[9.259178   0.75353765] [9.231888  0.7837562]\n",
      "[8.432303  2.4304647] [8.41001   2.4418528]\n",
      "[8.41001   2.4418528] [8.389638  2.4495912]\n",
      "[8.389638  2.4495912] [8.368185  2.4588256]\n",
      "[9.424177   0.87345284] [9.397346 0.897792]\n",
      "[9.397346 0.897792] [9.372545   0.92225915]\n",
      "[9.372545   0.92225915] [9.345297  0.9442276]\n",
      "[8.805306  2.5913527] [8.7863035 2.5923865]\n",
      "[8.7863035 2.5923865] [8.767929  2.5936654]\n",
      "[8.767929  2.5936654] [8.749683  2.5943694]\n",
      "[9.477282   0.52839565] [9.449288 0.558594]\n",
      "[9.449288 0.558594] [9.421045  0.5881902]\n",
      "[9.421045  0.5881902] [9.392826  0.6185036]\n",
      "[7.4791775 2.8184335] [7.459271  2.8364363]\n",
      "[7.459271  2.8364363] [7.4406624 2.8500636]\n",
      "[7.4406624 2.8500636] [7.420934 2.87033 ]\n",
      "[7.032965 3.365791] [7.013649 3.377095]\n",
      "[7.013649 3.377095] [6.9907045 3.3955321]\n",
      "[6.9907045 3.3955321] [6.967874 3.41782 ]\n",
      "[9.477054   0.81656176] [9.449146   0.84132606]\n",
      "[9.449146   0.84132606] [9.424212   0.86641765]\n",
      "[9.424212   0.86641765] [9.396463   0.88856614]\n",
      "[1.1964419 9.119301 ] [1.168261 9.135884]\n",
      "[1.168261 9.135884] [1.152235 9.154409]\n",
      "[1.152235 9.154409] [1.1471784 9.17339  ]\n",
      "[8.604962  2.6258166] [8.585682 2.630347]\n",
      "[8.585682 2.630347] [8.56755   2.6328244]\n",
      "[8.56755   2.6328244] [8.548434  2.6361492]\n",
      "[9.744082   0.24705596] [9.713734   0.27210903]\n",
      "[9.713734   0.27210903] [9.681109   0.29818785]\n",
      "[9.681109   0.29818785] [9.653583   0.32714194]\n",
      "[9.389598  0.8570878] [9.363346   0.88130474]\n",
      "[9.363346   0.88130474] [9.338254   0.90647453]\n",
      "[9.338254   0.90647453] [9.310909   0.92945135]\n",
      "[8.395829 2.392924] [8.372427  2.4060216]\n",
      "[8.372427  2.4060216] [8.351697  2.4149625]\n",
      "[8.351697  2.4149625] [8.330048  2.4257483]\n",
      "[9.583335  0.5051114] [9.553882   0.53369516]\n",
      "[9.553882   0.53369516] [9.525808  0.5624777]\n",
      "[9.525808  0.5624777] [9.496866  0.5905143]\n",
      "[9.045888 1.237736] [9.022564  1.2576213]\n",
      "[9.022564  1.2576213] [8.99725   1.2789758]\n",
      "[8.99725   1.2789758] [8.973867  1.3013265]\n",
      "[2.5090113 9.376487 ] [2.5115995 9.376408 ]\n",
      "[2.5115995 9.376408 ] [2.5074468 9.374129 ]\n",
      "[2.5074468 9.374129 ] [2.4960806 9.3720665]\n",
      "[8.864467  2.5938084] [8.845594  2.5937676]\n",
      "[8.845594  2.5937676] [8.82747   2.5946882]\n",
      "[8.82747   2.5946882] [8.810124  2.5945754]\n",
      "[9.445583  0.5659269] [9.417907  0.5962319]\n",
      "[9.417907  0.5962319] [9.390018  0.6256854]\n",
      "[9.390018  0.6256854] [9.362049  0.6558364]\n",
      "[8.901021 2.326348] [8.879592  2.3291693]\n",
      "[8.879592  2.3291693] [8.857336 2.334431]\n",
      "[8.857336 2.334431] [8.836025  2.3388488]\n",
      "[9.615895   0.68010455] [9.5854435 0.704944 ]\n",
      "[9.5854435 0.704944 ] [9.559995  0.7309882]\n",
      "[9.559995  0.7309882] [9.53123   0.7530568]\n",
      "[8.825142 2.336308] [8.803463 2.341281]\n",
      "[8.803463 2.341281] [8.781465  2.3470023]\n",
      "[8.781465  2.3470023] [8.759718  2.3522997]\n",
      "[5.5969334 5.276574 ] [5.58416   5.2912397]\n",
      "[5.58416   5.2912397] [5.5740952 5.305723 ]\n",
      "[5.5740952 5.305723 ] [5.559654  5.3143053]\n",
      "[8.908295  2.5741045] [8.889025  2.5734444]\n",
      "[8.889025  2.5734444] [8.870795  2.5745165]\n",
      "[8.870795  2.5745165] [8.853327 2.574071]\n",
      "[8.801068  2.6046698] [8.782134  2.6057224]\n",
      "[8.782134  2.6057224] [8.763948  2.6068301]\n",
      "[8.763948  2.6068301] [8.746058  2.6074927]\n",
      "[9.610058  0.4255172] [9.580506   0.45458153]\n",
      "[9.580506   0.45458153] [9.551077  0.4834889]\n",
      "[9.551077  0.4834889] [9.522344   0.51295185]\n",
      "[8.930355 2.560089] [8.911252  2.5588856]\n",
      "[8.911252  2.5588856] [8.892842  2.5600317]\n",
      "[8.892842  2.5600317] [8.875402  2.5593553]\n",
      "[8.843526  2.5867689] [8.824697  2.5869727]\n",
      "[8.824697  2.5869727] [8.806355  2.5880644]\n",
      "[8.806355  2.5880644] [8.788497  2.5882206]\n",
      "[9.562796  0.7906842] [9.53325   0.8149115]\n",
      "[9.53325   0.8149115] [9.508603  0.8395335]\n",
      "[9.508603  0.8395335] [9.480671  0.8601542]\n",
      "[9.549226  0.7106984] [9.520006   0.73591864]\n",
      "[9.520006   0.73591864] [9.4945     0.76215154]\n",
      "[9.4945     0.76215154] [9.4660425  0.78505296]\n",
      "[9.565858   0.74758416] [9.536102   0.77284235]\n",
      "[9.536102   0.77284235] [9.51101   0.7982691]\n",
      "[9.51101   0.7982691] [9.482798  0.8201377]\n",
      "[9.04528   1.2253263] [9.021986  1.2457132]\n",
      "[9.021986  1.2457132] [8.996633  1.2673324]\n",
      "[8.996633  1.2673324] [8.973258  1.2900608]\n",
      "[9.192134   0.85426205] [9.167429  0.8823597]\n",
      "[9.167429  0.8823597] [9.140963   0.90985835]\n",
      "[9.140963   0.90985835] [9.114706  0.9389387]\n",
      "[5.275028 5.148352] [5.263373  5.1704473]\n",
      "[5.263373  5.1704473] [5.2497125 5.1899085]\n",
      "[5.2497125 5.1899085] [5.232868  5.2040644]\n",
      "[2.674966 9.788716] [2.6695442 9.779101 ]\n",
      "[2.6695442 9.779101 ] [2.667143 9.771898]\n",
      "[2.667143 9.771898] [2.6652064 9.763344 ]\n",
      "[8.822841  2.5841963] [8.803869  2.5849462]\n",
      "[8.803869  2.5849462] [8.785419 2.586197]\n",
      "[8.785419 2.586197] [8.767256  2.5867374]\n",
      "[9.641333  0.5889026] [9.61049   0.6152357]\n",
      "[9.61049   0.6152357] [9.583936  0.6423108]\n",
      "[9.583936  0.6423108] [9.554878   0.66637933]\n",
      "[9.2830305 1.0127968] [9.258431  1.0350461]\n",
      "[9.258431  1.0350461] [9.233525  1.0579689]\n",
      "[9.233525  1.0579689] [9.207455  1.0796438]\n",
      "[8.827703  2.5562274] [8.80866   2.5573323]\n",
      "[8.80866   2.5573323] [8.789812  2.5589774]\n",
      "[8.789812  2.5589774] [8.771241 2.559809]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.40977  6.277756] [4.398903  6.2964635]\n",
      "[4.398903  6.2964635] [4.3869095 6.311941 ]\n",
      "[4.3869095 6.311941 ] [4.3709364 6.3218203]\n",
      "[9.590545   0.76694685] [9.560345  0.7910651]\n",
      "[9.560345  0.7910651] [9.535637  0.8158405]\n",
      "[9.535637  0.8158405] [9.507487  0.8363552]\n",
      "[8.602065  2.3408654] [8.579082 2.350966]\n",
      "[8.579082 2.350966] [8.557512  2.3588853]\n",
      "[8.557512  2.3588853] [8.535393  2.3676057]\n",
      "[9.599223  0.7303732] [9.568773 0.756206]\n",
      "[9.568773 0.756206] [9.543661  0.7815923]\n",
      "[9.543661  0.7815923] [9.515377  0.8033642]\n",
      "[8.816148  2.3069384] [8.7940645 2.3122034]\n",
      "[8.7940645 2.3122034] [8.771594  2.3186412]\n",
      "[8.771594  2.3186412] [8.749451  2.3247695]\n",
      "[8.728295  1.6231943] [8.7017355 1.640737 ]\n",
      "[8.7017355 1.640737 ] [8.676551  1.6612186]\n",
      "[8.676551  1.6612186] [8.655593  1.6854504]\n",
      "[9.020139  1.3174489] [8.996662  1.3356348]\n",
      "[8.996662  1.3356348] [8.971593  1.3557981]\n",
      "[8.971593  1.3557981] [8.949106  1.3770012]\n",
      "[8.404754 2.380998] [8.38133   2.3931334]\n",
      "[8.38133   2.3931334] [8.3603325 2.4021752]\n",
      "[8.3603325 2.4021752] [8.338316  2.4128325]\n",
      "[8.900237  2.3689349] [8.879757  2.3714914]\n",
      "[8.879757  2.3714914] [8.8581085 2.3758311]\n",
      "[8.8581085 2.3758311] [8.837639  2.3792253]\n",
      "[8.946525 2.578239] [8.927067 2.576573]\n",
      "[8.927067 2.576573] [8.90904  2.577445]\n",
      "[8.90904  2.577445] [8.891978  2.5764155]\n",
      "[9.125152  1.1824505] [9.101358 1.203075]\n",
      "[9.101358 1.203075] [9.075898  1.2241379]\n",
      "[9.075898  1.2241379] [9.051778  1.2458563]\n",
      "[9.337161  0.6622813] [9.310299  0.6923187]\n",
      "[9.310299  0.6923187] [9.283009   0.72171694]\n",
      "[9.283009   0.72171694] [9.25556    0.75226384]\n",
      "[8.85588   2.2793274] [8.83357  2.283861]\n",
      "[8.83357  2.283861] [8.810664  2.2905066]\n",
      "[8.810664  2.2905066] [8.788548 2.296767]\n",
      "[9.48599    0.79127294] [9.457875  0.8159526]\n",
      "[9.457875  0.8159526] [9.432795  0.8414511]\n",
      "[9.432795  0.8414511] [9.40482    0.86389863]\n",
      "[9.378314  0.9565635] [9.352543  0.9789651]\n",
      "[9.352543  0.9789651] [9.328049 1.002082]\n",
      "[9.328049 1.002082] [9.301436  1.0226798]\n",
      "[8.670906  1.6046976] [8.64508   1.6216067]\n",
      "[8.64508   1.6216067] [8.619888  1.6432048]\n",
      "[8.619888  1.6432048] [8.598911 1.668374]\n",
      "[9.742226   0.24461742] [9.712021   0.26919192]\n",
      "[9.712021   0.26919192] [9.679383   0.29528093]\n",
      "[9.679383   0.29528093] [9.651745   0.32411492]\n",
      "[9.161057  0.8594865] [9.136532  0.8880452]\n",
      "[9.136532  0.8880452] [9.1099615  0.91592175]\n",
      "[9.1099615  0.91592175] [9.083952   0.94581777]\n",
      "[9.063683  1.2483084] [9.040116  1.2680175]\n",
      "[9.040116  1.2680175] [9.014848  1.2887734]\n",
      "[9.014848  1.2887734] [8.99164   1.3104559]\n",
      "[8.907049  2.5494683] [8.887935 2.549094]\n",
      "[8.887935 2.549094] [8.869294  2.5504987]\n",
      "[8.869294  2.5504987] [8.85148   2.5503378]\n",
      "[9.491496  0.7823144] [9.463338  0.8069466]\n",
      "[9.463338  0.8069466] [9.438205 0.832492]\n",
      "[9.438205 0.832492] [9.410237   0.85490775]\n",
      "[8.458897  2.4588377] [8.436983  2.4698055]\n",
      "[8.436983  2.4698055] [8.417001  2.4767952]\n",
      "[8.417001  2.4767952] [8.395696  2.4852338]\n",
      "[9.544408   0.75475377] [9.515147   0.77928567]\n",
      "[9.515147   0.77928567] [9.490054   0.80482054]\n",
      "[9.490054   0.80482054] [9.461789   0.82671016]\n",
      "[9.048259  1.2024759] [9.025162  1.2228395]\n",
      "[9.025162  1.2228395] [8.999742  1.2448142]\n",
      "[8.999742  1.2448142] [8.976182  1.2677591]\n",
      "[8.985742  1.3532459] [8.9615965 1.3724691]\n",
      "[8.9615965 1.3724691] [8.936587  1.3928448]\n",
      "[8.936587  1.3928448] [8.914649  1.4150579]\n",
      "[8.451564 2.495135] [8.430343 2.505041]\n",
      "[8.430343 2.505041] [8.410715  2.5113628]\n",
      "[8.410715  2.5113628] [8.389623  2.5190725]\n",
      "[9.339395   0.69591004] [9.31251   0.7254199]\n",
      "[9.31251   0.7254199] [9.285555   0.75421804]\n",
      "[9.285555   0.75421804] [9.258167  0.7837759]\n",
      "[8.777244 2.576852] [8.758177  2.5787458]\n",
      "[8.758177  2.5787458] [8.739571 2.580453]\n",
      "[8.739571 2.580453] [8.720949  2.5817256]\n",
      "[9.640496  0.6920266] [9.609327   0.71740943]\n",
      "[9.609327   0.71740943] [9.5840845 0.7429966]\n",
      "[9.5840845 0.7429966] [9.555475  0.7645566]\n",
      "[9.6166935  0.41963658] [9.587056   0.44833785]\n",
      "[9.587056   0.44833785] [9.557555   0.47719133]\n",
      "[9.557555   0.47719133] [9.528721  0.5065229]\n",
      "[9.343325  0.9425546] [9.317952  0.9658565]\n",
      "[9.317952  0.9658565] [9.293109   0.98972416]\n",
      "[9.293109   0.98972416] [9.266402  1.0117552]\n",
      "[8.916765 2.543863] [8.8978    2.5432093]\n",
      "[8.8978    2.5432093] [8.879071  2.5446265]\n",
      "[8.879071  2.5446265] [8.861308  2.5443408]\n",
      "[9.334281  0.6757637] [9.307419  0.7058044]\n",
      "[9.307419  0.7058044] [9.280231  0.7350175]\n",
      "[9.280231  0.7350175] [9.252837   0.76534426]\n",
      "[9.538451  0.7474079] [9.509319  0.7721522]\n",
      "[9.509319  0.7721522] [9.484115  0.7978762]\n",
      "[9.484115  0.7978762] [9.455829   0.82010424]\n",
      "[9.637728  0.3836755] [9.60797    0.41202223]\n",
      "[9.60797    0.41202223] [9.577809 0.440612]\n",
      "[9.577809 0.440612] [9.549082   0.47018886]\n",
      "[1.8157625 9.132864 ] [1.8044385 9.145447 ]\n",
      "[1.8044385 9.145447 ] [1.7899935 9.151762 ]\n",
      "[1.7899935 9.151762 ] [1.771619 9.160048]\n",
      "[8.3183775 2.5076878] [8.296971  2.5180218]\n",
      "[8.296971  2.5180218] [8.277328  2.5252907]\n",
      "[8.277328  2.5252907] [8.255779  2.5342987]\n",
      "[9.696676  0.3284019] [9.666083   0.35553136]\n",
      "[9.666083   0.35553136] [9.634955  0.3831349]\n",
      "[9.634955  0.3831349] [9.606472  0.4122532]\n",
      "[9.258804  1.0569876] [9.234317  1.0788249]\n",
      "[9.234317  1.0788249] [9.209389  1.1009698]\n",
      "[9.209389  1.1009698] [9.183792  1.1222204]\n",
      "[3.266054 7.169911] [3.2455025 7.1885786]\n",
      "[3.2455025 7.1885786] [3.2242408 7.206818 ]\n",
      "[3.2242408 7.206818 ] [3.2108707 7.226562 ]\n",
      "[7.4997234 3.0532339] [7.4824677 3.065642 ]\n",
      "[7.4824677 3.065642 ] [7.462661 3.077524]\n",
      "[7.462661 3.077524] [7.4390287 3.0961895]\n",
      "[2.7773757 9.081761 ] [2.7789838 9.08482  ]\n",
      "[2.7789838 9.08482  ] [2.7798822 9.085446 ]\n",
      "[2.7798822 9.085446 ] [2.7731297 9.083807 ]\n",
      "[8.877091  2.5907505] [8.857642  2.5906339]\n",
      "[8.857642  2.5906339] [8.8395815 2.591637 ]\n",
      "[8.8395815 2.591637 ] [8.821933  2.5914931]\n",
      "[9.217982 1.091199] [9.193775  1.1128005]\n",
      "[9.193775  1.1128005] [9.168646  1.1345663]\n",
      "[9.168646  1.1345663] [9.1435    1.1560174]\n",
      "[9.542074  0.7606155] [9.512846  0.7852532]\n",
      "[9.512846  0.7852532] [9.487783   0.81066096]\n",
      "[9.487783   0.81066096] [9.459659  0.8324802]\n",
      "[8.391589 2.476849] [8.369863  2.4878538]\n",
      "[8.369863  2.4878538] [8.350001  2.4950898]\n",
      "[8.350001  2.4950898] [8.328652  2.5039597]\n",
      "[2.6546576 9.810622 ] [2.6511726 9.799698 ]\n",
      "[2.6511726 9.799698 ] [2.6477416 9.792481 ]\n",
      "[2.6477416 9.792481 ] [2.6440878 9.783706 ]\n",
      "[2.704394 9.869605] [2.6963298 9.85739  ]\n",
      "[2.6963298 9.85739  ] [2.694223 9.850472]\n",
      "[2.694223 9.850472] [2.6932943 9.841081 ]\n",
      "[8.949679  2.3593442] [8.9285965 2.3606262]\n",
      "[8.9285965 2.3606262] [8.906978 2.364981]\n",
      "[8.906978 2.364981] [8.886493  2.3679461]\n",
      "[2.6890314 9.744305 ] [2.6828065 9.735918 ]\n",
      "[2.6828065 9.735918 ] [2.6811814 9.728807 ]\n",
      "[2.6811814 9.728807 ] [2.6798868 9.720451 ]\n",
      "[9.7313175  0.28997222] [9.70041   0.3160726]\n",
      "[9.70041   0.3160726] [9.668592   0.34288323]\n",
      "[9.668592   0.34288323] [9.640433   0.37168327]\n",
      "[8.40888   2.7041461] [8.389435 2.710711]\n",
      "[8.389435 2.710711] [8.371667  2.7143147]\n",
      "[8.371667  2.7143147] [8.351861 2.720385]\n",
      "[6.5401154 4.3317623] [6.5212197 4.3444986]\n",
      "[6.5212197 4.3444986] [6.5036945 4.3620853]\n",
      "[6.5036945 4.3620853] [6.4888453 4.377275 ]\n",
      "[9.763236 0.229993] [9.7328205  0.25443056]\n",
      "[9.7328205  0.25443056] [9.69991    0.28007877]\n",
      "[9.69991    0.28007877] [9.672472  0.3087901]\n",
      "[8.805242  1.4428568] [8.781354  1.4611843]\n",
      "[8.781354  1.4611843] [8.756389  1.4832076]\n",
      "[8.756389  1.4832076] [8.735385  1.5081509]\n",
      "[9.475702  0.8525122] [9.447951 0.876515]\n",
      "[9.447951 0.876515] [9.423328 0.900883]\n",
      "[9.423328 0.900883] [9.395812 0.922048]\n",
      "[9.5005045 0.7759322] [9.472133  0.8005443]\n",
      "[9.472133  0.8005443] [9.446997  0.8261492]\n",
      "[9.446997  0.8261492] [9.418926   0.84853756]\n",
      "[8.887508  2.5533943] [8.868807  2.5528789]\n",
      "[8.868807  2.5528789] [8.850077 2.554226]\n",
      "[8.850077 2.554226] [8.832092  2.5540638]\n",
      "[8.99055   2.3118265] [8.968526  2.3125813]\n",
      "[8.968526  2.3125813] [8.946348  2.3175948]\n",
      "[8.946348  2.3175948] [8.925817 2.321177]\n",
      "[7.990037  2.6626894] [7.9692826 2.6774726]\n",
      "[7.9692826 2.6774726] [7.950968  2.6861544]\n",
      "[7.950968  2.6861544] [7.930588  2.6996589]\n",
      "[8.87239   2.3404505] [8.851379 2.343605]\n",
      "[8.851379 2.343605] [8.829263  2.3487685]\n",
      "[8.829263  2.3487685] [8.807939  2.3531308]\n",
      "[8.23594  2.772741] [8.217421  2.7799127]\n",
      "[8.217421  2.7799127] [8.199569  2.7846103]\n",
      "[8.199569  2.7846103] [8.17918   2.7928698]\n",
      "[8.690076  2.5618918] [8.670541  2.5656767]\n",
      "[8.670541  2.5656767] [8.651696  2.5683603]\n",
      "[8.651696  2.5683603] [8.632107  2.5710733]\n",
      "[8.976719  1.3235275] [8.953156  1.3426086]\n",
      "[8.953156  1.3426086] [8.928082  1.3636605]\n",
      "[8.928082  1.3636605] [8.905802  1.3863316]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8.893552  2.2707283] [8.870853  2.2746403]\n",
      "[8.870853  2.2746403] [8.8479185 2.2811744]\n",
      "[8.8479185 2.2811744] [8.825877  2.2872243]\n",
      "[8.886137  2.5556595] [8.867184  2.5553563]\n",
      "[8.867184  2.5553563] [8.84853   2.5567284]\n",
      "[8.84853   2.5567284] [8.830527 2.556668]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "close_visualize(model,input_batch_padded,target_batch_padded,INPUT_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
