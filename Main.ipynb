{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data.txt','\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_data = data.copy()\n",
    "# normalize X\n",
    "high = data['X'].max()\n",
    "low = data['X'].min()\n",
    "normalized_data['X'] = ((data['X'] - low) / (high - low)) * 10 # within [-0.1,0.1]\n",
    "\n",
    "# normalize Y\n",
    "high = data['Y'].max()\n",
    "low = data['Y'].min()\n",
    "normalized_data['Y'] = ((data['Y'] - low) / (high - low)) * 10 # within [-0.1,0.1]\n",
    "\n",
    "# timestamp / 40\n",
    "normalized_data['timestamp'] = (data['timestamp'] / 40).map(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_data.to_csv('data_normalized.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(128,), (128, 8, 2), (128, 4, 2)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# functions for generate a batch of sample\n",
    "'''\n",
    "    Input:\n",
    "        data: the CSV\n",
    "        num_data: size of batch\n",
    "    Output:\n",
    "        IDs: list of selected IDs\n",
    "        input_sequence: batch with shape (num_data,input_length, 2)\n",
    "        output_sequence: batch with shape (num_data, output_length, 2)\n",
    "'''\n",
    "from random import shuffle\n",
    "def get_batch(data,num_data = 128,input_length = 8, output_length = 4):\n",
    "    # evaluate the total length of series required\n",
    "    total_length = input_length + output_length\n",
    "    # filter out the series that has at least the number of `total_length` long\n",
    "    id_counts = data.groupby('ID').ID.count()\n",
    "    # get a table of candidate id, whose sequence is longer than (or eq. to) total_length\n",
    "    candidate_id_counts = id_counts[id_counts >= total_length]\n",
    "    # get the random sequence...\n",
    "    # the candidate_id_counts is a series with ID as x and count as y\n",
    "    # to get the usable indices, get list series as a list of tuple like (id, count),\n",
    "    # then take the first one (list of id)\n",
    "    # and make it a list, and shuffle on it\n",
    "    random_ids_selected = list(map(lambda tup: tup[0],candidate_id_counts.items()))\n",
    "    shuffle(random_ids_selected)\n",
    "    \n",
    "    selected_ids = []\n",
    "    input_batch = []\n",
    "    target_batch = []\n",
    "    # retrieve the coordinates of the sequence (from the beginning to `total_length`)\n",
    "    for i in random_ids_selected[:num_data]:\n",
    "        selected_ids.append(i)\n",
    "        # select X,Y from ID where ID == i order by timestamp...\n",
    "        sequence_of_i = data[data.ID == i].sort_values(by = \"timestamp\")[[\"X\",\"Y\"]]\n",
    "        # divide the sequence into two parts...\n",
    "        input_sequence = sequence_of_i.iloc[:input_length]\n",
    "        target_sequence = sequence_of_i.iloc[input_length:total_length]\n",
    "        # and append the new sequence to existing arrays\n",
    "        input_batch.append(np.array(input_sequence))\n",
    "        target_batch.append(np.array(target_sequence))\n",
    "    \n",
    "    # return and array of selected ids as well as the batch...\n",
    "    return np.stack(selected_ids), np.stack(input_batch), np.stack(target_batch)\n",
    "    \n",
    "\n",
    "# verify the shape is right...\n",
    "list(map(lambda a: a.shape,get_batch(normalized_data)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''\n",
    "    Visualize the traces in a batch\n",
    "    If batch size = B, sequence length = L...\n",
    "    Input:\n",
    "        batch: batch of sequence of arbitrary length, i.e. array of shape (B,L,2)\n",
    "    Output:\n",
    "        None, a graph will be drawn instead..\n",
    "'''\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "def visualize_trace(batch,target_batch):\n",
    "    # first we make sure that the shape of the batch looks like (_, _, 2)\n",
    "    def check_shape(shape):\n",
    "        if len(shape) != 3:\n",
    "            raise ValueError(\"batch should be in 3 dimension\")\n",
    "        if shape[-1] != 2:\n",
    "            raise ValueError(\"Last axis should be storing X,Y coordinates\")\n",
    "    \n",
    "    check_shape(batch.shape)\n",
    "    check_shape(target_batch.shape)\n",
    "    # sub-routine for draw a particular batch\n",
    "    def draw_batch(batch,linestyle = None):\n",
    "        # now extract the dimension\n",
    "        batch_size, sequence_length, _ = batch.shape\n",
    "        for batch_id in range(batch_size):\n",
    "            # pick a random color for this trace\n",
    "            line_color = np.random.rand(3)\n",
    "            for sequence_pos in range(sequence_length - 1):\n",
    "                # get the two adjacent coordinates\n",
    "                cur_coord = batch[batch_id, sequence_pos]\n",
    "                next_coord = batch[batch_id, sequence_pos + 1]\n",
    "                # and draw the line...\n",
    "                # sneaky plot function requires x-coordinates to be put in the same argument, so are y-coordinates...\n",
    "                plt.plot([cur_coord[0],next_coord[0]],\n",
    "                         [cur_coord[1],next_coord[1]],\n",
    "                         linestyle = linestyle,\n",
    "                         c = line_color)\n",
    "    \n",
    "    draw_batch(batch)\n",
    "    draw_batch(target_batch, \":\")\n",
    "    # finally show the graph\n",
    "#     plt.show()\n",
    "    \n",
    "# let's test this visualization,\n",
    "_, input_batch, target_batch = get_batch(normalized_data,128,16,8)\n",
    "visualize_trace(input_batch,target_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now lets define a vanilla LSTM model\n",
    "'''\n",
    "    According to the paper, there should be an RNN that takes a sequence and gives a sequence (like seq-to-seq)\n",
    "    except this output are hidden layers, like vectors of length 128\n",
    "    To interpret such result, a dense layer with ReLU is added to condense the output to 5 numbers,\n",
    "    namely, the mean_x, mean_y, sxx, syy, and sxy \n",
    "    of the bivariate gaussian of the probability of the agent at that given timestamp.\n",
    "    \n",
    "    The negative log likelihood between the real coordinate and this estimated distribution will be the loss.\n",
    "'''\n",
    "# first, the loss function, in Keras backend\n",
    "from keras.layers import *\n",
    "import keras.backend as K\n",
    "'''\n",
    "    The function takes a series of params of bivariate normal distribution, and a batch of observed coordinates,\n",
    "    and return the log likelikhood of them...\n",
    "    \n",
    "    probability (likelihood) of the observed point (x,y) given the 5 parameters (mx,my,sx,sy,sp):\n",
    "        det(2 * pi * [[sx,sp],[sp,sy]]) ^(-0.5) \n",
    "            * exp(-0.5 * ((x,y) - (mx,my)).T * [[sx,sp],[sp,sy]] * ((x,y) - (mx,my)))\n",
    "    \n",
    "    after taking log and add a minus (* -1)...\n",
    "        -( (-0.5 * log(4 * pi ^ 2 * sx * sy - sp * sp)) + (-0.5 * (...)))\n",
    "    \n",
    "    If batch size = B, sequence length = D...\n",
    "    Input:\n",
    "        Batch bivariate parameters (estimated): K.variable with shape (B,D,5),\n",
    "        Batch of overserved coordinates (label): K.variable with shape (B,D,2)\n",
    "    \n",
    "    Output:\n",
    "        a scaler (K.variable with shape ()), which is the sum of negative log likelihood\n",
    "'''\n",
    "\n",
    "def negative_log_likelihood_loss(batch_observed_coordinates,batch_bivariate_params):\n",
    "    # first check the dimension...\n",
    "    input_shape = K.int_shape(batch_bivariate_params)\n",
    "    target_shape = K.int_shape(batch_observed_coordinates)\n",
    "    \n",
    "    if len(input_shape) != 3 or len(target_shape) != 3:\n",
    "        raise ValueError(\"Dimension of both tensors should be 3\")\n",
    "    \n",
    "    if input_shape[0] != target_shape[0]:\n",
    "        raise ValueError(\"Batch size of both tensors should be the same\")\n",
    "    \n",
    "    if input_shape[1] != target_shape[1]:\n",
    "        raise ValueError(\"Sequence length of both tensors should be the same\")\n",
    "    \n",
    "    if input_shape[2] != 5:\n",
    "        raise ValueError(\"Number of predicted parameters should be 5. Namely, (mx,my,sx,sy,sp)\")\n",
    "    \n",
    "    if target_shape[2] != 2:\n",
    "        raise ValueError(\"Dimension of target coordinates should be 2. Namely, (x,y)\")\n",
    "    \n",
    "    # then split the tensors into (mx,my,sx,sy,sp)...\n",
    "    # all of them should be of shape (B,D)\n",
    "    batch_mx = batch_bivariate_params[:,:,0]\n",
    "    batch_my = batch_bivariate_params[:,:,1]\n",
    "    batch_sx = batch_bivariate_params[:,:,2]\n",
    "    batch_sy = batch_bivariate_params[:,:,3]\n",
    "    batch_sp = batch_bivariate_params[:,:,4]\n",
    "    \n",
    "    batch_x = batch_observed_coordinates[:,:,0]\n",
    "    batch_y = batch_observed_coordinates[:,:,1]\n",
    "    \n",
    "    dx = batch_x - batch_mx # (B,D), (x - mx)\n",
    "    dy = batch_y - batch_my # (B,D), (y - my)\n",
    "    dydx = Multiply()([dx,dy])\n",
    "    \n",
    "    batch_x_change =  K.concatenate([batch_mx[:,0:1], batch_mx[:,1:] - batch_mx[:,:-1]])\n",
    "    batch_y_change =  K.concatenate([batch_my[:,0:1], batch_my[:,1:] - batch_my[:,:-1]])\n",
    "    target_x_change =  K.concatenate([batch_x[:,0:1], batch_x[:,1:] - batch_x[:,:-1]])\n",
    "    target_y_change =  K.concatenate([batch_y[:,0:1], batch_y[:,1:] - batch_y[:,:-1]])\n",
    "    \n",
    "    xy_dot = Multiply()([batch_x_change,target_x_change]) + Multiply()([batch_y_change,target_y_change])\n",
    "    batch_change_norm = K.sqrt(K.square(batch_x_change) + K.square(batch_y_change))\n",
    "    target_change_norm = K.sqrt(K.square(target_x_change) + K.square(target_y_change))\n",
    "    \n",
    "    norm_prod = Multiply()([batch_change_norm,target_change_norm]) + 1e-6\n",
    "    norm_prod_inv = K.pow(norm_prod,-1) # for numerical stability\n",
    "    direction_loss =  - K.mean(Multiply()([xy_dot,norm_prod_inv]))\n",
    "#     det_inv = K.print_tensor(K.pow(det,-1), message=\"det_inv\") # (B,D), (sx * sy - sp^2) ^-1\n",
    "    # (B,D), (dx^2 * sy - 2 * sp * dy * dx + sx * dy^2)\n",
    "    exp = -0.5 * Multiply()([K.square(dx),batch_sy]) - 2 * Multiply()([dydx, batch_sp]) + Multiply()([K.square(dy),batch_sx])\n",
    "    # (B,D), -0.5 * (dx^2 * sy - 2 * sp * dy * dx + sx * dy^2) * det(Cov)^(-1)\n",
    "#     exp = Multiply()([det_inv,exp]) * (-0.5)\n",
    "    \n",
    "    # evaluate the final NLL\n",
    "    '''\n",
    "        A remark here: it is determined that the determininat of the covariance matrix will not be considered as a loss,\n",
    "        as the value of that generally became very large (under the log function)\n",
    "        therefore only the exponents are used as the loss\n",
    "    '''\n",
    "#     batch_nll = - (exp)\n",
    "    batch_nll = K.square(dx) + K.square(dy) - 0.1 * exp + 0.1 * direction_loss\n",
    "    batch_error_total = K.sum(batch_nll, axis = 1) # (B,) sum of NLL in a sequence...\n",
    "    return K.print_tensor(K.mean(batch_error_total, axis = 0)) # (), average of sum of NLL...\n",
    "\n",
    "# now test it...\n",
    "# a regular 0-centered, non-skewing normal\n",
    "[mx,my,sx,sy,sp] = [0,0,.1,.1,0.]\n",
    "bivariate_params = np.array([[[mx,my,sx,sy,sp]]]) # (1,1,5)\n",
    "target_point = np.array([[[-0.,0]]]) # (1,1,2)\n",
    "\n",
    "bivariate_ph = K.variable(value = bivariate_params, dtype = \"float32\")\n",
    "target_ph = K.variable(value = target_point, dtype = \"float32\")\n",
    "nll = negative_log_likelihood_loss(target_ph,bivariate_ph)\n",
    "K.eval(nll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_layer(batch_predicted_coordinates):\n",
    "    return K.cumsum(batch_predicted_coordinates,axis = 1)\n",
    "    \n",
    "def check_tensor(batch_observed_coordinates,batch_predicted_coordinates):\n",
    "    input_shape = K.int_shape(batch_predicted_coordinates)\n",
    "    target_shape = K.int_shape(batch_observed_coordinates)\n",
    "\n",
    "    if len(input_shape) != 3 or len(target_shape) != 3:\n",
    "        raise ValueError(\"Dimension of both tensors should be 3\")\n",
    "\n",
    "    if input_shape[0] != target_shape[0]:\n",
    "        raise ValueError(\"Batch size of both tensors should be the same\")\n",
    "\n",
    "    if input_shape[1] != target_shape[1]:\n",
    "        raise ValueError(\"Sequence length of both tensors should be the same\")\n",
    "\n",
    "    if input_shape[2] != 2:\n",
    "        raise ValueError(\"Number of predicted parameters should be 2. Namely, (mx,my)\")\n",
    "\n",
    "    if target_shape[2] != 2:\n",
    "        raise ValueError(\"Dimension of target coordinates should be 2. Namely, (x,y)\")\n",
    "    \n",
    "def ms_loss(input_tensor):\n",
    "    batch_observed_coordinates,batch_predicted_coordinates = input_tensor\n",
    "    # first check the dimension...\n",
    "    check_tensor(batch_observed_coordinates,batch_predicted_coordinates)\n",
    "    \n",
    "    diff = K.square(batch_predicted_coordinates - batch_observed_coordinates)\n",
    "\n",
    "    return K.sum(diff)\n",
    "\n",
    "def dir_loss(input_tensor):\n",
    "    batch_observed_coordinates,batch_predicted_coordinates = input_tensor\n",
    "    check_tensor(batch_observed_coordinates,batch_predicted_coordinates)\n",
    "    \n",
    "    predict_dir = batch_predicted_coordinates[:,1:] - batch_predicted_coordinates[:,:-1]\n",
    "    \n",
    "    target_dir = batch_observed_coordinates[:,1:] - batch_observed_coordinates[:,:-1]\n",
    "    \n",
    "#     predic_dir_norm = \n",
    "    # this is to maximize the cosine (therefore angle = 0)\n",
    "    return K.sum(K.square((predict_dir - target_dir)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# then, the Vanilla LSTM\n",
    "from keras.models import Model\n",
    "from keras.initializers import RandomNormal\n",
    "from keras import optimizers\n",
    "def vanilla_lstm_model(num_hidden, input_length, predict_length, lr = 1e-3):\n",
    "# def vanilla_lstm_model(num_hidden,input_length, predict_length, input_tensor, target_tensor):\n",
    "    total_length = input_length + predict_length\n",
    "    # the input\n",
    "    input_sequence = Input(shape = (total_length,2), name = 'input_sequence', dtype = 'float32') # (T, 2)\n",
    "    target_sequence = Input(shape = (total_length,2), name = 'target_sequence', dtype = 'float32') # (T, 2)\n",
    "    lstm = LSTM(num_hidden, return_sequences = True)(input_sequence) # (B,T,num_hidden)\n",
    "#     params = TimeDistributed(Dense(5, activation = 'elu'), name = 'params')(lstm) # (B,T,5)\n",
    "    predicted_coordinates_raw = TimeDistributed(Dense(2, activation = 'elu'), name = 'params')(lstm)\n",
    "    \n",
    "    # retrieve the prediction\n",
    "    extract_target_sequence_layer = Lambda(lambda x: x[:,input_length:,:])\n",
    "    predicted_coordinates_masked = extract_target_sequence_layer(predicted_coordinates_raw)\n",
    "    target_coordinates_masked = extract_target_sequence_layer(target_sequence)\n",
    "    # the output layer\n",
    "    predicted_output = Lambda(infer_layer, name = \"predict\")(predicted_coordinates_masked)\n",
    "    # compute the loss\n",
    "    \n",
    "    # first part: the square loss\n",
    "    sq_loss = Lambda(ms_loss, name = 'square_loss')([target_coordinates_masked, predicted_output])\n",
    "    # second part: the direction loss\n",
    "    ori_loss = Lambda(dir_loss, name = 'dir_loss')([target_coordinates_masked, predicted_output])\n",
    "    \n",
    "    loss = Lambda(lambda ts: ts[0] + ts[1],name = 'loss')([sq_loss, ori_loss])\n",
    "    \n",
    "    model = Model(\n",
    "        inputs = [input_sequence,target_sequence],\n",
    "        outputs = [predicted_output,loss])\n",
    "    \n",
    "    model.compile(optimizer=optimizers.RMSprop(lr = lr, clipvalue = 10., decay = 1e-6),\n",
    "                  # since there are two outputs of the model, the estimated params and the NLL,\n",
    "                  # their loss value should be specified.\n",
    "                  # for params there are no loss regarding its value, but I just assign a zero as loss (or the computational graph will break)\n",
    "                  # I made it loss - loss = 0.\n",
    "                  # and for the NLL, return the loss as-is.\n",
    "                  loss= {\n",
    "                        'predict': lambda _, loss: loss - loss, # meh...\n",
    "                          'loss': lambda _, loss: loss\n",
    "                    })\n",
    "    \n",
    "    return input_sequence, target_sequence, model, predicted_output,loss\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# helper function for monitoring training progress\n",
    "from scipy.stats import multivariate_normal\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "'''\n",
    "    Given a set of parameter (array of 5), visualize the heatmap of the bivariate normal distribution\n",
    "'''\n",
    "def draw_heatmap(params):\n",
    "    resolution = 100\n",
    "    interval = 1. / resolution\n",
    "    \n",
    "    x,y = np.mgrid[0:1:interval,0:1:interval]\n",
    "    pos = np.empty(x.shape + (2,))\n",
    "    pos[:,:,0] = x\n",
    "    pos[:,:,1] = y\n",
    "    result_distribution = None\n",
    "    for param in params:\n",
    "        mx,my,sx,sy,sp = param\n",
    "        F = multivariate_normal([mx,my],[[sx,sp],[sp,sy]])\n",
    "        result_distribution = F.pdf(pos) if result_distribution is None else result_distribution + F.pdf(pos)\n",
    "    plt.contourf(x,y,result_distribution)\n",
    "\n",
    "# try it out\n",
    "# fig = plt.figure()\n",
    "draw_heatmap(np.array([[0.3,0.1,0.4,0.2,.2],[0.9,0.2,0.1,0.9,0.]]))\n",
    "visualize_trace(input_batch,target_batch)\n",
    "# draw_heatmap(0.3,0.2,0.01,0.2,0.,fig)\n",
    "plt.show()\n",
    "\n",
    "'''\n",
    "    Draw the mean of the predicted params of all timestamps\n",
    "'''\n",
    "import matplotlib.cm as cm\n",
    "def draw_mean(params):\n",
    "    plt.xlim(0,10)\n",
    "    plt.ylim(0,10)\n",
    "    \n",
    "    \n",
    "    for batch in range(params.shape[0]):\n",
    "        line_color = np.random.rand(3) # choose a color to tell that these scatter points belong to the same prediction\n",
    "        xy_series = params[batch,:,:] # (D,2)\n",
    "        # prgressively change the color to indicate the direction\n",
    "        colors = cm.rainbow(np.linspace(0, 1, params.shape[1]))\n",
    "        plt.scatter(xy_series[:,0],xy_series[:,1], c = colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ReduceLROnPlateau, CSVLogger, LambdaCallback\n",
    "def get_callbacks(input_batch_padded,target_batch_padded,finetune = False):\n",
    "    # prepare callbacks\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='loss', \n",
    "                                  factor=0.5,\n",
    "                                  patience=50, \n",
    "                                  min_lr=1e-6)\n",
    "    csv_logger = CSVLogger(\"log.csv\")\n",
    "\n",
    "    def visualize_prediction(epoch, logs):\n",
    "        params, loss = model.predict([input_batch_padded,target_batch_padded])\n",
    "        # visualize the trace, as well as the distributions generated by the params...\n",
    "        # first clear the previous drawing...\n",
    "    #     try:\n",
    "        plt.gcf().clear()\n",
    "        visualize_trace(input_batch,target_batch)\n",
    "    #     params = params[:,INPUT_LENGTH + 1, :] # (B,5), and it should be the params immediately after the input\n",
    "    #     draw_heatmap(params)\n",
    "        draw_mean(params)\n",
    "        filename = '{}.png' if not finetune else '{}-finetune.png'\n",
    "        plt.savefig(filename.format(epoch))\n",
    "\n",
    "    plot_callback = LambdaCallback(on_epoch_begin = visualize_prediction)\n",
    "    \n",
    "    return [reduce_lr, \n",
    "            csv_logger,\n",
    "           # plot_callback\n",
    "           ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_sequence (InputLayer)     (None, 16, 2)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_10 (LSTM)                  (None, 16, 128)      67072       input_sequence[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "params (TimeDistributed)        (None, 16, 2)        258         lstm_10[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_10 (Lambda)              (None, 8, 2)         0           params[0][0]                     \n",
      "                                                                 target_sequence[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "target_sequence (InputLayer)    (None, 16, 2)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "predict (Lambda)                (None, 8, 2)         0           lambda_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "square_loss (Lambda)            ()                   0           lambda_10[1][0]                  \n",
      "                                                                 predict[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dir_loss (Lambda)               ()                   0           lambda_10[1][0]                  \n",
      "                                                                 predict[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "loss (Lambda)                   ()                   0           square_loss[0][0]                \n",
      "                                                                 dir_loss[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 67,330\n",
      "Trainable params: 67,330\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Train on 204 samples, validate on 52 samples\n",
      "Epoch 1/1500\n",
      "204/204 [==============================] - 1s 4ms/step - loss: 7690.4652 - predict_loss: 0.0000e+00 - loss_loss: 7690.4652 - val_loss: 3259.8879 - val_predict_loss: 0.0000e+00 - val_loss_loss: 3259.8879\n",
      "Epoch 2/1500\n",
      "204/204 [==============================] - 0s 794us/step - loss: 3434.7352 - predict_loss: 0.0000e+00 - loss_loss: 3434.7352 - val_loss: 2476.6252 - val_predict_loss: 0.0000e+00 - val_loss_loss: 2476.6252\n",
      "Epoch 3/1500\n",
      "204/204 [==============================] - 0s 848us/step - loss: 2700.1505 - predict_loss: 0.0000e+00 - loss_loss: 2700.1505 - val_loss: 2046.4436 - val_predict_loss: 0.0000e+00 - val_loss_loss: 2046.4436\n",
      "Epoch 4/1500\n",
      "204/204 [==============================] - 0s 746us/step - loss: 2366.3474 - predict_loss: 0.0000e+00 - loss_loss: 2366.3474 - val_loss: 1785.0348 - val_predict_loss: 0.0000e+00 - val_loss_loss: 1785.0348\n",
      "Epoch 5/1500\n",
      "204/204 [==============================] - 0s 729us/step - loss: 2116.5753 - predict_loss: 0.0000e+00 - loss_loss: 2116.5753 - val_loss: 1693.5482 - val_predict_loss: 0.0000e+00 - val_loss_loss: 1693.5482\n",
      "Epoch 6/1500\n",
      "204/204 [==============================] - 0s 681us/step - loss: 1954.7774 - predict_loss: 0.0000e+00 - loss_loss: 1954.7774 - val_loss: 1508.8852 - val_predict_loss: 0.0000e+00 - val_loss_loss: 1508.8852\n",
      "Epoch 7/1500\n",
      "204/204 [==============================] - 0s 696us/step - loss: 1799.5108 - predict_loss: 0.0000e+00 - loss_loss: 1799.5108 - val_loss: 1477.6425 - val_predict_loss: 0.0000e+00 - val_loss_loss: 1477.6425\n",
      "Epoch 8/1500\n",
      "204/204 [==============================] - 0s 653us/step - loss: 1693.6621 - predict_loss: 0.0000e+00 - loss_loss: 1693.6621 - val_loss: 1277.8147 - val_predict_loss: 0.0000e+00 - val_loss_loss: 1277.8147\n",
      "Epoch 9/1500\n",
      "204/204 [==============================] - 0s 638us/step - loss: 1584.0533 - predict_loss: 0.0000e+00 - loss_loss: 1584.0533 - val_loss: 1335.8468 - val_predict_loss: 0.0000e+00 - val_loss_loss: 1335.8468\n",
      "Epoch 10/1500\n",
      "204/204 [==============================] - 0s 697us/step - loss: 1501.9967 - predict_loss: 0.0000e+00 - loss_loss: 1501.9967 - val_loss: 1110.7561 - val_predict_loss: 0.0000e+00 - val_loss_loss: 1110.7561\n",
      "Epoch 11/1500\n",
      "204/204 [==============================] - 0s 677us/step - loss: 1441.2156 - predict_loss: 0.0000e+00 - loss_loss: 1441.2156 - val_loss: 1174.3122 - val_predict_loss: 0.0000e+00 - val_loss_loss: 1174.3122\n",
      "Epoch 12/1500\n",
      "204/204 [==============================] - 0s 733us/step - loss: 1358.5823 - predict_loss: 0.0000e+00 - loss_loss: 1358.5823 - val_loss: 1019.5504 - val_predict_loss: 0.0000e+00 - val_loss_loss: 1019.5504\n",
      "Epoch 13/1500\n",
      "204/204 [==============================] - 0s 733us/step - loss: 1299.0785 - predict_loss: 0.0000e+00 - loss_loss: 1299.0785 - val_loss: 1112.7010 - val_predict_loss: 0.0000e+00 - val_loss_loss: 1112.7010\n",
      "Epoch 14/1500\n",
      "204/204 [==============================] - 0s 729us/step - loss: 1258.8451 - predict_loss: 0.0000e+00 - loss_loss: 1258.8451 - val_loss: 903.2674 - val_predict_loss: 0.0000e+00 - val_loss_loss: 903.2674\n",
      "Epoch 15/1500\n",
      "204/204 [==============================] - 0s 698us/step - loss: 1198.8788 - predict_loss: 0.0000e+00 - loss_loss: 1198.8788 - val_loss: 984.0407 - val_predict_loss: 0.0000e+00 - val_loss_loss: 984.0407\n",
      "Epoch 16/1500\n",
      "204/204 [==============================] - 0s 673us/step - loss: 1126.3025 - predict_loss: 0.0000e+00 - loss_loss: 1126.3025 - val_loss: 757.4866 - val_predict_loss: 0.0000e+00 - val_loss_loss: 757.4866\n",
      "Epoch 17/1500\n",
      "204/204 [==============================] - 0s 686us/step - loss: 1120.7888 - predict_loss: 0.0000e+00 - loss_loss: 1120.7888 - val_loss: 958.4184 - val_predict_loss: 0.0000e+00 - val_loss_loss: 958.4184\n",
      "Epoch 18/1500\n",
      "204/204 [==============================] - 0s 690us/step - loss: 1045.4435 - predict_loss: 0.0000e+00 - loss_loss: 1045.4435 - val_loss: 753.0675 - val_predict_loss: 0.0000e+00 - val_loss_loss: 753.0675\n",
      "Epoch 19/1500\n",
      "204/204 [==============================] - 0s 806us/step - loss: 958.6388 - predict_loss: 0.0000e+00 - loss_loss: 958.6388 - val_loss: 670.3886 - val_predict_loss: 0.0000e+00 - val_loss_loss: 670.3886\n",
      "Epoch 20/1500\n",
      "204/204 [==============================] - 0s 696us/step - loss: 966.0421 - predict_loss: 0.0000e+00 - loss_loss: 966.0421 - val_loss: 857.8559 - val_predict_loss: 0.0000e+00 - val_loss_loss: 857.8559\n",
      "Epoch 21/1500\n",
      "204/204 [==============================] - 0s 659us/step - loss: 933.6645 - predict_loss: 0.0000e+00 - loss_loss: 933.6645 - val_loss: 594.6391 - val_predict_loss: 0.0000e+00 - val_loss_loss: 594.6391\n",
      "Epoch 22/1500\n",
      "204/204 [==============================] - 0s 662us/step - loss: 875.7440 - predict_loss: 0.0000e+00 - loss_loss: 875.7440 - val_loss: 763.5639 - val_predict_loss: 0.0000e+00 - val_loss_loss: 763.5639\n",
      "Epoch 23/1500\n",
      "204/204 [==============================] - 0s 631us/step - loss: 830.4673 - predict_loss: 0.0000e+00 - loss_loss: 830.4673 - val_loss: 486.8913 - val_predict_loss: 0.0000e+00 - val_loss_loss: 486.8913\n",
      "Epoch 24/1500\n",
      "204/204 [==============================] - 0s 790us/step - loss: 791.5327 - predict_loss: 0.0000e+00 - loss_loss: 791.5327 - val_loss: 736.8948 - val_predict_loss: 0.0000e+00 - val_loss_loss: 736.8948\n",
      "Epoch 25/1500\n",
      "204/204 [==============================] - 0s 833us/step - loss: 769.5203 - predict_loss: 0.0000e+00 - loss_loss: 769.5203 - val_loss: 450.6235 - val_predict_loss: 0.0000e+00 - val_loss_loss: 450.6235\n",
      "Epoch 26/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "204/204 [==============================] - 0s 759us/step - loss: 725.4391 - predict_loss: 0.0000e+00 - loss_loss: 725.4391 - val_loss: 628.0798 - val_predict_loss: 0.0000e+00 - val_loss_loss: 628.0798\n",
      "Epoch 27/1500\n",
      "204/204 [==============================] - 0s 613us/step - loss: 713.2060 - predict_loss: 0.0000e+00 - loss_loss: 713.2060 - val_loss: 410.4204 - val_predict_loss: 0.0000e+00 - val_loss_loss: 410.4204\n",
      "Epoch 28/1500\n",
      "204/204 [==============================] - 0s 638us/step - loss: 668.3244 - predict_loss: 0.0000e+00 - loss_loss: 668.3244 - val_loss: 562.8657 - val_predict_loss: 0.0000e+00 - val_loss_loss: 562.8657\n",
      "Epoch 29/1500\n",
      "204/204 [==============================] - 0s 629us/step - loss: 663.2917 - predict_loss: 0.0000e+00 - loss_loss: 663.2917 - val_loss: 386.3713 - val_predict_loss: 0.0000e+00 - val_loss_loss: 386.3713\n",
      "Epoch 30/1500\n",
      "204/204 [==============================] - 0s 718us/step - loss: 626.4989 - predict_loss: 0.0000e+00 - loss_loss: 626.4989 - val_loss: 523.2723 - val_predict_loss: 0.0000e+00 - val_loss_loss: 523.2723\n",
      "Epoch 31/1500\n",
      "204/204 [==============================] - 0s 732us/step - loss: 600.7079 - predict_loss: 0.0000e+00 - loss_loss: 600.7079 - val_loss: 368.0660 - val_predict_loss: 0.0000e+00 - val_loss_loss: 368.0660\n",
      "Epoch 32/1500\n",
      "204/204 [==============================] - 0s 832us/step - loss: 602.4202 - predict_loss: 0.0000e+00 - loss_loss: 602.4202 - val_loss: 487.8267 - val_predict_loss: 0.0000e+00 - val_loss_loss: 487.8267\n",
      "Epoch 33/1500\n",
      "204/204 [==============================] - 0s 668us/step - loss: 533.7608 - predict_loss: 0.0000e+00 - loss_loss: 533.7608 - val_loss: 345.5189 - val_predict_loss: 0.0000e+00 - val_loss_loss: 345.5189\n",
      "Epoch 34/1500\n",
      "204/204 [==============================] - 0s 596us/step - loss: 487.0611 - predict_loss: 0.0000e+00 - loss_loss: 487.0611 - val_loss: 361.9646 - val_predict_loss: 0.0000e+00 - val_loss_loss: 361.9646\n",
      "Epoch 35/1500\n",
      "204/204 [==============================] - 0s 662us/step - loss: 541.1134 - predict_loss: 0.0000e+00 - loss_loss: 541.1134 - val_loss: 450.0963 - val_predict_loss: 0.0000e+00 - val_loss_loss: 450.0963\n",
      "Epoch 36/1500\n",
      "204/204 [==============================] - 0s 704us/step - loss: 500.9241 - predict_loss: 0.0000e+00 - loss_loss: 500.9241 - val_loss: 261.2958 - val_predict_loss: 0.0000e+00 - val_loss_loss: 261.2958\n",
      "Epoch 37/1500\n",
      "204/204 [==============================] - 0s 815us/step - loss: 472.1532 - predict_loss: 0.0000e+00 - loss_loss: 472.1532 - val_loss: 377.5791 - val_predict_loss: 0.0000e+00 - val_loss_loss: 377.5791\n",
      "Epoch 38/1500\n",
      "204/204 [==============================] - 0s 775us/step - loss: 447.6846 - predict_loss: 0.0000e+00 - loss_loss: 447.6846 - val_loss: 208.8337 - val_predict_loss: 0.0000e+00 - val_loss_loss: 208.8337\n",
      "Epoch 39/1500\n",
      "204/204 [==============================] - 0s 758us/step - loss: 421.8930 - predict_loss: 0.0000e+00 - loss_loss: 421.8930 - val_loss: 323.1780 - val_predict_loss: 0.0000e+00 - val_loss_loss: 323.1780\n",
      "Epoch 40/1500\n",
      "204/204 [==============================] - 0s 695us/step - loss: 427.1159 - predict_loss: 0.0000e+00 - loss_loss: 427.1159 - val_loss: 192.4140 - val_predict_loss: 0.0000e+00 - val_loss_loss: 192.4140\n",
      "Epoch 41/1500\n",
      "204/204 [==============================] - 0s 803us/step - loss: 380.1846 - predict_loss: 0.0000e+00 - loss_loss: 380.1846 - val_loss: 225.2202 - val_predict_loss: 0.0000e+00 - val_loss_loss: 225.2202\n",
      "Epoch 42/1500\n",
      "204/204 [==============================] - 0s 785us/step - loss: 394.8998 - predict_loss: 0.0000e+00 - loss_loss: 394.8998 - val_loss: 299.2022 - val_predict_loss: 0.0000e+00 - val_loss_loss: 299.2022\n",
      "Epoch 43/1500\n",
      "204/204 [==============================] - 0s 694us/step - loss: 380.5399 - predict_loss: 0.0000e+00 - loss_loss: 380.5399 - val_loss: 159.9561 - val_predict_loss: 0.0000e+00 - val_loss_loss: 159.9561\n",
      "Epoch 44/1500\n",
      "204/204 [==============================] - 0s 810us/step - loss: 352.7235 - predict_loss: 0.0000e+00 - loss_loss: 352.7235 - val_loss: 277.0800 - val_predict_loss: 0.0000e+00 - val_loss_loss: 277.0800\n",
      "Epoch 45/1500\n",
      "204/204 [==============================] - 0s 672us/step - loss: 336.2503 - predict_loss: 0.0000e+00 - loss_loss: 336.2503 - val_loss: 120.9374 - val_predict_loss: 0.0000e+00 - val_loss_loss: 120.9374\n",
      "Epoch 46/1500\n",
      "204/204 [==============================] - 0s 611us/step - loss: 332.9134 - predict_loss: 0.0000e+00 - loss_loss: 332.9134 - val_loss: 224.7500 - val_predict_loss: 0.0000e+00 - val_loss_loss: 224.7500\n",
      "Epoch 47/1500\n",
      "204/204 [==============================] - 0s 802us/step - loss: 301.1531 - predict_loss: 0.0000e+00 - loss_loss: 301.1531 - val_loss: 128.7345 - val_predict_loss: 0.0000e+00 - val_loss_loss: 128.7345\n",
      "Epoch 48/1500\n",
      "204/204 [==============================] - 0s 754us/step - loss: 319.3818 - predict_loss: 0.0000e+00 - loss_loss: 319.3818 - val_loss: 189.2473 - val_predict_loss: 0.0000e+00 - val_loss_loss: 189.2473\n",
      "Epoch 49/1500\n",
      "204/204 [==============================] - 0s 659us/step - loss: 280.6292 - predict_loss: 0.0000e+00 - loss_loss: 280.6292 - val_loss: 111.7720 - val_predict_loss: 0.0000e+00 - val_loss_loss: 111.7720\n",
      "Epoch 50/1500\n",
      "204/204 [==============================] - 0s 578us/step - loss: 289.5303 - predict_loss: 0.0000e+00 - loss_loss: 289.5303 - val_loss: 175.8572 - val_predict_loss: 0.0000e+00 - val_loss_loss: 175.8572\n",
      "Epoch 51/1500\n",
      "204/204 [==============================] - 0s 646us/step - loss: 276.1976 - predict_loss: 0.0000e+00 - loss_loss: 276.1976 - val_loss: 99.0033 - val_predict_loss: 0.0000e+00 - val_loss_loss: 99.0033\n",
      "Epoch 52/1500\n",
      "204/204 [==============================] - 0s 638us/step - loss: 255.2472 - predict_loss: 0.0000e+00 - loss_loss: 255.2472 - val_loss: 129.5320 - val_predict_loss: 0.0000e+00 - val_loss_loss: 129.5320\n",
      "Epoch 53/1500\n",
      "204/204 [==============================] - 0s 543us/step - loss: 259.5662 - predict_loss: 0.0000e+00 - loss_loss: 259.5662 - val_loss: 144.1002 - val_predict_loss: 0.0000e+00 - val_loss_loss: 144.1002\n",
      "Epoch 54/1500\n",
      "204/204 [==============================] - 0s 630us/step - loss: 233.3396 - predict_loss: 0.0000e+00 - loss_loss: 233.3396 - val_loss: 81.6393 - val_predict_loss: 0.0000e+00 - val_loss_loss: 81.6393\n",
      "Epoch 55/1500\n",
      "204/204 [==============================] - 0s 529us/step - loss: 215.5089 - predict_loss: 0.0000e+00 - loss_loss: 215.5089 - val_loss: 123.4772 - val_predict_loss: 0.0000e+00 - val_loss_loss: 123.4772\n",
      "Epoch 56/1500\n",
      "204/204 [==============================] - 0s 650us/step - loss: 222.4192 - predict_loss: 0.0000e+00 - loss_loss: 222.4192 - val_loss: 104.1331 - val_predict_loss: 0.0000e+00 - val_loss_loss: 104.1331\n",
      "Epoch 57/1500\n",
      "204/204 [==============================] - 0s 666us/step - loss: 219.7352 - predict_loss: 0.0000e+00 - loss_loss: 219.7352 - val_loss: 71.9642 - val_predict_loss: 0.0000e+00 - val_loss_loss: 71.9642\n",
      "Epoch 58/1500\n",
      "204/204 [==============================] - 0s 680us/step - loss: 198.6986 - predict_loss: 0.0000e+00 - loss_loss: 198.6986 - val_loss: 102.8056 - val_predict_loss: 0.0000e+00 - val_loss_loss: 102.8056\n",
      "Epoch 59/1500\n",
      "204/204 [==============================] - 0s 550us/step - loss: 203.9048 - predict_loss: 0.0000e+00 - loss_loss: 203.9048 - val_loss: 81.5973 - val_predict_loss: 0.0000e+00 - val_loss_loss: 81.5973\n",
      "Epoch 60/1500\n",
      "204/204 [==============================] - 0s 651us/step - loss: 195.6181 - predict_loss: 0.0000e+00 - loss_loss: 195.6181 - val_loss: 60.8159 - val_predict_loss: 0.0000e+00 - val_loss_loss: 60.8159\n",
      "Epoch 61/1500\n",
      "204/204 [==============================] - 0s 595us/step - loss: 175.3782 - predict_loss: 0.0000e+00 - loss_loss: 175.3782 - val_loss: 99.5140 - val_predict_loss: 0.0000e+00 - val_loss_loss: 99.5140\n",
      "Epoch 62/1500\n",
      "204/204 [==============================] - 0s 640us/step - loss: 185.1465 - predict_loss: 0.0000e+00 - loss_loss: 185.1465 - val_loss: 85.5621 - val_predict_loss: 0.0000e+00 - val_loss_loss: 85.5621\n",
      "Epoch 63/1500\n",
      "204/204 [==============================] - 0s 648us/step - loss: 182.8817 - predict_loss: 0.0000e+00 - loss_loss: 182.8817 - val_loss: 68.6137 - val_predict_loss: 0.0000e+00 - val_loss_loss: 68.6137\n",
      "Epoch 64/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "204/204 [==============================] - 0s 770us/step - loss: 157.4170 - predict_loss: 0.0000e+00 - loss_loss: 157.4170 - val_loss: 74.4626 - val_predict_loss: 0.0000e+00 - val_loss_loss: 74.4626\n",
      "Epoch 65/1500\n",
      "204/204 [==============================] - 0s 767us/step - loss: 145.9061 - predict_loss: 0.0000e+00 - loss_loss: 145.9061 - val_loss: 55.3346 - val_predict_loss: 0.0000e+00 - val_loss_loss: 55.3346\n",
      "Epoch 66/1500\n",
      "204/204 [==============================] - 0s 704us/step - loss: 176.1611 - predict_loss: 0.0000e+00 - loss_loss: 176.1611 - val_loss: 60.5655 - val_predict_loss: 0.0000e+00 - val_loss_loss: 60.5655\n",
      "Epoch 67/1500\n",
      "204/204 [==============================] - 0s 612us/step - loss: 150.3481 - predict_loss: 0.0000e+00 - loss_loss: 150.3481 - val_loss: 33.7425 - val_predict_loss: 0.0000e+00 - val_loss_loss: 33.7425\n",
      "Epoch 68/1500\n",
      "204/204 [==============================] - 0s 639us/step - loss: 135.1877 - predict_loss: 0.0000e+00 - loss_loss: 135.1877 - val_loss: 44.5646 - val_predict_loss: 0.0000e+00 - val_loss_loss: 44.5646\n",
      "Epoch 69/1500\n",
      "204/204 [==============================] - 0s 621us/step - loss: 132.8731 - predict_loss: 0.0000e+00 - loss_loss: 132.8731 - val_loss: 70.3396 - val_predict_loss: 0.0000e+00 - val_loss_loss: 70.3396\n",
      "Epoch 70/1500\n",
      "204/204 [==============================] - 0s 635us/step - loss: 147.3274 - predict_loss: 0.0000e+00 - loss_loss: 147.3274 - val_loss: 29.3130 - val_predict_loss: 0.0000e+00 - val_loss_loss: 29.3130\n",
      "Epoch 71/1500\n",
      "204/204 [==============================] - 0s 712us/step - loss: 115.7632 - predict_loss: 0.0000e+00 - loss_loss: 115.7632 - val_loss: 44.8434 - val_predict_loss: 0.0000e+00 - val_loss_loss: 44.8434\n",
      "Epoch 72/1500\n",
      "204/204 [==============================] - 0s 540us/step - loss: 121.7302 - predict_loss: 0.0000e+00 - loss_loss: 121.7302 - val_loss: 42.6310 - val_predict_loss: 0.0000e+00 - val_loss_loss: 42.6310\n",
      "Epoch 73/1500\n",
      "204/204 [==============================] - 0s 650us/step - loss: 125.4768 - predict_loss: 0.0000e+00 - loss_loss: 125.4768 - val_loss: 48.9160 - val_predict_loss: 0.0000e+00 - val_loss_loss: 48.9160\n",
      "Epoch 74/1500\n",
      "204/204 [==============================] - 0s 657us/step - loss: 125.2118 - predict_loss: 0.0000e+00 - loss_loss: 125.2118 - val_loss: 38.3947 - val_predict_loss: 0.0000e+00 - val_loss_loss: 38.3947\n",
      "Epoch 75/1500\n",
      "204/204 [==============================] - 0s 640us/step - loss: 120.8109 - predict_loss: 0.0000e+00 - loss_loss: 120.8109 - val_loss: 32.9720 - val_predict_loss: 0.0000e+00 - val_loss_loss: 32.9720\n",
      "Epoch 76/1500\n",
      "204/204 [==============================] - 0s 563us/step - loss: 102.8346 - predict_loss: 0.0000e+00 - loss_loss: 102.8346 - val_loss: 30.5377 - val_predict_loss: 0.0000e+00 - val_loss_loss: 30.5377\n",
      "Epoch 77/1500\n",
      "204/204 [==============================] - 0s 596us/step - loss: 93.2070 - predict_loss: 0.0000e+00 - loss_loss: 93.2070 - val_loss: 31.2904 - val_predict_loss: 0.0000e+00 - val_loss_loss: 31.2904\n",
      "Epoch 78/1500\n",
      "204/204 [==============================] - 0s 582us/step - loss: 115.9587 - predict_loss: 0.0000e+00 - loss_loss: 115.9587 - val_loss: 29.3844 - val_predict_loss: 0.0000e+00 - val_loss_loss: 29.3844\n",
      "Epoch 79/1500\n",
      "204/204 [==============================] - 0s 666us/step - loss: 97.6640 - predict_loss: 0.0000e+00 - loss_loss: 97.6640 - val_loss: 22.7904 - val_predict_loss: 0.0000e+00 - val_loss_loss: 22.7904\n",
      "Epoch 80/1500\n",
      "204/204 [==============================] - 0s 528us/step - loss: 88.5822 - predict_loss: 0.0000e+00 - loss_loss: 88.5822 - val_loss: 35.2790 - val_predict_loss: 0.0000e+00 - val_loss_loss: 35.2790\n",
      "Epoch 81/1500\n",
      "204/204 [==============================] - 0s 616us/step - loss: 113.8870 - predict_loss: 0.0000e+00 - loss_loss: 113.8870 - val_loss: 27.7053 - val_predict_loss: 0.0000e+00 - val_loss_loss: 27.7053\n",
      "Epoch 82/1500\n",
      "204/204 [==============================] - 0s 622us/step - loss: 86.1369 - predict_loss: 0.0000e+00 - loss_loss: 86.1369 - val_loss: 33.5448 - val_predict_loss: 0.0000e+00 - val_loss_loss: 33.5448\n",
      "Epoch 83/1500\n",
      "204/204 [==============================] - 0s 540us/step - loss: 95.8999 - predict_loss: 0.0000e+00 - loss_loss: 95.8999 - val_loss: 36.7727 - val_predict_loss: 0.0000e+00 - val_loss_loss: 36.7727\n",
      "Epoch 84/1500\n",
      "204/204 [==============================] - 0s 614us/step - loss: 100.2850 - predict_loss: 0.0000e+00 - loss_loss: 100.2850 - val_loss: 34.7854 - val_predict_loss: 0.0000e+00 - val_loss_loss: 34.7854\n",
      "Epoch 85/1500\n",
      "204/204 [==============================] - 0s 591us/step - loss: 91.3673 - predict_loss: 0.0000e+00 - loss_loss: 91.3673 - val_loss: 37.6853 - val_predict_loss: 0.0000e+00 - val_loss_loss: 37.6853\n",
      "Epoch 86/1500\n",
      "204/204 [==============================] - 0s 669us/step - loss: 84.6325 - predict_loss: 0.0000e+00 - loss_loss: 84.6325 - val_loss: 27.0519 - val_predict_loss: 0.0000e+00 - val_loss_loss: 27.0519\n",
      "Epoch 87/1500\n",
      "204/204 [==============================] - 0s 668us/step - loss: 81.5310 - predict_loss: 0.0000e+00 - loss_loss: 81.5310 - val_loss: 20.8409 - val_predict_loss: 0.0000e+00 - val_loss_loss: 20.8409\n",
      "Epoch 88/1500\n",
      "204/204 [==============================] - 0s 628us/step - loss: 82.3918 - predict_loss: 0.0000e+00 - loss_loss: 82.3918 - val_loss: 18.7187 - val_predict_loss: 0.0000e+00 - val_loss_loss: 18.7187\n",
      "Epoch 89/1500\n",
      "204/204 [==============================] - 0s 599us/step - loss: 79.1209 - predict_loss: 0.0000e+00 - loss_loss: 79.1209 - val_loss: 28.6986 - val_predict_loss: 0.0000e+00 - val_loss_loss: 28.6986\n",
      "Epoch 90/1500\n",
      "204/204 [==============================] - 0s 681us/step - loss: 82.3358 - predict_loss: 0.0000e+00 - loss_loss: 82.3358 - val_loss: 35.1332 - val_predict_loss: 0.0000e+00 - val_loss_loss: 35.1332\n",
      "Epoch 91/1500\n",
      "204/204 [==============================] - 0s 643us/step - loss: 90.4531 - predict_loss: 0.0000e+00 - loss_loss: 90.4531 - val_loss: 18.9290 - val_predict_loss: 0.0000e+00 - val_loss_loss: 18.9290\n",
      "Epoch 92/1500\n",
      "204/204 [==============================] - 0s 561us/step - loss: 63.1510 - predict_loss: 0.0000e+00 - loss_loss: 63.1510 - val_loss: 21.5736 - val_predict_loss: 0.0000e+00 - val_loss_loss: 21.5736\n",
      "Epoch 93/1500\n",
      "204/204 [==============================] - 0s 597us/step - loss: 79.0163 - predict_loss: 0.0000e+00 - loss_loss: 79.0163 - val_loss: 17.7278 - val_predict_loss: 0.0000e+00 - val_loss_loss: 17.7278\n",
      "Epoch 94/1500\n",
      "204/204 [==============================] - 0s 647us/step - loss: 69.0016 - predict_loss: 0.0000e+00 - loss_loss: 69.0016 - val_loss: 23.6978 - val_predict_loss: 0.0000e+00 - val_loss_loss: 23.6978\n",
      "Epoch 95/1500\n",
      "204/204 [==============================] - 0s 642us/step - loss: 86.5067 - predict_loss: 0.0000e+00 - loss_loss: 86.5067 - val_loss: 16.1390 - val_predict_loss: 0.0000e+00 - val_loss_loss: 16.1390\n",
      "Epoch 96/1500\n",
      "204/204 [==============================] - 0s 575us/step - loss: 66.4652 - predict_loss: 0.0000e+00 - loss_loss: 66.4652 - val_loss: 21.9608 - val_predict_loss: 0.0000e+00 - val_loss_loss: 21.9608\n",
      "Epoch 97/1500\n",
      "204/204 [==============================] - 0s 638us/step - loss: 71.6550 - predict_loss: 0.0000e+00 - loss_loss: 71.6550 - val_loss: 16.9717 - val_predict_loss: 0.0000e+00 - val_loss_loss: 16.9717\n",
      "Epoch 98/1500\n",
      "204/204 [==============================] - 0s 575us/step - loss: 65.7591 - predict_loss: 0.0000e+00 - loss_loss: 65.7591 - val_loss: 15.3925 - val_predict_loss: 0.0000e+00 - val_loss_loss: 15.3925\n",
      "Epoch 99/1500\n",
      "204/204 [==============================] - 0s 642us/step - loss: 78.3573 - predict_loss: 0.0000e+00 - loss_loss: 78.3573 - val_loss: 14.9272 - val_predict_loss: 0.0000e+00 - val_loss_loss: 14.9272\n",
      "Epoch 100/1500\n",
      "204/204 [==============================] - 0s 584us/step - loss: 63.2105 - predict_loss: 0.0000e+00 - loss_loss: 63.2105 - val_loss: 11.3156 - val_predict_loss: 0.0000e+00 - val_loss_loss: 11.3156\n",
      "Epoch 101/1500\n",
      "204/204 [==============================] - 0s 609us/step - loss: 64.4236 - predict_loss: 0.0000e+00 - loss_loss: 64.4236 - val_loss: 10.1340 - val_predict_loss: 0.0000e+00 - val_loss_loss: 10.1340\n",
      "Epoch 102/1500\n",
      "204/204 [==============================] - 0s 569us/step - loss: 70.5162 - predict_loss: 0.0000e+00 - loss_loss: 70.5162 - val_loss: 21.8649 - val_predict_loss: 0.0000e+00 - val_loss_loss: 21.8649\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 103/1500\n",
      "204/204 [==============================] - 0s 533us/step - loss: 65.4573 - predict_loss: 0.0000e+00 - loss_loss: 65.4573 - val_loss: 18.3728 - val_predict_loss: 0.0000e+00 - val_loss_loss: 18.3728\n",
      "Epoch 104/1500\n",
      "204/204 [==============================] - 0s 553us/step - loss: 61.6312 - predict_loss: 0.0000e+00 - loss_loss: 61.6312 - val_loss: 12.7724 - val_predict_loss: 0.0000e+00 - val_loss_loss: 12.7724\n",
      "Epoch 105/1500\n",
      "204/204 [==============================] - 0s 561us/step - loss: 53.8914 - predict_loss: 0.0000e+00 - loss_loss: 53.8914 - val_loss: 15.9415 - val_predict_loss: 0.0000e+00 - val_loss_loss: 15.9415\n",
      "Epoch 106/1500\n",
      "204/204 [==============================] - 0s 593us/step - loss: 67.4557 - predict_loss: 0.0000e+00 - loss_loss: 67.4557 - val_loss: 12.1935 - val_predict_loss: 0.0000e+00 - val_loss_loss: 12.1935\n",
      "Epoch 107/1500\n",
      "204/204 [==============================] - 0s 620us/step - loss: 52.7111 - predict_loss: 0.0000e+00 - loss_loss: 52.7111 - val_loss: 21.3905 - val_predict_loss: 0.0000e+00 - val_loss_loss: 21.3905\n",
      "Epoch 108/1500\n",
      "204/204 [==============================] - 0s 543us/step - loss: 60.9140 - predict_loss: 0.0000e+00 - loss_loss: 60.9140 - val_loss: 11.7685 - val_predict_loss: 0.0000e+00 - val_loss_loss: 11.7685\n",
      "Epoch 109/1500\n",
      "204/204 [==============================] - 0s 520us/step - loss: 52.7135 - predict_loss: 0.0000e+00 - loss_loss: 52.7135 - val_loss: 16.5912 - val_predict_loss: 0.0000e+00 - val_loss_loss: 16.5912\n",
      "Epoch 110/1500\n",
      "204/204 [==============================] - 0s 514us/step - loss: 50.6942 - predict_loss: 0.0000e+00 - loss_loss: 50.6942 - val_loss: 13.3712 - val_predict_loss: 0.0000e+00 - val_loss_loss: 13.3712\n",
      "Epoch 111/1500\n",
      "204/204 [==============================] - 0s 615us/step - loss: 68.9207 - predict_loss: 0.0000e+00 - loss_loss: 68.9207 - val_loss: 10.8188 - val_predict_loss: 0.0000e+00 - val_loss_loss: 10.8188\n",
      "Epoch 112/1500\n",
      "204/204 [==============================] - 0s 582us/step - loss: 48.7571 - predict_loss: 0.0000e+00 - loss_loss: 48.7571 - val_loss: 8.6499 - val_predict_loss: 0.0000e+00 - val_loss_loss: 8.6499\n",
      "Epoch 113/1500\n",
      "204/204 [==============================] - 0s 667us/step - loss: 45.3600 - predict_loss: 0.0000e+00 - loss_loss: 45.3600 - val_loss: 24.4029 - val_predict_loss: 0.0000e+00 - val_loss_loss: 24.4029\n",
      "Epoch 114/1500\n",
      "204/204 [==============================] - 0s 704us/step - loss: 66.0424 - predict_loss: 0.0000e+00 - loss_loss: 66.0424 - val_loss: 11.9482 - val_predict_loss: 0.0000e+00 - val_loss_loss: 11.9482\n",
      "Epoch 115/1500\n",
      "204/204 [==============================] - 0s 548us/step - loss: 47.8547 - predict_loss: 0.0000e+00 - loss_loss: 47.8547 - val_loss: 15.6077 - val_predict_loss: 0.0000e+00 - val_loss_loss: 15.6077\n",
      "Epoch 116/1500\n",
      "204/204 [==============================] - 0s 622us/step - loss: 59.9919 - predict_loss: 0.0000e+00 - loss_loss: 59.9919 - val_loss: 9.9114 - val_predict_loss: 0.0000e+00 - val_loss_loss: 9.9114\n",
      "Epoch 117/1500\n",
      "204/204 [==============================] - 0s 512us/step - loss: 48.1707 - predict_loss: 0.0000e+00 - loss_loss: 48.1707 - val_loss: 8.8273 - val_predict_loss: 0.0000e+00 - val_loss_loss: 8.8273\n",
      "Epoch 118/1500\n",
      "204/204 [==============================] - 0s 618us/step - loss: 52.9361 - predict_loss: 0.0000e+00 - loss_loss: 52.9361 - val_loss: 11.9002 - val_predict_loss: 0.0000e+00 - val_loss_loss: 11.9002\n",
      "Epoch 119/1500\n",
      "204/204 [==============================] - 0s 675us/step - loss: 44.7806 - predict_loss: 0.0000e+00 - loss_loss: 44.7806 - val_loss: 10.7153 - val_predict_loss: 0.0000e+00 - val_loss_loss: 10.7153\n",
      "Epoch 120/1500\n",
      "204/204 [==============================] - 0s 736us/step - loss: 46.7131 - predict_loss: 0.0000e+00 - loss_loss: 46.7131 - val_loss: 15.4918 - val_predict_loss: 0.0000e+00 - val_loss_loss: 15.4918\n",
      "Epoch 121/1500\n",
      "204/204 [==============================] - 0s 693us/step - loss: 63.0194 - predict_loss: 0.0000e+00 - loss_loss: 63.0194 - val_loss: 11.3680 - val_predict_loss: 0.0000e+00 - val_loss_loss: 11.3680\n",
      "Epoch 122/1500\n",
      "204/204 [==============================] - 0s 661us/step - loss: 42.5385 - predict_loss: 0.0000e+00 - loss_loss: 42.5385 - val_loss: 13.2893 - val_predict_loss: 0.0000e+00 - val_loss_loss: 13.2893\n",
      "Epoch 123/1500\n",
      "204/204 [==============================] - 0s 699us/step - loss: 53.5225 - predict_loss: 0.0000e+00 - loss_loss: 53.5225 - val_loss: 8.8808 - val_predict_loss: 0.0000e+00 - val_loss_loss: 8.8808\n",
      "Epoch 124/1500\n",
      "204/204 [==============================] - 0s 746us/step - loss: 44.0126 - predict_loss: 0.0000e+00 - loss_loss: 44.0126 - val_loss: 16.6302 - val_predict_loss: 0.0000e+00 - val_loss_loss: 16.6302\n",
      "Epoch 125/1500\n",
      "204/204 [==============================] - 0s 678us/step - loss: 44.5824 - predict_loss: 0.0000e+00 - loss_loss: 44.5824 - val_loss: 13.0394 - val_predict_loss: 0.0000e+00 - val_loss_loss: 13.0394\n",
      "Epoch 126/1500\n",
      "204/204 [==============================] - 0s 663us/step - loss: 51.0659 - predict_loss: 0.0000e+00 - loss_loss: 51.0659 - val_loss: 9.4088 - val_predict_loss: 0.0000e+00 - val_loss_loss: 9.4088\n",
      "Epoch 127/1500\n",
      "204/204 [==============================] - 0s 720us/step - loss: 49.2341 - predict_loss: 0.0000e+00 - loss_loss: 49.2341 - val_loss: 10.9915 - val_predict_loss: 0.0000e+00 - val_loss_loss: 10.9915\n",
      "Epoch 128/1500\n",
      "204/204 [==============================] - 0s 579us/step - loss: 46.4916 - predict_loss: 0.0000e+00 - loss_loss: 46.4916 - val_loss: 49.7424 - val_predict_loss: 0.0000e+00 - val_loss_loss: 49.7424\n",
      "Epoch 129/1500\n",
      "204/204 [==============================] - 0s 689us/step - loss: 47.8035 - predict_loss: 0.0000e+00 - loss_loss: 47.8035 - val_loss: 13.3284 - val_predict_loss: 0.0000e+00 - val_loss_loss: 13.3284\n",
      "Epoch 130/1500\n",
      "204/204 [==============================] - 0s 645us/step - loss: 44.1367 - predict_loss: 0.0000e+00 - loss_loss: 44.1367 - val_loss: 12.0609 - val_predict_loss: 0.0000e+00 - val_loss_loss: 12.0609\n",
      "Epoch 131/1500\n",
      "204/204 [==============================] - 0s 622us/step - loss: 40.3335 - predict_loss: 0.0000e+00 - loss_loss: 40.3335 - val_loss: 9.0467 - val_predict_loss: 0.0000e+00 - val_loss_loss: 9.0467\n",
      "Epoch 132/1500\n",
      "204/204 [==============================] - 0s 582us/step - loss: 45.7516 - predict_loss: 0.0000e+00 - loss_loss: 45.7516 - val_loss: 11.5763 - val_predict_loss: 0.0000e+00 - val_loss_loss: 11.5763\n",
      "Epoch 133/1500\n",
      "204/204 [==============================] - 0s 595us/step - loss: 39.6727 - predict_loss: 0.0000e+00 - loss_loss: 39.6727 - val_loss: 8.5024 - val_predict_loss: 0.0000e+00 - val_loss_loss: 8.5024\n",
      "Epoch 134/1500\n",
      "204/204 [==============================] - 0s 658us/step - loss: 47.7034 - predict_loss: 0.0000e+00 - loss_loss: 47.7034 - val_loss: 10.6591 - val_predict_loss: 0.0000e+00 - val_loss_loss: 10.6591\n",
      "Epoch 135/1500\n",
      "204/204 [==============================] - 0s 681us/step - loss: 49.2402 - predict_loss: 0.0000e+00 - loss_loss: 49.2402 - val_loss: 6.5550 - val_predict_loss: 0.0000e+00 - val_loss_loss: 6.5550\n",
      "Epoch 136/1500\n",
      "204/204 [==============================] - 0s 573us/step - loss: 41.4005 - predict_loss: 0.0000e+00 - loss_loss: 41.4005 - val_loss: 9.2948 - val_predict_loss: 0.0000e+00 - val_loss_loss: 9.2948\n",
      "Epoch 137/1500\n",
      "204/204 [==============================] - 0s 634us/step - loss: 33.6732 - predict_loss: 0.0000e+00 - loss_loss: 33.6732 - val_loss: 8.6890 - val_predict_loss: 0.0000e+00 - val_loss_loss: 8.6890\n",
      "Epoch 138/1500\n",
      "204/204 [==============================] - 0s 705us/step - loss: 49.8894 - predict_loss: 0.0000e+00 - loss_loss: 49.8894 - val_loss: 7.0955 - val_predict_loss: 0.0000e+00 - val_loss_loss: 7.0955\n",
      "Epoch 139/1500\n",
      "204/204 [==============================] - 0s 634us/step - loss: 36.5018 - predict_loss: 0.0000e+00 - loss_loss: 36.5018 - val_loss: 22.2887 - val_predict_loss: 0.0000e+00 - val_loss_loss: 22.2887\n",
      "Epoch 140/1500\n",
      "204/204 [==============================] - 0s 667us/step - loss: 45.0463 - predict_loss: 0.0000e+00 - loss_loss: 45.0463 - val_loss: 11.5574 - val_predict_loss: 0.0000e+00 - val_loss_loss: 11.5574\n",
      "Epoch 141/1500\n",
      "204/204 [==============================] - 0s 617us/step - loss: 35.0421 - predict_loss: 0.0000e+00 - loss_loss: 35.0421 - val_loss: 7.9665 - val_predict_loss: 0.0000e+00 - val_loss_loss: 7.9665\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 142/1500\n",
      "204/204 [==============================] - 0s 610us/step - loss: 43.0718 - predict_loss: 0.0000e+00 - loss_loss: 43.0718 - val_loss: 14.2300 - val_predict_loss: 0.0000e+00 - val_loss_loss: 14.2300\n",
      "Epoch 143/1500\n",
      "204/204 [==============================] - 0s 608us/step - loss: 40.2711 - predict_loss: 0.0000e+00 - loss_loss: 40.2711 - val_loss: 9.1002 - val_predict_loss: 0.0000e+00 - val_loss_loss: 9.1002\n",
      "Epoch 144/1500\n",
      "204/204 [==============================] - 0s 642us/step - loss: 38.3358 - predict_loss: 0.0000e+00 - loss_loss: 38.3358 - val_loss: 6.3239 - val_predict_loss: 0.0000e+00 - val_loss_loss: 6.3239\n",
      "Epoch 145/1500\n",
      "204/204 [==============================] - 0s 601us/step - loss: 37.4621 - predict_loss: 0.0000e+00 - loss_loss: 37.4621 - val_loss: 7.8759 - val_predict_loss: 0.0000e+00 - val_loss_loss: 7.8759\n",
      "Epoch 146/1500\n",
      "204/204 [==============================] - 0s 575us/step - loss: 30.6785 - predict_loss: 0.0000e+00 - loss_loss: 30.6785 - val_loss: 8.5707 - val_predict_loss: 0.0000e+00 - val_loss_loss: 8.5707\n",
      "Epoch 147/1500\n",
      "204/204 [==============================] - 0s 544us/step - loss: 50.5994 - predict_loss: 0.0000e+00 - loss_loss: 50.5994 - val_loss: 8.8180 - val_predict_loss: 0.0000e+00 - val_loss_loss: 8.8180\n",
      "Epoch 148/1500\n",
      "204/204 [==============================] - 0s 554us/step - loss: 32.1263 - predict_loss: 0.0000e+00 - loss_loss: 32.1263 - val_loss: 11.5252 - val_predict_loss: 0.0000e+00 - val_loss_loss: 11.5252\n",
      "Epoch 149/1500\n",
      "204/204 [==============================] - 0s 590us/step - loss: 35.9332 - predict_loss: 0.0000e+00 - loss_loss: 35.9332 - val_loss: 9.1759 - val_predict_loss: 0.0000e+00 - val_loss_loss: 9.1759\n",
      "Epoch 150/1500\n",
      "204/204 [==============================] - 0s 599us/step - loss: 39.0823 - predict_loss: 0.0000e+00 - loss_loss: 39.0823 - val_loss: 7.4901 - val_predict_loss: 0.0000e+00 - val_loss_loss: 7.4901\n",
      "Epoch 151/1500\n",
      "204/204 [==============================] - 0s 562us/step - loss: 34.2258 - predict_loss: 0.0000e+00 - loss_loss: 34.2258 - val_loss: 16.3849 - val_predict_loss: 0.0000e+00 - val_loss_loss: 16.3849\n",
      "Epoch 152/1500\n",
      "204/204 [==============================] - 0s 612us/step - loss: 30.3021 - predict_loss: 0.0000e+00 - loss_loss: 30.3021 - val_loss: 11.5146 - val_predict_loss: 0.0000e+00 - val_loss_loss: 11.5146\n",
      "Epoch 153/1500\n",
      "204/204 [==============================] - 0s 593us/step - loss: 41.2025 - predict_loss: 0.0000e+00 - loss_loss: 41.2025 - val_loss: 8.9232 - val_predict_loss: 0.0000e+00 - val_loss_loss: 8.9232\n",
      "Epoch 154/1500\n",
      "204/204 [==============================] - 0s 621us/step - loss: 41.6318 - predict_loss: 0.0000e+00 - loss_loss: 41.6318 - val_loss: 6.4495 - val_predict_loss: 0.0000e+00 - val_loss_loss: 6.4495\n",
      "Epoch 155/1500\n",
      "204/204 [==============================] - 0s 606us/step - loss: 34.0295 - predict_loss: 0.0000e+00 - loss_loss: 34.0295 - val_loss: 7.5034 - val_predict_loss: 0.0000e+00 - val_loss_loss: 7.5034\n",
      "Epoch 156/1500\n",
      "204/204 [==============================] - 0s 605us/step - loss: 26.5725 - predict_loss: 0.0000e+00 - loss_loss: 26.5725 - val_loss: 6.4659 - val_predict_loss: 0.0000e+00 - val_loss_loss: 6.4659\n",
      "Epoch 157/1500\n",
      "204/204 [==============================] - 0s 599us/step - loss: 38.0716 - predict_loss: 0.0000e+00 - loss_loss: 38.0716 - val_loss: 10.0488 - val_predict_loss: 0.0000e+00 - val_loss_loss: 10.0488\n",
      "Epoch 158/1500\n",
      "204/204 [==============================] - 0s 558us/step - loss: 37.7534 - predict_loss: 0.0000e+00 - loss_loss: 37.7534 - val_loss: 6.3566 - val_predict_loss: 0.0000e+00 - val_loss_loss: 6.3566\n",
      "Epoch 159/1500\n",
      "204/204 [==============================] - 0s 585us/step - loss: 32.3446 - predict_loss: 0.0000e+00 - loss_loss: 32.3446 - val_loss: 17.9250 - val_predict_loss: 0.0000e+00 - val_loss_loss: 17.9250\n",
      "Epoch 160/1500\n",
      "204/204 [==============================] - 0s 639us/step - loss: 26.5408 - predict_loss: 0.0000e+00 - loss_loss: 26.5408 - val_loss: 9.9512 - val_predict_loss: 0.0000e+00 - val_loss_loss: 9.9512\n",
      "Epoch 161/1500\n",
      "204/204 [==============================] - 0s 561us/step - loss: 33.5647 - predict_loss: 0.0000e+00 - loss_loss: 33.5647 - val_loss: 11.0351 - val_predict_loss: 0.0000e+00 - val_loss_loss: 11.0351\n",
      "Epoch 162/1500\n",
      "204/204 [==============================] - 0s 612us/step - loss: 40.4258 - predict_loss: 0.0000e+00 - loss_loss: 40.4258 - val_loss: 5.6673 - val_predict_loss: 0.0000e+00 - val_loss_loss: 5.6673\n",
      "Epoch 163/1500\n",
      "204/204 [==============================] - 0s 610us/step - loss: 30.2436 - predict_loss: 0.0000e+00 - loss_loss: 30.2436 - val_loss: 10.7380 - val_predict_loss: 0.0000e+00 - val_loss_loss: 10.7380\n",
      "Epoch 164/1500\n",
      "204/204 [==============================] - 0s 637us/step - loss: 31.4387 - predict_loss: 0.0000e+00 - loss_loss: 31.4387 - val_loss: 8.7403 - val_predict_loss: 0.0000e+00 - val_loss_loss: 8.7403\n",
      "Epoch 165/1500\n",
      "204/204 [==============================] - 0s 604us/step - loss: 27.5329 - predict_loss: 0.0000e+00 - loss_loss: 27.5329 - val_loss: 13.8022 - val_predict_loss: 0.0000e+00 - val_loss_loss: 13.8022\n",
      "Epoch 166/1500\n",
      "204/204 [==============================] - 0s 652us/step - loss: 37.3951 - predict_loss: 0.0000e+00 - loss_loss: 37.3951 - val_loss: 7.9479 - val_predict_loss: 0.0000e+00 - val_loss_loss: 7.9479\n",
      "Epoch 167/1500\n",
      "204/204 [==============================] - 0s 603us/step - loss: 28.7620 - predict_loss: 0.0000e+00 - loss_loss: 28.7620 - val_loss: 6.4792 - val_predict_loss: 0.0000e+00 - val_loss_loss: 6.4792\n",
      "Epoch 168/1500\n",
      "204/204 [==============================] - 0s 571us/step - loss: 37.2954 - predict_loss: 0.0000e+00 - loss_loss: 37.2954 - val_loss: 11.9319 - val_predict_loss: 0.0000e+00 - val_loss_loss: 11.9319\n",
      "Epoch 169/1500\n",
      "204/204 [==============================] - 0s 551us/step - loss: 25.4108 - predict_loss: 0.0000e+00 - loss_loss: 25.4108 - val_loss: 13.9089 - val_predict_loss: 0.0000e+00 - val_loss_loss: 13.9089\n",
      "Epoch 170/1500\n",
      "204/204 [==============================] - 0s 551us/step - loss: 41.3112 - predict_loss: 0.0000e+00 - loss_loss: 41.3112 - val_loss: 8.0360 - val_predict_loss: 0.0000e+00 - val_loss_loss: 8.0360\n",
      "Epoch 171/1500\n",
      "204/204 [==============================] - 0s 555us/step - loss: 27.1294 - predict_loss: 0.0000e+00 - loss_loss: 27.1294 - val_loss: 8.5217 - val_predict_loss: 0.0000e+00 - val_loss_loss: 8.5217\n",
      "Epoch 172/1500\n",
      "204/204 [==============================] - 0s 605us/step - loss: 35.4605 - predict_loss: 0.0000e+00 - loss_loss: 35.4605 - val_loss: 10.9777 - val_predict_loss: 0.0000e+00 - val_loss_loss: 10.9777\n",
      "Epoch 173/1500\n",
      "204/204 [==============================] - 0s 679us/step - loss: 26.7301 - predict_loss: 0.0000e+00 - loss_loss: 26.7301 - val_loss: 9.6189 - val_predict_loss: 0.0000e+00 - val_loss_loss: 9.6189\n",
      "Epoch 174/1500\n",
      "204/204 [==============================] - 0s 590us/step - loss: 29.9436 - predict_loss: 0.0000e+00 - loss_loss: 29.9436 - val_loss: 9.0717 - val_predict_loss: 0.0000e+00 - val_loss_loss: 9.0717\n",
      "Epoch 175/1500\n",
      "204/204 [==============================] - 0s 613us/step - loss: 23.4894 - predict_loss: 0.0000e+00 - loss_loss: 23.4894 - val_loss: 5.8816 - val_predict_loss: 0.0000e+00 - val_loss_loss: 5.8816\n",
      "Epoch 176/1500\n",
      "204/204 [==============================] - 0s 687us/step - loss: 20.0776 - predict_loss: 0.0000e+00 - loss_loss: 20.0776 - val_loss: 5.3010 - val_predict_loss: 0.0000e+00 - val_loss_loss: 5.3010\n",
      "Epoch 177/1500\n",
      "204/204 [==============================] - 0s 592us/step - loss: 22.8423 - predict_loss: 0.0000e+00 - loss_loss: 22.8423 - val_loss: 19.2746 - val_predict_loss: 0.0000e+00 - val_loss_loss: 19.2746\n",
      "Epoch 178/1500\n",
      "204/204 [==============================] - 0s 610us/step - loss: 51.5168 - predict_loss: 0.0000e+00 - loss_loss: 51.5168 - val_loss: 7.8045 - val_predict_loss: 0.0000e+00 - val_loss_loss: 7.8045\n",
      "Epoch 179/1500\n",
      "204/204 [==============================] - 0s 568us/step - loss: 26.1008 - predict_loss: 0.0000e+00 - loss_loss: 26.1008 - val_loss: 8.1553 - val_predict_loss: 0.0000e+00 - val_loss_loss: 8.1553\n",
      "Epoch 180/1500\n",
      "204/204 [==============================] - 0s 575us/step - loss: 22.9345 - predict_loss: 0.0000e+00 - loss_loss: 22.9345 - val_loss: 5.0337 - val_predict_loss: 0.0000e+00 - val_loss_loss: 5.0337\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 181/1500\n",
      "204/204 [==============================] - 0s 593us/step - loss: 27.7643 - predict_loss: 0.0000e+00 - loss_loss: 27.7643 - val_loss: 14.4787 - val_predict_loss: 0.0000e+00 - val_loss_loss: 14.4787\n",
      "Epoch 182/1500\n",
      "204/204 [==============================] - 0s 607us/step - loss: 38.8281 - predict_loss: 0.0000e+00 - loss_loss: 38.8281 - val_loss: 14.3693 - val_predict_loss: 0.0000e+00 - val_loss_loss: 14.3693\n",
      "Epoch 183/1500\n",
      "204/204 [==============================] - 0s 602us/step - loss: 23.1660 - predict_loss: 0.0000e+00 - loss_loss: 23.1660 - val_loss: 6.2135 - val_predict_loss: 0.0000e+00 - val_loss_loss: 6.2135\n",
      "Epoch 184/1500\n",
      "204/204 [==============================] - 0s 594us/step - loss: 28.4225 - predict_loss: 0.0000e+00 - loss_loss: 28.4225 - val_loss: 11.4047 - val_predict_loss: 0.0000e+00 - val_loss_loss: 11.4047\n",
      "Epoch 185/1500\n",
      "204/204 [==============================] - 0s 604us/step - loss: 23.3427 - predict_loss: 0.0000e+00 - loss_loss: 23.3427 - val_loss: 9.2260 - val_predict_loss: 0.0000e+00 - val_loss_loss: 9.2260\n",
      "Epoch 186/1500\n",
      "204/204 [==============================] - 0s 597us/step - loss: 25.9726 - predict_loss: 0.0000e+00 - loss_loss: 25.9726 - val_loss: 7.1001 - val_predict_loss: 0.0000e+00 - val_loss_loss: 7.1001\n",
      "Epoch 187/1500\n",
      "204/204 [==============================] - 0s 650us/step - loss: 32.6727 - predict_loss: 0.0000e+00 - loss_loss: 32.6727 - val_loss: 4.7007 - val_predict_loss: 0.0000e+00 - val_loss_loss: 4.7007\n",
      "Epoch 188/1500\n",
      "204/204 [==============================] - 0s 585us/step - loss: 22.7611 - predict_loss: 0.0000e+00 - loss_loss: 22.7611 - val_loss: 7.6657 - val_predict_loss: 0.0000e+00 - val_loss_loss: 7.6657\n",
      "Epoch 189/1500\n",
      "204/204 [==============================] - 0s 615us/step - loss: 27.2087 - predict_loss: 0.0000e+00 - loss_loss: 27.2087 - val_loss: 9.5509 - val_predict_loss: 0.0000e+00 - val_loss_loss: 9.5509\n",
      "Epoch 190/1500\n",
      "204/204 [==============================] - 0s 601us/step - loss: 29.9882 - predict_loss: 0.0000e+00 - loss_loss: 29.9882 - val_loss: 7.7261 - val_predict_loss: 0.0000e+00 - val_loss_loss: 7.7261\n",
      "Epoch 191/1500\n",
      "204/204 [==============================] - 0s 597us/step - loss: 24.6374 - predict_loss: 0.0000e+00 - loss_loss: 24.6374 - val_loss: 8.3907 - val_predict_loss: 0.0000e+00 - val_loss_loss: 8.3907\n",
      "Epoch 192/1500\n",
      "204/204 [==============================] - 0s 614us/step - loss: 25.3528 - predict_loss: 0.0000e+00 - loss_loss: 25.3528 - val_loss: 6.4282 - val_predict_loss: 0.0000e+00 - val_loss_loss: 6.4282\n",
      "Epoch 193/1500\n",
      "204/204 [==============================] - 0s 610us/step - loss: 22.7689 - predict_loss: 0.0000e+00 - loss_loss: 22.7689 - val_loss: 7.3695 - val_predict_loss: 0.0000e+00 - val_loss_loss: 7.3695\n",
      "Epoch 194/1500\n",
      "204/204 [==============================] - 0s 631us/step - loss: 30.5733 - predict_loss: 0.0000e+00 - loss_loss: 30.5733 - val_loss: 7.0993 - val_predict_loss: 0.0000e+00 - val_loss_loss: 7.0993\n",
      "Epoch 195/1500\n",
      "204/204 [==============================] - 0s 584us/step - loss: 28.3297 - predict_loss: 0.0000e+00 - loss_loss: 28.3297 - val_loss: 5.0031 - val_predict_loss: 0.0000e+00 - val_loss_loss: 5.0031\n",
      "Epoch 196/1500\n",
      "204/204 [==============================] - 0s 634us/step - loss: 21.6707 - predict_loss: 0.0000e+00 - loss_loss: 21.6707 - val_loss: 14.6168 - val_predict_loss: 0.0000e+00 - val_loss_loss: 14.6168\n",
      "Epoch 197/1500\n",
      "204/204 [==============================] - 0s 574us/step - loss: 29.5455 - predict_loss: 0.0000e+00 - loss_loss: 29.5455 - val_loss: 11.4821 - val_predict_loss: 0.0000e+00 - val_loss_loss: 11.4821\n",
      "Epoch 198/1500\n",
      "204/204 [==============================] - 0s 589us/step - loss: 22.4485 - predict_loss: 0.0000e+00 - loss_loss: 22.4485 - val_loss: 7.5615 - val_predict_loss: 0.0000e+00 - val_loss_loss: 7.5615\n",
      "Epoch 199/1500\n",
      "204/204 [==============================] - 0s 594us/step - loss: 21.8198 - predict_loss: 0.0000e+00 - loss_loss: 21.8198 - val_loss: 9.6962 - val_predict_loss: 0.0000e+00 - val_loss_loss: 9.6962\n",
      "Epoch 200/1500\n",
      "204/204 [==============================] - 0s 593us/step - loss: 21.7089 - predict_loss: 0.0000e+00 - loss_loss: 21.7089 - val_loss: 7.1166 - val_predict_loss: 0.0000e+00 - val_loss_loss: 7.1166\n",
      "Epoch 201/1500\n",
      "204/204 [==============================] - 0s 536us/step - loss: 38.2756 - predict_loss: 0.0000e+00 - loss_loss: 38.2756 - val_loss: 5.3998 - val_predict_loss: 0.0000e+00 - val_loss_loss: 5.3998\n",
      "Epoch 202/1500\n",
      "204/204 [==============================] - 0s 596us/step - loss: 21.0867 - predict_loss: 0.0000e+00 - loss_loss: 21.0867 - val_loss: 6.0210 - val_predict_loss: 0.0000e+00 - val_loss_loss: 6.0210\n",
      "Epoch 203/1500\n",
      "204/204 [==============================] - 0s 617us/step - loss: 23.3526 - predict_loss: 0.0000e+00 - loss_loss: 23.3526 - val_loss: 8.0665 - val_predict_loss: 0.0000e+00 - val_loss_loss: 8.0665\n",
      "Epoch 204/1500\n",
      "204/204 [==============================] - 0s 647us/step - loss: 26.8604 - predict_loss: 0.0000e+00 - loss_loss: 26.8604 - val_loss: 4.4988 - val_predict_loss: 0.0000e+00 - val_loss_loss: 4.4988\n",
      "Epoch 205/1500\n",
      "204/204 [==============================] - 0s 638us/step - loss: 18.8655 - predict_loss: 0.0000e+00 - loss_loss: 18.8655 - val_loss: 4.8106 - val_predict_loss: 0.0000e+00 - val_loss_loss: 4.8106\n",
      "Epoch 206/1500\n",
      "204/204 [==============================] - 0s 647us/step - loss: 37.4696 - predict_loss: 0.0000e+00 - loss_loss: 37.4696 - val_loss: 7.9864 - val_predict_loss: 0.0000e+00 - val_loss_loss: 7.9864\n",
      "Epoch 207/1500\n",
      "204/204 [==============================] - 0s 628us/step - loss: 21.3002 - predict_loss: 0.0000e+00 - loss_loss: 21.3002 - val_loss: 5.8905 - val_predict_loss: 0.0000e+00 - val_loss_loss: 5.8905\n",
      "Epoch 208/1500\n",
      "204/204 [==============================] - 0s 624us/step - loss: 21.3456 - predict_loss: 0.0000e+00 - loss_loss: 21.3456 - val_loss: 5.9734 - val_predict_loss: 0.0000e+00 - val_loss_loss: 5.9734\n",
      "Epoch 209/1500\n",
      "204/204 [==============================] - 0s 619us/step - loss: 22.5446 - predict_loss: 0.0000e+00 - loss_loss: 22.5446 - val_loss: 10.0350 - val_predict_loss: 0.0000e+00 - val_loss_loss: 10.0350\n",
      "Epoch 210/1500\n",
      "204/204 [==============================] - 0s 557us/step - loss: 39.0931 - predict_loss: 0.0000e+00 - loss_loss: 39.0931 - val_loss: 6.7407 - val_predict_loss: 0.0000e+00 - val_loss_loss: 6.7407\n",
      "Epoch 211/1500\n",
      "204/204 [==============================] - 0s 587us/step - loss: 22.2571 - predict_loss: 0.0000e+00 - loss_loss: 22.2571 - val_loss: 4.6060 - val_predict_loss: 0.0000e+00 - val_loss_loss: 4.6060\n",
      "Epoch 212/1500\n",
      "204/204 [==============================] - 0s 640us/step - loss: 19.0922 - predict_loss: 0.0000e+00 - loss_loss: 19.0922 - val_loss: 6.2843 - val_predict_loss: 0.0000e+00 - val_loss_loss: 6.2843\n",
      "Epoch 213/1500\n",
      "204/204 [==============================] - 0s 596us/step - loss: 31.5725 - predict_loss: 0.0000e+00 - loss_loss: 31.5725 - val_loss: 5.8827 - val_predict_loss: 0.0000e+00 - val_loss_loss: 5.8827\n",
      "Epoch 214/1500\n",
      "204/204 [==============================] - 0s 635us/step - loss: 27.3446 - predict_loss: 0.0000e+00 - loss_loss: 27.3446 - val_loss: 4.3794 - val_predict_loss: 0.0000e+00 - val_loss_loss: 4.3794\n",
      "Epoch 215/1500\n",
      "204/204 [==============================] - 0s 663us/step - loss: 24.5443 - predict_loss: 0.0000e+00 - loss_loss: 24.5443 - val_loss: 5.2968 - val_predict_loss: 0.0000e+00 - val_loss_loss: 5.2968\n",
      "Epoch 216/1500\n",
      "204/204 [==============================] - 0s 602us/step - loss: 21.8516 - predict_loss: 0.0000e+00 - loss_loss: 21.8516 - val_loss: 12.9926 - val_predict_loss: 0.0000e+00 - val_loss_loss: 12.9926\n",
      "Epoch 217/1500\n",
      "204/204 [==============================] - 0s 601us/step - loss: 27.0776 - predict_loss: 0.0000e+00 - loss_loss: 27.0776 - val_loss: 5.8103 - val_predict_loss: 0.0000e+00 - val_loss_loss: 5.8103\n",
      "Epoch 218/1500\n",
      "204/204 [==============================] - 0s 640us/step - loss: 26.0874 - predict_loss: 0.0000e+00 - loss_loss: 26.0874 - val_loss: 7.1814 - val_predict_loss: 0.0000e+00 - val_loss_loss: 7.1814\n",
      "Epoch 219/1500\n",
      "204/204 [==============================] - 0s 598us/step - loss: 24.2926 - predict_loss: 0.0000e+00 - loss_loss: 24.2926 - val_loss: 5.4677 - val_predict_loss: 0.0000e+00 - val_loss_loss: 5.4677\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 220/1500\n",
      "204/204 [==============================] - 0s 668us/step - loss: 23.0730 - predict_loss: 0.0000e+00 - loss_loss: 23.0730 - val_loss: 6.2328 - val_predict_loss: 0.0000e+00 - val_loss_loss: 6.2328\n",
      "Epoch 221/1500\n",
      "204/204 [==============================] - 0s 500us/step - loss: 27.7890 - predict_loss: 0.0000e+00 - loss_loss: 27.7890 - val_loss: 6.7051 - val_predict_loss: 0.0000e+00 - val_loss_loss: 6.7051\n",
      "Epoch 222/1500\n",
      "204/204 [==============================] - 0s 570us/step - loss: 18.6299 - predict_loss: 0.0000e+00 - loss_loss: 18.6299 - val_loss: 8.1647 - val_predict_loss: 0.0000e+00 - val_loss_loss: 8.1647\n",
      "Epoch 223/1500\n",
      "204/204 [==============================] - 0s 633us/step - loss: 27.1161 - predict_loss: 0.0000e+00 - loss_loss: 27.1161 - val_loss: 5.8408 - val_predict_loss: 0.0000e+00 - val_loss_loss: 5.8408\n",
      "Epoch 224/1500\n",
      "204/204 [==============================] - 0s 644us/step - loss: 23.1960 - predict_loss: 0.0000e+00 - loss_loss: 23.1960 - val_loss: 8.4993 - val_predict_loss: 0.0000e+00 - val_loss_loss: 8.4993\n",
      "Epoch 225/1500\n",
      "204/204 [==============================] - 0s 618us/step - loss: 17.8558 - predict_loss: 0.0000e+00 - loss_loss: 17.8558 - val_loss: 11.0752 - val_predict_loss: 0.0000e+00 - val_loss_loss: 11.0752\n",
      "Epoch 226/1500\n",
      "204/204 [==============================] - 0s 595us/step - loss: 28.3005 - predict_loss: 0.0000e+00 - loss_loss: 28.3005 - val_loss: 7.5893 - val_predict_loss: 0.0000e+00 - val_loss_loss: 7.5893\n",
      "Epoch 227/1500\n",
      "204/204 [==============================] - 0s 596us/step - loss: 29.5870 - predict_loss: 0.0000e+00 - loss_loss: 29.5870 - val_loss: 6.8589 - val_predict_loss: 0.0000e+00 - val_loss_loss: 6.8589\n",
      "Epoch 228/1500\n",
      "204/204 [==============================] - 0s 571us/step - loss: 18.6715 - predict_loss: 0.0000e+00 - loss_loss: 18.6715 - val_loss: 8.0996 - val_predict_loss: 0.0000e+00 - val_loss_loss: 8.0996\n",
      "Epoch 229/1500\n",
      "204/204 [==============================] - 0s 563us/step - loss: 30.7666 - predict_loss: 0.0000e+00 - loss_loss: 30.7666 - val_loss: 4.8670 - val_predict_loss: 0.0000e+00 - val_loss_loss: 4.8670\n",
      "Epoch 230/1500\n",
      "204/204 [==============================] - 0s 569us/step - loss: 20.0255 - predict_loss: 0.0000e+00 - loss_loss: 20.0255 - val_loss: 5.2001 - val_predict_loss: 0.0000e+00 - val_loss_loss: 5.2001\n",
      "Epoch 231/1500\n",
      "204/204 [==============================] - 0s 528us/step - loss: 24.7861 - predict_loss: 0.0000e+00 - loss_loss: 24.7861 - val_loss: 12.3317 - val_predict_loss: 0.0000e+00 - val_loss_loss: 12.3317\n",
      "Epoch 232/1500\n",
      "204/204 [==============================] - 0s 561us/step - loss: 22.6907 - predict_loss: 0.0000e+00 - loss_loss: 22.6907 - val_loss: 5.7718 - val_predict_loss: 0.0000e+00 - val_loss_loss: 5.7718\n",
      "Epoch 233/1500\n",
      "204/204 [==============================] - 0s 551us/step - loss: 30.5954 - predict_loss: 0.0000e+00 - loss_loss: 30.5954 - val_loss: 5.3237 - val_predict_loss: 0.0000e+00 - val_loss_loss: 5.3237\n",
      "Epoch 234/1500\n",
      "204/204 [==============================] - 0s 626us/step - loss: 20.3968 - predict_loss: 0.0000e+00 - loss_loss: 20.3968 - val_loss: 6.1882 - val_predict_loss: 0.0000e+00 - val_loss_loss: 6.1882\n",
      "Epoch 235/1500\n",
      "204/204 [==============================] - 0s 628us/step - loss: 27.9671 - predict_loss: 0.0000e+00 - loss_loss: 27.9671 - val_loss: 7.5926 - val_predict_loss: 0.0000e+00 - val_loss_loss: 7.5926\n",
      "Epoch 236/1500\n",
      "204/204 [==============================] - 0s 637us/step - loss: 16.3324 - predict_loss: 0.0000e+00 - loss_loss: 16.3324 - val_loss: 5.6059 - val_predict_loss: 0.0000e+00 - val_loss_loss: 5.6059\n",
      "Epoch 237/1500\n",
      "204/204 [==============================] - 0s 616us/step - loss: 24.7072 - predict_loss: 0.0000e+00 - loss_loss: 24.7072 - val_loss: 7.4289 - val_predict_loss: 0.0000e+00 - val_loss_loss: 7.4289\n",
      "Epoch 238/1500\n",
      "204/204 [==============================] - 0s 598us/step - loss: 28.0672 - predict_loss: 0.0000e+00 - loss_loss: 28.0672 - val_loss: 7.5099 - val_predict_loss: 0.0000e+00 - val_loss_loss: 7.5099\n",
      "Epoch 239/1500\n",
      "204/204 [==============================] - 0s 595us/step - loss: 17.1698 - predict_loss: 0.0000e+00 - loss_loss: 17.1698 - val_loss: 4.9693 - val_predict_loss: 0.0000e+00 - val_loss_loss: 4.9693\n",
      "Epoch 240/1500\n",
      "204/204 [==============================] - 0s 603us/step - loss: 24.0443 - predict_loss: 0.0000e+00 - loss_loss: 24.0443 - val_loss: 7.1317 - val_predict_loss: 0.0000e+00 - val_loss_loss: 7.1317\n",
      "Epoch 241/1500\n",
      "204/204 [==============================] - 0s 652us/step - loss: 22.8296 - predict_loss: 0.0000e+00 - loss_loss: 22.8296 - val_loss: 5.1435 - val_predict_loss: 0.0000e+00 - val_loss_loss: 5.1435\n",
      "Epoch 242/1500\n",
      "204/204 [==============================] - 0s 620us/step - loss: 28.5684 - predict_loss: 0.0000e+00 - loss_loss: 28.5684 - val_loss: 9.2979 - val_predict_loss: 0.0000e+00 - val_loss_loss: 9.2979\n",
      "Epoch 243/1500\n",
      "204/204 [==============================] - 0s 624us/step - loss: 19.1105 - predict_loss: 0.0000e+00 - loss_loss: 19.1105 - val_loss: 6.4897 - val_predict_loss: 0.0000e+00 - val_loss_loss: 6.4897\n",
      "Epoch 244/1500\n",
      "204/204 [==============================] - 0s 624us/step - loss: 19.8208 - predict_loss: 0.0000e+00 - loss_loss: 19.8208 - val_loss: 6.7006 - val_predict_loss: 0.0000e+00 - val_loss_loss: 6.7006\n",
      "Epoch 245/1500\n",
      "204/204 [==============================] - 0s 649us/step - loss: 23.2015 - predict_loss: 0.0000e+00 - loss_loss: 23.2015 - val_loss: 4.7995 - val_predict_loss: 0.0000e+00 - val_loss_loss: 4.7995\n",
      "Epoch 246/1500\n",
      "204/204 [==============================] - 0s 626us/step - loss: 23.7435 - predict_loss: 0.0000e+00 - loss_loss: 23.7435 - val_loss: 4.0791 - val_predict_loss: 0.0000e+00 - val_loss_loss: 4.0791\n",
      "Epoch 247/1500\n",
      "204/204 [==============================] - 0s 640us/step - loss: 39.0606 - predict_loss: 0.0000e+00 - loss_loss: 39.0606 - val_loss: 7.1612 - val_predict_loss: 0.0000e+00 - val_loss_loss: 7.1612\n",
      "Epoch 248/1500\n",
      "204/204 [==============================] - 0s 579us/step - loss: 20.7252 - predict_loss: 0.0000e+00 - loss_loss: 20.7252 - val_loss: 4.7261 - val_predict_loss: 0.0000e+00 - val_loss_loss: 4.7261\n",
      "Epoch 249/1500\n",
      "204/204 [==============================] - 0s 646us/step - loss: 17.3107 - predict_loss: 0.0000e+00 - loss_loss: 17.3107 - val_loss: 5.8514 - val_predict_loss: 0.0000e+00 - val_loss_loss: 5.8514\n",
      "Epoch 250/1500\n",
      "204/204 [==============================] - 0s 647us/step - loss: 28.8390 - predict_loss: 0.0000e+00 - loss_loss: 28.8390 - val_loss: 6.0008 - val_predict_loss: 0.0000e+00 - val_loss_loss: 6.0008\n",
      "Epoch 251/1500\n",
      "204/204 [==============================] - 0s 632us/step - loss: 19.2527 - predict_loss: 0.0000e+00 - loss_loss: 19.2527 - val_loss: 4.8415 - val_predict_loss: 0.0000e+00 - val_loss_loss: 4.8415\n",
      "Epoch 252/1500\n",
      "204/204 [==============================] - 0s 633us/step - loss: 21.1571 - predict_loss: 0.0000e+00 - loss_loss: 21.1571 - val_loss: 7.3079 - val_predict_loss: 0.0000e+00 - val_loss_loss: 7.3079\n",
      "Epoch 253/1500\n",
      "204/204 [==============================] - 0s 615us/step - loss: 18.4396 - predict_loss: 0.0000e+00 - loss_loss: 18.4396 - val_loss: 6.1079 - val_predict_loss: 0.0000e+00 - val_loss_loss: 6.1079\n",
      "Epoch 254/1500\n",
      "204/204 [==============================] - 0s 603us/step - loss: 23.7664 - predict_loss: 0.0000e+00 - loss_loss: 23.7664 - val_loss: 11.0727 - val_predict_loss: 0.0000e+00 - val_loss_loss: 11.0727\n",
      "Epoch 255/1500\n",
      "204/204 [==============================] - 0s 605us/step - loss: 20.4266 - predict_loss: 0.0000e+00 - loss_loss: 20.4266 - val_loss: 5.2316 - val_predict_loss: 0.0000e+00 - val_loss_loss: 5.2316\n",
      "Epoch 256/1500\n",
      "204/204 [==============================] - 0s 624us/step - loss: 28.1933 - predict_loss: 0.0000e+00 - loss_loss: 28.1933 - val_loss: 7.6727 - val_predict_loss: 0.0000e+00 - val_loss_loss: 7.6727\n",
      "Epoch 257/1500\n",
      "204/204 [==============================] - 0s 612us/step - loss: 18.5668 - predict_loss: 0.0000e+00 - loss_loss: 18.5668 - val_loss: 8.6492 - val_predict_loss: 0.0000e+00 - val_loss_loss: 8.6492\n",
      "Epoch 258/1500\n",
      "204/204 [==============================] - 0s 621us/step - loss: 20.4002 - predict_loss: 0.0000e+00 - loss_loss: 20.4002 - val_loss: 5.3655 - val_predict_loss: 0.0000e+00 - val_loss_loss: 5.3655\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 259/1500\n",
      "204/204 [==============================] - 0s 590us/step - loss: 29.7931 - predict_loss: 0.0000e+00 - loss_loss: 29.7931 - val_loss: 5.9951 - val_predict_loss: 0.0000e+00 - val_loss_loss: 5.9951\n",
      "Epoch 260/1500\n",
      "204/204 [==============================] - 0s 620us/step - loss: 16.3345 - predict_loss: 0.0000e+00 - loss_loss: 16.3345 - val_loss: 4.8087 - val_predict_loss: 0.0000e+00 - val_loss_loss: 4.8087\n",
      "Epoch 261/1500\n",
      "204/204 [==============================] - 0s 555us/step - loss: 24.8365 - predict_loss: 0.0000e+00 - loss_loss: 24.8365 - val_loss: 6.4450 - val_predict_loss: 0.0000e+00 - val_loss_loss: 6.4450\n",
      "Epoch 262/1500\n",
      "204/204 [==============================] - 0s 667us/step - loss: 22.0485 - predict_loss: 0.0000e+00 - loss_loss: 22.0485 - val_loss: 5.1379 - val_predict_loss: 0.0000e+00 - val_loss_loss: 5.1379\n",
      "Epoch 263/1500\n",
      "204/204 [==============================] - 0s 577us/step - loss: 18.5093 - predict_loss: 0.0000e+00 - loss_loss: 18.5093 - val_loss: 6.1417 - val_predict_loss: 0.0000e+00 - val_loss_loss: 6.1417\n",
      "Epoch 264/1500\n",
      "204/204 [==============================] - 0s 591us/step - loss: 25.7551 - predict_loss: 0.0000e+00 - loss_loss: 25.7551 - val_loss: 3.4824 - val_predict_loss: 0.0000e+00 - val_loss_loss: 3.4824\n",
      "Epoch 265/1500\n",
      "204/204 [==============================] - 0s 559us/step - loss: 15.9381 - predict_loss: 0.0000e+00 - loss_loss: 15.9381 - val_loss: 11.5997 - val_predict_loss: 0.0000e+00 - val_loss_loss: 11.5997\n",
      "Epoch 266/1500\n",
      "204/204 [==============================] - 0s 628us/step - loss: 20.5568 - predict_loss: 0.0000e+00 - loss_loss: 20.5568 - val_loss: 5.2201 - val_predict_loss: 0.0000e+00 - val_loss_loss: 5.2201\n",
      "Epoch 267/1500\n",
      "204/204 [==============================] - 0s 585us/step - loss: 16.1582 - predict_loss: 0.0000e+00 - loss_loss: 16.1582 - val_loss: 6.1026 - val_predict_loss: 0.0000e+00 - val_loss_loss: 6.1026\n",
      "Epoch 268/1500\n",
      "204/204 [==============================] - 0s 565us/step - loss: 26.3541 - predict_loss: 0.0000e+00 - loss_loss: 26.3541 - val_loss: 5.8260 - val_predict_loss: 0.0000e+00 - val_loss_loss: 5.8260\n",
      "Epoch 269/1500\n",
      "204/204 [==============================] - 0s 578us/step - loss: 33.3599 - predict_loss: 0.0000e+00 - loss_loss: 33.3599 - val_loss: 5.0460 - val_predict_loss: 0.0000e+00 - val_loss_loss: 5.0460\n",
      "Epoch 270/1500\n",
      "204/204 [==============================] - 0s 582us/step - loss: 16.4979 - predict_loss: 0.0000e+00 - loss_loss: 16.4979 - val_loss: 4.7475 - val_predict_loss: 0.0000e+00 - val_loss_loss: 4.7475\n",
      "Epoch 271/1500\n",
      "204/204 [==============================] - 0s 595us/step - loss: 21.4233 - predict_loss: 0.0000e+00 - loss_loss: 21.4233 - val_loss: 6.0765 - val_predict_loss: 0.0000e+00 - val_loss_loss: 6.0765\n",
      "Epoch 272/1500\n",
      "204/204 [==============================] - 0s 641us/step - loss: 20.1866 - predict_loss: 0.0000e+00 - loss_loss: 20.1866 - val_loss: 7.2960 - val_predict_loss: 0.0000e+00 - val_loss_loss: 7.2960\n",
      "Epoch 273/1500\n",
      "204/204 [==============================] - 0s 621us/step - loss: 20.2025 - predict_loss: 0.0000e+00 - loss_loss: 20.2025 - val_loss: 7.3154 - val_predict_loss: 0.0000e+00 - val_loss_loss: 7.3154\n",
      "Epoch 274/1500\n",
      "204/204 [==============================] - 0s 528us/step - loss: 21.2742 - predict_loss: 0.0000e+00 - loss_loss: 21.2742 - val_loss: 6.6204 - val_predict_loss: 0.0000e+00 - val_loss_loss: 6.6204\n",
      "Epoch 275/1500\n",
      "204/204 [==============================] - 0s 555us/step - loss: 16.7142 - predict_loss: 0.0000e+00 - loss_loss: 16.7142 - val_loss: 7.1449 - val_predict_loss: 0.0000e+00 - val_loss_loss: 7.1449\n",
      "Epoch 276/1500\n",
      "204/204 [==============================] - 0s 617us/step - loss: 25.3992 - predict_loss: 0.0000e+00 - loss_loss: 25.3992 - val_loss: 10.3446 - val_predict_loss: 0.0000e+00 - val_loss_loss: 10.3446\n",
      "Epoch 277/1500\n",
      "204/204 [==============================] - 0s 602us/step - loss: 21.1349 - predict_loss: 0.0000e+00 - loss_loss: 21.1349 - val_loss: 4.9064 - val_predict_loss: 0.0000e+00 - val_loss_loss: 4.9064\n",
      "Epoch 278/1500\n",
      "204/204 [==============================] - 0s 570us/step - loss: 15.5218 - predict_loss: 0.0000e+00 - loss_loss: 15.5218 - val_loss: 6.4394 - val_predict_loss: 0.0000e+00 - val_loss_loss: 6.4394\n",
      "Epoch 279/1500\n",
      "204/204 [==============================] - 0s 561us/step - loss: 30.1758 - predict_loss: 0.0000e+00 - loss_loss: 30.1758 - val_loss: 6.1025 - val_predict_loss: 0.0000e+00 - val_loss_loss: 6.1025\n",
      "Epoch 280/1500\n",
      "204/204 [==============================] - 0s 590us/step - loss: 16.9110 - predict_loss: 0.0000e+00 - loss_loss: 16.9110 - val_loss: 4.9342 - val_predict_loss: 0.0000e+00 - val_loss_loss: 4.9342\n",
      "Epoch 281/1500\n",
      "204/204 [==============================] - 0s 567us/step - loss: 22.5984 - predict_loss: 0.0000e+00 - loss_loss: 22.5984 - val_loss: 5.3897 - val_predict_loss: 0.0000e+00 - val_loss_loss: 5.3897\n",
      "Epoch 282/1500\n",
      "204/204 [==============================] - 0s 587us/step - loss: 23.5597 - predict_loss: 0.0000e+00 - loss_loss: 23.5597 - val_loss: 6.1066 - val_predict_loss: 0.0000e+00 - val_loss_loss: 6.1066\n",
      "Epoch 283/1500\n",
      "204/204 [==============================] - 0s 547us/step - loss: 21.4220 - predict_loss: 0.0000e+00 - loss_loss: 21.4220 - val_loss: 4.6853 - val_predict_loss: 0.0000e+00 - val_loss_loss: 4.6853\n",
      "Epoch 284/1500\n",
      "204/204 [==============================] - 0s 604us/step - loss: 15.9877 - predict_loss: 0.0000e+00 - loss_loss: 15.9877 - val_loss: 7.2866 - val_predict_loss: 0.0000e+00 - val_loss_loss: 7.2866\n",
      "Epoch 285/1500\n",
      "204/204 [==============================] - 0s 629us/step - loss: 27.8628 - predict_loss: 0.0000e+00 - loss_loss: 27.8628 - val_loss: 7.3662 - val_predict_loss: 0.0000e+00 - val_loss_loss: 7.3662\n",
      "Epoch 286/1500\n",
      "204/204 [==============================] - 0s 646us/step - loss: 19.9785 - predict_loss: 0.0000e+00 - loss_loss: 19.9785 - val_loss: 7.0698 - val_predict_loss: 0.0000e+00 - val_loss_loss: 7.0698\n",
      "Epoch 287/1500\n",
      "204/204 [==============================] - 0s 640us/step - loss: 17.0229 - predict_loss: 0.0000e+00 - loss_loss: 17.0229 - val_loss: 6.5932 - val_predict_loss: 0.0000e+00 - val_loss_loss: 6.5932\n",
      "Epoch 288/1500\n",
      "204/204 [==============================] - 0s 605us/step - loss: 28.7214 - predict_loss: 0.0000e+00 - loss_loss: 28.7214 - val_loss: 4.4109 - val_predict_loss: 0.0000e+00 - val_loss_loss: 4.4109\n",
      "Epoch 289/1500\n",
      "204/204 [==============================] - 0s 586us/step - loss: 15.7000 - predict_loss: 0.0000e+00 - loss_loss: 15.7000 - val_loss: 7.3974 - val_predict_loss: 0.0000e+00 - val_loss_loss: 7.3974\n",
      "Epoch 290/1500\n",
      "204/204 [==============================] - 0s 669us/step - loss: 20.1207 - predict_loss: 0.0000e+00 - loss_loss: 20.1207 - val_loss: 6.0451 - val_predict_loss: 0.0000e+00 - val_loss_loss: 6.0451\n",
      "Epoch 291/1500\n",
      "204/204 [==============================] - 0s 573us/step - loss: 22.5157 - predict_loss: 0.0000e+00 - loss_loss: 22.5157 - val_loss: 8.8388 - val_predict_loss: 0.0000e+00 - val_loss_loss: 8.8388\n",
      "Epoch 292/1500\n",
      "204/204 [==============================] - 0s 643us/step - loss: 19.1994 - predict_loss: 0.0000e+00 - loss_loss: 19.1994 - val_loss: 5.8123 - val_predict_loss: 0.0000e+00 - val_loss_loss: 5.8123\n",
      "Epoch 293/1500\n",
      "204/204 [==============================] - 0s 620us/step - loss: 17.1520 - predict_loss: 0.0000e+00 - loss_loss: 17.1520 - val_loss: 5.4721 - val_predict_loss: 0.0000e+00 - val_loss_loss: 5.4721\n",
      "Epoch 294/1500\n",
      "204/204 [==============================] - 0s 606us/step - loss: 16.6221 - predict_loss: 0.0000e+00 - loss_loss: 16.6221 - val_loss: 5.4184 - val_predict_loss: 0.0000e+00 - val_loss_loss: 5.4184\n",
      "Epoch 295/1500\n",
      "204/204 [==============================] - 0s 607us/step - loss: 26.8642 - predict_loss: 0.0000e+00 - loss_loss: 26.8642 - val_loss: 7.8482 - val_predict_loss: 0.0000e+00 - val_loss_loss: 7.8482\n",
      "Epoch 296/1500\n",
      "204/204 [==============================] - 0s 621us/step - loss: 25.8295 - predict_loss: 0.0000e+00 - loss_loss: 25.8295 - val_loss: 5.6515 - val_predict_loss: 0.0000e+00 - val_loss_loss: 5.6515\n",
      "Epoch 297/1500\n",
      "204/204 [==============================] - 0s 577us/step - loss: 15.9094 - predict_loss: 0.0000e+00 - loss_loss: 15.9094 - val_loss: 6.1060 - val_predict_loss: 0.0000e+00 - val_loss_loss: 6.1060\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 298/1500\n",
      "204/204 [==============================] - 0s 585us/step - loss: 21.4639 - predict_loss: 0.0000e+00 - loss_loss: 21.4639 - val_loss: 6.2553 - val_predict_loss: 0.0000e+00 - val_loss_loss: 6.2553\n",
      "Epoch 299/1500\n",
      "204/204 [==============================] - 0s 618us/step - loss: 19.5110 - predict_loss: 0.0000e+00 - loss_loss: 19.5110 - val_loss: 4.5131 - val_predict_loss: 0.0000e+00 - val_loss_loss: 4.5131\n",
      "Epoch 300/1500\n",
      "204/204 [==============================] - 0s 595us/step - loss: 18.1357 - predict_loss: 0.0000e+00 - loss_loss: 18.1357 - val_loss: 6.3350 - val_predict_loss: 0.0000e+00 - val_loss_loss: 6.3350\n",
      "Epoch 301/1500\n",
      "204/204 [==============================] - 0s 557us/step - loss: 24.7593 - predict_loss: 0.0000e+00 - loss_loss: 24.7593 - val_loss: 9.3862 - val_predict_loss: 0.0000e+00 - val_loss_loss: 9.3862\n",
      "Epoch 302/1500\n",
      "204/204 [==============================] - 0s 567us/step - loss: 18.4030 - predict_loss: 0.0000e+00 - loss_loss: 18.4030 - val_loss: 5.0072 - val_predict_loss: 0.0000e+00 - val_loss_loss: 5.0072\n",
      "Epoch 303/1500\n",
      "204/204 [==============================] - 0s 548us/step - loss: 16.9544 - predict_loss: 0.0000e+00 - loss_loss: 16.9544 - val_loss: 7.2984 - val_predict_loss: 0.0000e+00 - val_loss_loss: 7.2984\n",
      "Epoch 304/1500\n",
      "204/204 [==============================] - 0s 549us/step - loss: 25.3907 - predict_loss: 0.0000e+00 - loss_loss: 25.3907 - val_loss: 5.7194 - val_predict_loss: 0.0000e+00 - val_loss_loss: 5.7194\n",
      "Epoch 305/1500\n",
      "204/204 [==============================] - 0s 559us/step - loss: 16.0921 - predict_loss: 0.0000e+00 - loss_loss: 16.0921 - val_loss: 4.6838 - val_predict_loss: 0.0000e+00 - val_loss_loss: 4.6838\n",
      "Epoch 306/1500\n",
      "204/204 [==============================] - 0s 570us/step - loss: 24.8773 - predict_loss: 0.0000e+00 - loss_loss: 24.8773 - val_loss: 6.5524 - val_predict_loss: 0.0000e+00 - val_loss_loss: 6.5524\n",
      "Epoch 307/1500\n",
      "204/204 [==============================] - 0s 574us/step - loss: 20.0620 - predict_loss: 0.0000e+00 - loss_loss: 20.0620 - val_loss: 8.1960 - val_predict_loss: 0.0000e+00 - val_loss_loss: 8.1960\n",
      "Epoch 308/1500\n",
      "204/204 [==============================] - 0s 509us/step - loss: 18.5434 - predict_loss: 0.0000e+00 - loss_loss: 18.5434 - val_loss: 6.4975 - val_predict_loss: 0.0000e+00 - val_loss_loss: 6.4975\n",
      "Epoch 309/1500\n",
      "204/204 [==============================] - 0s 589us/step - loss: 15.2455 - predict_loss: 0.0000e+00 - loss_loss: 15.2455 - val_loss: 6.3089 - val_predict_loss: 0.0000e+00 - val_loss_loss: 6.3089\n",
      "Epoch 310/1500\n",
      "204/204 [==============================] - 0s 516us/step - loss: 21.6847 - predict_loss: 0.0000e+00 - loss_loss: 21.6847 - val_loss: 6.6570 - val_predict_loss: 0.0000e+00 - val_loss_loss: 6.6570\n",
      "Epoch 311/1500\n",
      "204/204 [==============================] - 0s 479us/step - loss: 19.7921 - predict_loss: 0.0000e+00 - loss_loss: 19.7921 - val_loss: 9.6910 - val_predict_loss: 0.0000e+00 - val_loss_loss: 9.6910\n",
      "Epoch 312/1500\n",
      "204/204 [==============================] - 0s 550us/step - loss: 15.8517 - predict_loss: 0.0000e+00 - loss_loss: 15.8517 - val_loss: 6.6594 - val_predict_loss: 0.0000e+00 - val_loss_loss: 6.6594\n",
      "Epoch 313/1500\n",
      "204/204 [==============================] - 0s 506us/step - loss: 17.3826 - predict_loss: 0.0000e+00 - loss_loss: 17.3826 - val_loss: 4.8208 - val_predict_loss: 0.0000e+00 - val_loss_loss: 4.8208\n",
      "Epoch 314/1500\n",
      "204/204 [==============================] - 0s 657us/step - loss: 14.2611 - predict_loss: 0.0000e+00 - loss_loss: 14.2611 - val_loss: 4.4336 - val_predict_loss: 0.0000e+00 - val_loss_loss: 4.4336\n",
      "Epoch 315/1500\n",
      "204/204 [==============================] - 0s 558us/step - loss: 25.5717 - predict_loss: 0.0000e+00 - loss_loss: 25.5717 - val_loss: 8.3908 - val_predict_loss: 0.0000e+00 - val_loss_loss: 8.3908\n",
      "Epoch 316/1500\n",
      "204/204 [==============================] - 0s 565us/step - loss: 19.8664 - predict_loss: 0.0000e+00 - loss_loss: 19.8664 - val_loss: 6.4102 - val_predict_loss: 0.0000e+00 - val_loss_loss: 6.4102\n",
      "Epoch 317/1500\n",
      "204/204 [==============================] - 0s 580us/step - loss: 16.8813 - predict_loss: 0.0000e+00 - loss_loss: 16.8813 - val_loss: 9.5965 - val_predict_loss: 0.0000e+00 - val_loss_loss: 9.5965\n",
      "Epoch 318/1500\n",
      "204/204 [==============================] - 0s 595us/step - loss: 28.1159 - predict_loss: 0.0000e+00 - loss_loss: 28.1159 - val_loss: 7.6423 - val_predict_loss: 0.0000e+00 - val_loss_loss: 7.6423\n",
      "Epoch 319/1500\n",
      "204/204 [==============================] - 0s 602us/step - loss: 14.4194 - predict_loss: 0.0000e+00 - loss_loss: 14.4194 - val_loss: 7.7562 - val_predict_loss: 0.0000e+00 - val_loss_loss: 7.7562\n",
      "Epoch 320/1500\n",
      "204/204 [==============================] - 0s 630us/step - loss: 19.8969 - predict_loss: 0.0000e+00 - loss_loss: 19.8969 - val_loss: 9.6623 - val_predict_loss: 0.0000e+00 - val_loss_loss: 9.6623\n",
      "Epoch 321/1500\n",
      "204/204 [==============================] - 0s 672us/step - loss: 14.0977 - predict_loss: 0.0000e+00 - loss_loss: 14.0977 - val_loss: 5.8684 - val_predict_loss: 0.0000e+00 - val_loss_loss: 5.8684\n",
      "Epoch 322/1500\n",
      "204/204 [==============================] - 0s 651us/step - loss: 21.5796 - predict_loss: 0.0000e+00 - loss_loss: 21.5796 - val_loss: 3.6446 - val_predict_loss: 0.0000e+00 - val_loss_loss: 3.6446\n",
      "Epoch 323/1500\n",
      "204/204 [==============================] - 0s 653us/step - loss: 39.5448 - predict_loss: 0.0000e+00 - loss_loss: 39.5448 - val_loss: 4.8195 - val_predict_loss: 0.0000e+00 - val_loss_loss: 4.8195\n",
      "Epoch 324/1500\n",
      "204/204 [==============================] - 0s 613us/step - loss: 13.7407 - predict_loss: 0.0000e+00 - loss_loss: 13.7407 - val_loss: 5.5987 - val_predict_loss: 0.0000e+00 - val_loss_loss: 5.5987\n",
      "Epoch 325/1500\n",
      "204/204 [==============================] - 0s 634us/step - loss: 14.5702 - predict_loss: 0.0000e+00 - loss_loss: 14.5702 - val_loss: 4.2166 - val_predict_loss: 0.0000e+00 - val_loss_loss: 4.2166\n",
      "Epoch 326/1500\n",
      "204/204 [==============================] - 0s 608us/step - loss: 24.0402 - predict_loss: 0.0000e+00 - loss_loss: 24.0402 - val_loss: 5.2257 - val_predict_loss: 0.0000e+00 - val_loss_loss: 5.2257\n",
      "Epoch 327/1500\n",
      "204/204 [==============================] - 0s 518us/step - loss: 17.1810 - predict_loss: 0.0000e+00 - loss_loss: 17.1810 - val_loss: 5.0483 - val_predict_loss: 0.0000e+00 - val_loss_loss: 5.0483\n",
      "Epoch 328/1500\n",
      "204/204 [==============================] - 0s 721us/step - loss: 16.4437 - predict_loss: 0.0000e+00 - loss_loss: 16.4437 - val_loss: 6.5725 - val_predict_loss: 0.0000e+00 - val_loss_loss: 6.5725\n",
      "Epoch 329/1500\n",
      "204/204 [==============================] - 0s 610us/step - loss: 21.2523 - predict_loss: 0.0000e+00 - loss_loss: 21.2523 - val_loss: 5.4676 - val_predict_loss: 0.0000e+00 - val_loss_loss: 5.4676\n",
      "Epoch 330/1500\n",
      "204/204 [==============================] - 0s 650us/step - loss: 22.7761 - predict_loss: 0.0000e+00 - loss_loss: 22.7761 - val_loss: 9.2383 - val_predict_loss: 0.0000e+00 - val_loss_loss: 9.2383\n",
      "Epoch 331/1500\n",
      "204/204 [==============================] - 0s 648us/step - loss: 16.9832 - predict_loss: 0.0000e+00 - loss_loss: 16.9832 - val_loss: 5.7759 - val_predict_loss: 0.0000e+00 - val_loss_loss: 5.7759\n",
      "Epoch 332/1500\n",
      "204/204 [==============================] - 0s 668us/step - loss: 16.3918 - predict_loss: 0.0000e+00 - loss_loss: 16.3918 - val_loss: 4.3092 - val_predict_loss: 0.0000e+00 - val_loss_loss: 4.3092\n",
      "Epoch 333/1500\n",
      "204/204 [==============================] - 0s 634us/step - loss: 18.3211 - predict_loss: 0.0000e+00 - loss_loss: 18.3211 - val_loss: 8.4937 - val_predict_loss: 0.0000e+00 - val_loss_loss: 8.4937\n",
      "Epoch 334/1500\n",
      "204/204 [==============================] - 0s 628us/step - loss: 15.3683 - predict_loss: 0.0000e+00 - loss_loss: 15.3683 - val_loss: 5.3108 - val_predict_loss: 0.0000e+00 - val_loss_loss: 5.3108\n",
      "Epoch 335/1500\n",
      "204/204 [==============================] - 0s 582us/step - loss: 24.9632 - predict_loss: 0.0000e+00 - loss_loss: 24.9632 - val_loss: 5.6002 - val_predict_loss: 0.0000e+00 - val_loss_loss: 5.6002\n",
      "Epoch 336/1500\n",
      "204/204 [==============================] - 0s 606us/step - loss: 14.8985 - predict_loss: 0.0000e+00 - loss_loss: 14.8985 - val_loss: 5.8272 - val_predict_loss: 0.0000e+00 - val_loss_loss: 5.8272\n",
      "Epoch 337/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "204/204 [==============================] - 0s 624us/step - loss: 25.2984 - predict_loss: 0.0000e+00 - loss_loss: 25.2984 - val_loss: 10.2043 - val_predict_loss: 0.0000e+00 - val_loss_loss: 10.2043\n",
      "Epoch 338/1500\n",
      "204/204 [==============================] - 0s 603us/step - loss: 14.0044 - predict_loss: 0.0000e+00 - loss_loss: 14.0044 - val_loss: 8.1869 - val_predict_loss: 0.0000e+00 - val_loss_loss: 8.1869\n",
      "Epoch 339/1500\n",
      "204/204 [==============================] - 0s 584us/step - loss: 17.0846 - predict_loss: 0.0000e+00 - loss_loss: 17.0846 - val_loss: 5.9247 - val_predict_loss: 0.0000e+00 - val_loss_loss: 5.9247\n",
      "Epoch 340/1500\n",
      "204/204 [==============================] - 0s 560us/step - loss: 16.8393 - predict_loss: 0.0000e+00 - loss_loss: 16.8393 - val_loss: 8.8433 - val_predict_loss: 0.0000e+00 - val_loss_loss: 8.8433\n",
      "Epoch 341/1500\n",
      "204/204 [==============================] - 0s 652us/step - loss: 25.0302 - predict_loss: 0.0000e+00 - loss_loss: 25.0302 - val_loss: 12.3133 - val_predict_loss: 0.0000e+00 - val_loss_loss: 12.3133\n",
      "Epoch 342/1500\n",
      "204/204 [==============================] - 0s 529us/step - loss: 16.6482 - predict_loss: 0.0000e+00 - loss_loss: 16.6482 - val_loss: 6.7644 - val_predict_loss: 0.0000e+00 - val_loss_loss: 6.7644\n",
      "Epoch 343/1500\n",
      "204/204 [==============================] - 0s 557us/step - loss: 15.7579 - predict_loss: 0.0000e+00 - loss_loss: 15.7579 - val_loss: 3.8736 - val_predict_loss: 0.0000e+00 - val_loss_loss: 3.8736\n",
      "Epoch 344/1500\n",
      "204/204 [==============================] - 0s 644us/step - loss: 13.4899 - predict_loss: 0.0000e+00 - loss_loss: 13.4899 - val_loss: 4.7719 - val_predict_loss: 0.0000e+00 - val_loss_loss: 4.7719\n",
      "Epoch 345/1500\n",
      "204/204 [==============================] - 0s 568us/step - loss: 17.3816 - predict_loss: 0.0000e+00 - loss_loss: 17.3816 - val_loss: 8.5851 - val_predict_loss: 0.0000e+00 - val_loss_loss: 8.5851\n",
      "Epoch 346/1500\n",
      "204/204 [==============================] - 0s 645us/step - loss: 23.9782 - predict_loss: 0.0000e+00 - loss_loss: 23.9782 - val_loss: 6.9865 - val_predict_loss: 0.0000e+00 - val_loss_loss: 6.9865\n",
      "Epoch 347/1500\n",
      "204/204 [==============================] - 0s 582us/step - loss: 15.4732 - predict_loss: 0.0000e+00 - loss_loss: 15.4732 - val_loss: 8.7654 - val_predict_loss: 0.0000e+00 - val_loss_loss: 8.7654\n",
      "Epoch 348/1500\n",
      "204/204 [==============================] - 0s 628us/step - loss: 26.4649 - predict_loss: 0.0000e+00 - loss_loss: 26.4649 - val_loss: 7.0578 - val_predict_loss: 0.0000e+00 - val_loss_loss: 7.0578\n",
      "Epoch 349/1500\n",
      "204/204 [==============================] - 0s 566us/step - loss: 14.4335 - predict_loss: 0.0000e+00 - loss_loss: 14.4335 - val_loss: 4.5844 - val_predict_loss: 0.0000e+00 - val_loss_loss: 4.5844\n",
      "Epoch 350/1500\n",
      "204/204 [==============================] - 0s 590us/step - loss: 17.8791 - predict_loss: 0.0000e+00 - loss_loss: 17.8791 - val_loss: 5.5512 - val_predict_loss: 0.0000e+00 - val_loss_loss: 5.5512\n",
      "Epoch 351/1500\n",
      "204/204 [==============================] - 0s 586us/step - loss: 18.1950 - predict_loss: 0.0000e+00 - loss_loss: 18.1950 - val_loss: 9.3377 - val_predict_loss: 0.0000e+00 - val_loss_loss: 9.3377\n",
      "Epoch 352/1500\n",
      "204/204 [==============================] - 0s 558us/step - loss: 15.5326 - predict_loss: 0.0000e+00 - loss_loss: 15.5326 - val_loss: 5.7403 - val_predict_loss: 0.0000e+00 - val_loss_loss: 5.7403\n",
      "Epoch 353/1500\n",
      "204/204 [==============================] - 0s 608us/step - loss: 23.3369 - predict_loss: 0.0000e+00 - loss_loss: 23.3369 - val_loss: 15.3897 - val_predict_loss: 0.0000e+00 - val_loss_loss: 15.3897\n",
      "Epoch 354/1500\n",
      "204/204 [==============================] - 0s 527us/step - loss: 23.8667 - predict_loss: 0.0000e+00 - loss_loss: 23.8667 - val_loss: 5.4355 - val_predict_loss: 0.0000e+00 - val_loss_loss: 5.4355\n",
      "Epoch 355/1500\n",
      "204/204 [==============================] - 0s 604us/step - loss: 11.9442 - predict_loss: 0.0000e+00 - loss_loss: 11.9442 - val_loss: 3.6082 - val_predict_loss: 0.0000e+00 - val_loss_loss: 3.6082\n",
      "Epoch 356/1500\n",
      "204/204 [==============================] - 0s 572us/step - loss: 15.4829 - predict_loss: 0.0000e+00 - loss_loss: 15.4829 - val_loss: 4.0219 - val_predict_loss: 0.0000e+00 - val_loss_loss: 4.0219\n",
      "Epoch 357/1500\n",
      "204/204 [==============================] - 0s 570us/step - loss: 18.1709 - predict_loss: 0.0000e+00 - loss_loss: 18.1709 - val_loss: 6.0693 - val_predict_loss: 0.0000e+00 - val_loss_loss: 6.0693\n",
      "Epoch 358/1500\n",
      "204/204 [==============================] - 0s 571us/step - loss: 16.9634 - predict_loss: 0.0000e+00 - loss_loss: 16.9634 - val_loss: 5.3173 - val_predict_loss: 0.0000e+00 - val_loss_loss: 5.3173\n",
      "Epoch 359/1500\n",
      "204/204 [==============================] - 0s 618us/step - loss: 16.3948 - predict_loss: 0.0000e+00 - loss_loss: 16.3948 - val_loss: 11.6156 - val_predict_loss: 0.0000e+00 - val_loss_loss: 11.6156\n",
      "Epoch 360/1500\n",
      "204/204 [==============================] - 0s 552us/step - loss: 25.5707 - predict_loss: 0.0000e+00 - loss_loss: 25.5707 - val_loss: 5.1194 - val_predict_loss: 0.0000e+00 - val_loss_loss: 5.1194\n",
      "Epoch 361/1500\n",
      "204/204 [==============================] - 0s 607us/step - loss: 15.4196 - predict_loss: 0.0000e+00 - loss_loss: 15.4196 - val_loss: 5.6488 - val_predict_loss: 0.0000e+00 - val_loss_loss: 5.6488\n",
      "Epoch 362/1500\n",
      "204/204 [==============================] - 0s 602us/step - loss: 17.7818 - predict_loss: 0.0000e+00 - loss_loss: 17.7818 - val_loss: 6.5129 - val_predict_loss: 0.0000e+00 - val_loss_loss: 6.5129\n",
      "Epoch 363/1500\n",
      "204/204 [==============================] - 0s 650us/step - loss: 16.3081 - predict_loss: 0.0000e+00 - loss_loss: 16.3081 - val_loss: 5.9147 - val_predict_loss: 0.0000e+00 - val_loss_loss: 5.9147\n",
      "Epoch 364/1500\n",
      "204/204 [==============================] - 0s 647us/step - loss: 18.7692 - predict_loss: 0.0000e+00 - loss_loss: 18.7692 - val_loss: 5.8143 - val_predict_loss: 0.0000e+00 - val_loss_loss: 5.8143\n",
      "Epoch 365/1500\n",
      "204/204 [==============================] - 0s 628us/step - loss: 17.8944 - predict_loss: 0.0000e+00 - loss_loss: 17.8944 - val_loss: 8.6952 - val_predict_loss: 0.0000e+00 - val_loss_loss: 8.6952\n",
      "Epoch 366/1500\n",
      "204/204 [==============================] - 0s 585us/step - loss: 16.1990 - predict_loss: 0.0000e+00 - loss_loss: 16.1990 - val_loss: 5.8777 - val_predict_loss: 0.0000e+00 - val_loss_loss: 5.8777\n",
      "Epoch 367/1500\n",
      "204/204 [==============================] - 0s 652us/step - loss: 20.1427 - predict_loss: 0.0000e+00 - loss_loss: 20.1427 - val_loss: 8.4136 - val_predict_loss: 0.0000e+00 - val_loss_loss: 8.4136\n",
      "Epoch 368/1500\n",
      "204/204 [==============================] - 0s 653us/step - loss: 15.3670 - predict_loss: 0.0000e+00 - loss_loss: 15.3670 - val_loss: 6.1819 - val_predict_loss: 0.0000e+00 - val_loss_loss: 6.1819\n",
      "Epoch 369/1500\n",
      "204/204 [==============================] - 0s 585us/step - loss: 21.3652 - predict_loss: 0.0000e+00 - loss_loss: 21.3652 - val_loss: 6.3146 - val_predict_loss: 0.0000e+00 - val_loss_loss: 6.3146\n",
      "Epoch 370/1500\n",
      "204/204 [==============================] - 0s 612us/step - loss: 14.7619 - predict_loss: 0.0000e+00 - loss_loss: 14.7619 - val_loss: 4.6453 - val_predict_loss: 0.0000e+00 - val_loss_loss: 4.6453\n",
      "Epoch 371/1500\n",
      "204/204 [==============================] - 0s 631us/step - loss: 19.0807 - predict_loss: 0.0000e+00 - loss_loss: 19.0807 - val_loss: 3.8203 - val_predict_loss: 0.0000e+00 - val_loss_loss: 3.8203\n",
      "Epoch 372/1500\n",
      "204/204 [==============================] - 0s 617us/step - loss: 13.2070 - predict_loss: 0.0000e+00 - loss_loss: 13.2070 - val_loss: 9.0449 - val_predict_loss: 0.0000e+00 - val_loss_loss: 9.0449\n",
      "Epoch 373/1500\n",
      "204/204 [==============================] - 0s 638us/step - loss: 35.4731 - predict_loss: 0.0000e+00 - loss_loss: 35.4731 - val_loss: 7.6331 - val_predict_loss: 0.0000e+00 - val_loss_loss: 7.6331\n",
      "Epoch 374/1500\n",
      "204/204 [==============================] - 0s 673us/step - loss: 13.2225 - predict_loss: 0.0000e+00 - loss_loss: 13.2225 - val_loss: 5.4549 - val_predict_loss: 0.0000e+00 - val_loss_loss: 5.4549\n",
      "Epoch 375/1500\n",
      "204/204 [==============================] - 0s 608us/step - loss: 12.9223 - predict_loss: 0.0000e+00 - loss_loss: 12.9223 - val_loss: 4.6114 - val_predict_loss: 0.0000e+00 - val_loss_loss: 4.6114\n",
      "Epoch 376/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "204/204 [==============================] - 0s 631us/step - loss: 13.6092 - predict_loss: 0.0000e+00 - loss_loss: 13.6092 - val_loss: 8.5092 - val_predict_loss: 0.0000e+00 - val_loss_loss: 8.5092\n",
      "Epoch 377/1500\n",
      "204/204 [==============================] - 0s 587us/step - loss: 19.3104 - predict_loss: 0.0000e+00 - loss_loss: 19.3104 - val_loss: 6.8886 - val_predict_loss: 0.0000e+00 - val_loss_loss: 6.8886\n",
      "Epoch 378/1500\n",
      "204/204 [==============================] - 0s 576us/step - loss: 15.8212 - predict_loss: 0.0000e+00 - loss_loss: 15.8212 - val_loss: 6.0908 - val_predict_loss: 0.0000e+00 - val_loss_loss: 6.0908\n",
      "Epoch 379/1500\n",
      "204/204 [==============================] - 0s 519us/step - loss: 17.6507 - predict_loss: 0.0000e+00 - loss_loss: 17.6507 - val_loss: 9.9493 - val_predict_loss: 0.0000e+00 - val_loss_loss: 9.9493\n",
      "Epoch 380/1500\n",
      "204/204 [==============================] - 0s 585us/step - loss: 18.9068 - predict_loss: 0.0000e+00 - loss_loss: 18.9068 - val_loss: 6.4536 - val_predict_loss: 0.0000e+00 - val_loss_loss: 6.4536\n",
      "Epoch 381/1500\n",
      "204/204 [==============================] - 0s 671us/step - loss: 13.6824 - predict_loss: 0.0000e+00 - loss_loss: 13.6824 - val_loss: 9.5575 - val_predict_loss: 0.0000e+00 - val_loss_loss: 9.5575\n",
      "Epoch 382/1500\n",
      "204/204 [==============================] - 0s 641us/step - loss: 26.2148 - predict_loss: 0.0000e+00 - loss_loss: 26.2148 - val_loss: 7.3461 - val_predict_loss: 0.0000e+00 - val_loss_loss: 7.3461\n",
      "Epoch 383/1500\n",
      "204/204 [==============================] - 0s 604us/step - loss: 17.3475 - predict_loss: 0.0000e+00 - loss_loss: 17.3475 - val_loss: 5.7175 - val_predict_loss: 0.0000e+00 - val_loss_loss: 5.7175\n",
      "Epoch 384/1500\n",
      "204/204 [==============================] - 0s 606us/step - loss: 15.0629 - predict_loss: 0.0000e+00 - loss_loss: 15.0629 - val_loss: 4.4007 - val_predict_loss: 0.0000e+00 - val_loss_loss: 4.4007\n",
      "Epoch 385/1500\n",
      "204/204 [==============================] - 0s 549us/step - loss: 16.3660 - predict_loss: 0.0000e+00 - loss_loss: 16.3660 - val_loss: 7.3732 - val_predict_loss: 0.0000e+00 - val_loss_loss: 7.3732\n",
      "Epoch 386/1500\n",
      "204/204 [==============================] - 0s 558us/step - loss: 25.6938 - predict_loss: 0.0000e+00 - loss_loss: 25.6938 - val_loss: 5.9445 - val_predict_loss: 0.0000e+00 - val_loss_loss: 5.9445\n",
      "Epoch 387/1500\n",
      "204/204 [==============================] - 0s 598us/step - loss: 13.7764 - predict_loss: 0.0000e+00 - loss_loss: 13.7764 - val_loss: 5.0733 - val_predict_loss: 0.0000e+00 - val_loss_loss: 5.0733\n",
      "Epoch 388/1500\n",
      "204/204 [==============================] - 0s 617us/step - loss: 16.2717 - predict_loss: 0.0000e+00 - loss_loss: 16.2717 - val_loss: 9.1375 - val_predict_loss: 0.0000e+00 - val_loss_loss: 9.1375\n",
      "Epoch 389/1500\n",
      "204/204 [==============================] - 0s 565us/step - loss: 15.1021 - predict_loss: 0.0000e+00 - loss_loss: 15.1021 - val_loss: 4.9405 - val_predict_loss: 0.0000e+00 - val_loss_loss: 4.9405\n",
      "Epoch 390/1500\n",
      "204/204 [==============================] - 0s 531us/step - loss: 19.4060 - predict_loss: 0.0000e+00 - loss_loss: 19.4060 - val_loss: 6.3663 - val_predict_loss: 0.0000e+00 - val_loss_loss: 6.3663\n",
      "Epoch 391/1500\n",
      "204/204 [==============================] - 0s 660us/step - loss: 18.2863 - predict_loss: 0.0000e+00 - loss_loss: 18.2863 - val_loss: 5.5194 - val_predict_loss: 0.0000e+00 - val_loss_loss: 5.5194\n",
      "Epoch 392/1500\n",
      "204/204 [==============================] - 0s 663us/step - loss: 14.2948 - predict_loss: 0.0000e+00 - loss_loss: 14.2948 - val_loss: 7.5721 - val_predict_loss: 0.0000e+00 - val_loss_loss: 7.5721\n",
      "Epoch 393/1500\n",
      "204/204 [==============================] - 0s 636us/step - loss: 17.1510 - predict_loss: 0.0000e+00 - loss_loss: 17.1510 - val_loss: 5.2493 - val_predict_loss: 0.0000e+00 - val_loss_loss: 5.2493\n",
      "Epoch 394/1500\n",
      "204/204 [==============================] - 0s 626us/step - loss: 17.6584 - predict_loss: 0.0000e+00 - loss_loss: 17.6584 - val_loss: 7.9419 - val_predict_loss: 0.0000e+00 - val_loss_loss: 7.9419\n",
      "Epoch 395/1500\n",
      "204/204 [==============================] - 0s 609us/step - loss: 17.3171 - predict_loss: 0.0000e+00 - loss_loss: 17.3171 - val_loss: 5.7038 - val_predict_loss: 0.0000e+00 - val_loss_loss: 5.7038\n",
      "Epoch 396/1500\n",
      "204/204 [==============================] - 0s 627us/step - loss: 16.0591 - predict_loss: 0.0000e+00 - loss_loss: 16.0591 - val_loss: 7.4523 - val_predict_loss: 0.0000e+00 - val_loss_loss: 7.4523\n",
      "Epoch 397/1500\n",
      "204/204 [==============================] - 0s 642us/step - loss: 16.8674 - predict_loss: 0.0000e+00 - loss_loss: 16.8674 - val_loss: 5.6721 - val_predict_loss: 0.0000e+00 - val_loss_loss: 5.6721\n",
      "Epoch 398/1500\n",
      "204/204 [==============================] - 0s 651us/step - loss: 14.7851 - predict_loss: 0.0000e+00 - loss_loss: 14.7851 - val_loss: 6.2804 - val_predict_loss: 0.0000e+00 - val_loss_loss: 6.2804\n",
      "Epoch 399/1500\n",
      "204/204 [==============================] - 0s 673us/step - loss: 22.7053 - predict_loss: 0.0000e+00 - loss_loss: 22.7053 - val_loss: 7.5465 - val_predict_loss: 0.0000e+00 - val_loss_loss: 7.5465\n",
      "Epoch 400/1500\n",
      "204/204 [==============================] - 0s 649us/step - loss: 12.0522 - predict_loss: 0.0000e+00 - loss_loss: 12.0522 - val_loss: 5.0353 - val_predict_loss: 0.0000e+00 - val_loss_loss: 5.0353\n",
      "Epoch 401/1500\n",
      "204/204 [==============================] - 0s 628us/step - loss: 16.5146 - predict_loss: 0.0000e+00 - loss_loss: 16.5146 - val_loss: 3.8887 - val_predict_loss: 0.0000e+00 - val_loss_loss: 3.8887\n",
      "Epoch 402/1500\n",
      "204/204 [==============================] - 0s 611us/step - loss: 18.8560 - predict_loss: 0.0000e+00 - loss_loss: 18.8560 - val_loss: 5.3076 - val_predict_loss: 0.0000e+00 - val_loss_loss: 5.3076\n",
      "Epoch 403/1500\n",
      "204/204 [==============================] - 0s 654us/step - loss: 12.6468 - predict_loss: 0.0000e+00 - loss_loss: 12.6468 - val_loss: 3.6897 - val_predict_loss: 0.0000e+00 - val_loss_loss: 3.6897\n",
      "Epoch 404/1500\n",
      "204/204 [==============================] - 0s 642us/step - loss: 27.5140 - predict_loss: 0.0000e+00 - loss_loss: 27.5140 - val_loss: 4.6796 - val_predict_loss: 0.0000e+00 - val_loss_loss: 4.6796\n",
      "Epoch 405/1500\n",
      "204/204 [==============================] - 0s 712us/step - loss: 17.9628 - predict_loss: 0.0000e+00 - loss_loss: 17.9628 - val_loss: 6.2018 - val_predict_loss: 0.0000e+00 - val_loss_loss: 6.2018\n",
      "Epoch 406/1500\n",
      "204/204 [==============================] - 0s 2ms/step - loss: 14.3741 - predict_loss: 0.0000e+00 - loss_loss: 14.3741 - val_loss: 5.2828 - val_predict_loss: 0.0000e+00 - val_loss_loss: 5.2828\n",
      "Epoch 407/1500\n",
      "204/204 [==============================] - 0s 607us/step - loss: 3.7242 - predict_loss: 0.0000e+00 - loss_loss: 3.7242 - val_loss: 2.4167 - val_predict_loss: 0.0000e+00 - val_loss_loss: 2.4167\n",
      "Epoch 408/1500\n",
      "204/204 [==============================] - 0s 614us/step - loss: 5.3139 - predict_loss: 0.0000e+00 - loss_loss: 5.3139 - val_loss: 2.8669 - val_predict_loss: 0.0000e+00 - val_loss_loss: 2.8669\n",
      "Epoch 409/1500\n",
      "204/204 [==============================] - 0s 648us/step - loss: 5.7688 - predict_loss: 0.0000e+00 - loss_loss: 5.7688 - val_loss: 2.5635 - val_predict_loss: 0.0000e+00 - val_loss_loss: 2.5635\n",
      "Epoch 410/1500\n",
      "204/204 [==============================] - 0s 598us/step - loss: 5.6489 - predict_loss: 0.0000e+00 - loss_loss: 5.6489 - val_loss: 2.2197 - val_predict_loss: 0.0000e+00 - val_loss_loss: 2.2197\n",
      "Epoch 411/1500\n",
      "204/204 [==============================] - 0s 608us/step - loss: 5.8223 - predict_loss: 0.0000e+00 - loss_loss: 5.8223 - val_loss: 2.4152 - val_predict_loss: 0.0000e+00 - val_loss_loss: 2.4152\n",
      "Epoch 412/1500\n",
      "204/204 [==============================] - 0s 623us/step - loss: 4.6724 - predict_loss: 0.0000e+00 - loss_loss: 4.6724 - val_loss: 4.9591 - val_predict_loss: 0.0000e+00 - val_loss_loss: 4.9591\n",
      "Epoch 413/1500\n",
      "204/204 [==============================] - 0s 636us/step - loss: 6.1009 - predict_loss: 0.0000e+00 - loss_loss: 6.1009 - val_loss: 3.2017 - val_predict_loss: 0.0000e+00 - val_loss_loss: 3.2017\n",
      "Epoch 414/1500\n",
      "204/204 [==============================] - 0s 625us/step - loss: 7.6460 - predict_loss: 0.0000e+00 - loss_loss: 7.6460 - val_loss: 2.4453 - val_predict_loss: 0.0000e+00 - val_loss_loss: 2.4453\n",
      "Epoch 415/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "204/204 [==============================] - 0s 558us/step - loss: 5.5029 - predict_loss: 0.0000e+00 - loss_loss: 5.5029 - val_loss: 1.9980 - val_predict_loss: 0.0000e+00 - val_loss_loss: 1.9980\n",
      "Epoch 416/1500\n",
      "204/204 [==============================] - 0s 568us/step - loss: 6.6515 - predict_loss: 0.0000e+00 - loss_loss: 6.6515 - val_loss: 3.7740 - val_predict_loss: 0.0000e+00 - val_loss_loss: 3.7740\n",
      "Epoch 417/1500\n",
      "204/204 [==============================] - 0s 632us/step - loss: 5.3580 - predict_loss: 0.0000e+00 - loss_loss: 5.3580 - val_loss: 2.5067 - val_predict_loss: 0.0000e+00 - val_loss_loss: 2.5067\n",
      "Epoch 418/1500\n",
      "204/204 [==============================] - 0s 574us/step - loss: 7.0237 - predict_loss: 0.0000e+00 - loss_loss: 7.0237 - val_loss: 2.3051 - val_predict_loss: 0.0000e+00 - val_loss_loss: 2.3051\n",
      "Epoch 419/1500\n",
      "204/204 [==============================] - 0s 563us/step - loss: 6.3406 - predict_loss: 0.0000e+00 - loss_loss: 6.3406 - val_loss: 3.8780 - val_predict_loss: 0.0000e+00 - val_loss_loss: 3.8780\n",
      "Epoch 420/1500\n",
      "204/204 [==============================] - 0s 537us/step - loss: 6.9591 - predict_loss: 0.0000e+00 - loss_loss: 6.9591 - val_loss: 2.2781 - val_predict_loss: 0.0000e+00 - val_loss_loss: 2.2781\n",
      "Epoch 421/1500\n",
      "204/204 [==============================] - 0s 538us/step - loss: 6.0115 - predict_loss: 0.0000e+00 - loss_loss: 6.0115 - val_loss: 3.1707 - val_predict_loss: 0.0000e+00 - val_loss_loss: 3.1707\n",
      "Epoch 422/1500\n",
      "204/204 [==============================] - 0s 603us/step - loss: 4.9487 - predict_loss: 0.0000e+00 - loss_loss: 4.9487 - val_loss: 2.8580 - val_predict_loss: 0.0000e+00 - val_loss_loss: 2.8580\n",
      "Epoch 423/1500\n",
      "204/204 [==============================] - 0s 613us/step - loss: 6.3082 - predict_loss: 0.0000e+00 - loss_loss: 6.3082 - val_loss: 2.8843 - val_predict_loss: 0.0000e+00 - val_loss_loss: 2.8843\n",
      "Epoch 424/1500\n",
      "204/204 [==============================] - 0s 627us/step - loss: 6.2251 - predict_loss: 0.0000e+00 - loss_loss: 6.2251 - val_loss: 2.4798 - val_predict_loss: 0.0000e+00 - val_loss_loss: 2.4798\n",
      "Epoch 425/1500\n",
      "204/204 [==============================] - 0s 610us/step - loss: 6.2224 - predict_loss: 0.0000e+00 - loss_loss: 6.2224 - val_loss: 3.0213 - val_predict_loss: 0.0000e+00 - val_loss_loss: 3.0213\n",
      "Epoch 426/1500\n",
      "204/204 [==============================] - 0s 644us/step - loss: 5.8186 - predict_loss: 0.0000e+00 - loss_loss: 5.8186 - val_loss: 2.2361 - val_predict_loss: 0.0000e+00 - val_loss_loss: 2.2361\n",
      "Epoch 427/1500\n",
      "204/204 [==============================] - 0s 595us/step - loss: 5.1747 - predict_loss: 0.0000e+00 - loss_loss: 5.1747 - val_loss: 1.9779 - val_predict_loss: 0.0000e+00 - val_loss_loss: 1.9779\n",
      "Epoch 428/1500\n",
      "204/204 [==============================] - 0s 645us/step - loss: 8.5174 - predict_loss: 0.0000e+00 - loss_loss: 8.5174 - val_loss: 2.1961 - val_predict_loss: 0.0000e+00 - val_loss_loss: 2.1961\n",
      "Epoch 429/1500\n",
      "204/204 [==============================] - 0s 634us/step - loss: 5.1000 - predict_loss: 0.0000e+00 - loss_loss: 5.1000 - val_loss: 3.0398 - val_predict_loss: 0.0000e+00 - val_loss_loss: 3.0398\n",
      "Epoch 430/1500\n",
      "204/204 [==============================] - 0s 665us/step - loss: 6.6571 - predict_loss: 0.0000e+00 - loss_loss: 6.6571 - val_loss: 1.9355 - val_predict_loss: 0.0000e+00 - val_loss_loss: 1.9355\n",
      "Epoch 431/1500\n",
      "204/204 [==============================] - 0s 614us/step - loss: 4.4262 - predict_loss: 0.0000e+00 - loss_loss: 4.4262 - val_loss: 2.9715 - val_predict_loss: 0.0000e+00 - val_loss_loss: 2.9715\n",
      "Epoch 432/1500\n",
      "204/204 [==============================] - 0s 624us/step - loss: 6.9452 - predict_loss: 0.0000e+00 - loss_loss: 6.9452 - val_loss: 2.8646 - val_predict_loss: 0.0000e+00 - val_loss_loss: 2.8646\n",
      "Epoch 433/1500\n",
      "204/204 [==============================] - 0s 610us/step - loss: 4.8690 - predict_loss: 0.0000e+00 - loss_loss: 4.8690 - val_loss: 1.9336 - val_predict_loss: 0.0000e+00 - val_loss_loss: 1.9336\n",
      "Epoch 434/1500\n",
      "204/204 [==============================] - 0s 575us/step - loss: 7.1058 - predict_loss: 0.0000e+00 - loss_loss: 7.1058 - val_loss: 2.5808 - val_predict_loss: 0.0000e+00 - val_loss_loss: 2.5808\n",
      "Epoch 435/1500\n",
      "204/204 [==============================] - 0s 620us/step - loss: 5.7080 - predict_loss: 0.0000e+00 - loss_loss: 5.7080 - val_loss: 2.3223 - val_predict_loss: 0.0000e+00 - val_loss_loss: 2.3223\n",
      "Epoch 436/1500\n",
      "204/204 [==============================] - 0s 642us/step - loss: 7.4216 - predict_loss: 0.0000e+00 - loss_loss: 7.4216 - val_loss: 2.7809 - val_predict_loss: 0.0000e+00 - val_loss_loss: 2.7809\n",
      "Epoch 437/1500\n",
      "204/204 [==============================] - 0s 604us/step - loss: 5.1897 - predict_loss: 0.0000e+00 - loss_loss: 5.1897 - val_loss: 2.1906 - val_predict_loss: 0.0000e+00 - val_loss_loss: 2.1906\n",
      "Epoch 438/1500\n",
      "204/204 [==============================] - 0s 626us/step - loss: 5.5473 - predict_loss: 0.0000e+00 - loss_loss: 5.5473 - val_loss: 2.8828 - val_predict_loss: 0.0000e+00 - val_loss_loss: 2.8828\n",
      "Epoch 439/1500\n",
      "204/204 [==============================] - 0s 603us/step - loss: 5.6376 - predict_loss: 0.0000e+00 - loss_loss: 5.6376 - val_loss: 2.0230 - val_predict_loss: 0.0000e+00 - val_loss_loss: 2.0230\n",
      "Epoch 440/1500\n",
      "204/204 [==============================] - 0s 564us/step - loss: 7.1076 - predict_loss: 0.0000e+00 - loss_loss: 7.1076 - val_loss: 2.2487 - val_predict_loss: 0.0000e+00 - val_loss_loss: 2.2487\n",
      "Epoch 441/1500\n",
      "204/204 [==============================] - 0s 542us/step - loss: 6.0150 - predict_loss: 0.0000e+00 - loss_loss: 6.0150 - val_loss: 2.3705 - val_predict_loss: 0.0000e+00 - val_loss_loss: 2.3705\n",
      "Epoch 442/1500\n",
      "204/204 [==============================] - 0s 669us/step - loss: 5.5915 - predict_loss: 0.0000e+00 - loss_loss: 5.5915 - val_loss: 4.4800 - val_predict_loss: 0.0000e+00 - val_loss_loss: 4.4800\n",
      "Epoch 443/1500\n",
      "204/204 [==============================] - 0s 654us/step - loss: 6.1189 - predict_loss: 0.0000e+00 - loss_loss: 6.1189 - val_loss: 2.3381 - val_predict_loss: 0.0000e+00 - val_loss_loss: 2.3381\n",
      "Epoch 444/1500\n",
      "204/204 [==============================] - 0s 593us/step - loss: 6.0186 - predict_loss: 0.0000e+00 - loss_loss: 6.0186 - val_loss: 3.1185 - val_predict_loss: 0.0000e+00 - val_loss_loss: 3.1185\n",
      "Epoch 445/1500\n",
      "204/204 [==============================] - 0s 593us/step - loss: 5.2823 - predict_loss: 0.0000e+00 - loss_loss: 5.2823 - val_loss: 2.8224 - val_predict_loss: 0.0000e+00 - val_loss_loss: 2.8224\n",
      "Epoch 446/1500\n",
      "204/204 [==============================] - 0s 672us/step - loss: 7.6294 - predict_loss: 0.0000e+00 - loss_loss: 7.6294 - val_loss: 2.5226 - val_predict_loss: 0.0000e+00 - val_loss_loss: 2.5226\n",
      "Epoch 447/1500\n",
      "204/204 [==============================] - 0s 670us/step - loss: 5.3313 - predict_loss: 0.0000e+00 - loss_loss: 5.3313 - val_loss: 2.5860 - val_predict_loss: 0.0000e+00 - val_loss_loss: 2.5860\n",
      "Epoch 448/1500\n",
      "204/204 [==============================] - 0s 680us/step - loss: 5.8167 - predict_loss: 0.0000e+00 - loss_loss: 5.8167 - val_loss: 2.1643 - val_predict_loss: 0.0000e+00 - val_loss_loss: 2.1643\n",
      "Epoch 449/1500\n",
      "204/204 [==============================] - 0s 628us/step - loss: 5.9087 - predict_loss: 0.0000e+00 - loss_loss: 5.9087 - val_loss: 3.2870 - val_predict_loss: 0.0000e+00 - val_loss_loss: 3.2870\n",
      "Epoch 450/1500\n",
      "204/204 [==============================] - 0s 618us/step - loss: 6.6389 - predict_loss: 0.0000e+00 - loss_loss: 6.6389 - val_loss: 2.5245 - val_predict_loss: 0.0000e+00 - val_loss_loss: 2.5245\n",
      "Epoch 451/1500\n",
      "204/204 [==============================] - 0s 576us/step - loss: 6.6178 - predict_loss: 0.0000e+00 - loss_loss: 6.6178 - val_loss: 2.2692 - val_predict_loss: 0.0000e+00 - val_loss_loss: 2.2692\n",
      "Epoch 452/1500\n",
      "204/204 [==============================] - 0s 623us/step - loss: 4.3460 - predict_loss: 0.0000e+00 - loss_loss: 4.3460 - val_loss: 2.7047 - val_predict_loss: 0.0000e+00 - val_loss_loss: 2.7047\n",
      "Epoch 453/1500\n",
      "204/204 [==============================] - 0s 661us/step - loss: 4.8754 - predict_loss: 0.0000e+00 - loss_loss: 4.8754 - val_loss: 2.0510 - val_predict_loss: 0.0000e+00 - val_loss_loss: 2.0510\n",
      "Epoch 454/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "204/204 [==============================] - 0s 620us/step - loss: 9.0510 - predict_loss: 0.0000e+00 - loss_loss: 9.0510 - val_loss: 2.6959 - val_predict_loss: 0.0000e+00 - val_loss_loss: 2.6959\n",
      "Epoch 455/1500\n",
      "204/204 [==============================] - 0s 544us/step - loss: 4.6614 - predict_loss: 0.0000e+00 - loss_loss: 4.6614 - val_loss: 1.9355 - val_predict_loss: 0.0000e+00 - val_loss_loss: 1.9355\n",
      "Epoch 456/1500\n",
      "204/204 [==============================] - 0s 607us/step - loss: 5.1561 - predict_loss: 0.0000e+00 - loss_loss: 5.1561 - val_loss: 2.2414 - val_predict_loss: 0.0000e+00 - val_loss_loss: 2.2414\n",
      "Epoch 457/1500\n",
      "204/204 [==============================] - 0s 543us/step - loss: 6.5550 - predict_loss: 0.0000e+00 - loss_loss: 6.5550 - val_loss: 2.2318 - val_predict_loss: 0.0000e+00 - val_loss_loss: 2.2318\n",
      "Epoch 458/1500\n",
      "204/204 [==============================] - 0s 606us/step - loss: 5.5162 - predict_loss: 0.0000e+00 - loss_loss: 5.5162 - val_loss: 2.8025 - val_predict_loss: 0.0000e+00 - val_loss_loss: 2.8025\n",
      "Epoch 459/1500\n",
      "204/204 [==============================] - 0s 600us/step - loss: 1.8013 - predict_loss: 0.0000e+00 - loss_loss: 1.8013 - val_loss: 1.5090 - val_predict_loss: 0.0000e+00 - val_loss_loss: 1.5090\n",
      "Epoch 460/1500\n",
      "204/204 [==============================] - 0s 528us/step - loss: 1.8179 - predict_loss: 0.0000e+00 - loss_loss: 1.8179 - val_loss: 1.2871 - val_predict_loss: 0.0000e+00 - val_loss_loss: 1.2871\n",
      "Epoch 461/1500\n",
      "204/204 [==============================] - 0s 541us/step - loss: 2.5027 - predict_loss: 0.0000e+00 - loss_loss: 2.5027 - val_loss: 1.2940 - val_predict_loss: 0.0000e+00 - val_loss_loss: 1.2940\n",
      "Epoch 462/1500\n",
      "204/204 [==============================] - 0s 612us/step - loss: 2.2877 - predict_loss: 0.0000e+00 - loss_loss: 2.2877 - val_loss: 1.2448 - val_predict_loss: 0.0000e+00 - val_loss_loss: 1.2448\n",
      "Epoch 463/1500\n",
      "204/204 [==============================] - 0s 610us/step - loss: 1.8471 - predict_loss: 0.0000e+00 - loss_loss: 1.8471 - val_loss: 1.2350 - val_predict_loss: 0.0000e+00 - val_loss_loss: 1.2350\n",
      "Epoch 464/1500\n",
      "204/204 [==============================] - 0s 580us/step - loss: 3.6876 - predict_loss: 0.0000e+00 - loss_loss: 3.6876 - val_loss: 1.4631 - val_predict_loss: 0.0000e+00 - val_loss_loss: 1.4631\n",
      "Epoch 465/1500\n",
      "204/204 [==============================] - 0s 627us/step - loss: 2.2298 - predict_loss: 0.0000e+00 - loss_loss: 2.2298 - val_loss: 1.9969 - val_predict_loss: 0.0000e+00 - val_loss_loss: 1.9969\n",
      "Epoch 466/1500\n",
      "204/204 [==============================] - 0s 561us/step - loss: 2.0259 - predict_loss: 0.0000e+00 - loss_loss: 2.0259 - val_loss: 1.4390 - val_predict_loss: 0.0000e+00 - val_loss_loss: 1.4390\n",
      "Epoch 467/1500\n",
      "204/204 [==============================] - 0s 573us/step - loss: 2.3816 - predict_loss: 0.0000e+00 - loss_loss: 2.3816 - val_loss: 1.2003 - val_predict_loss: 0.0000e+00 - val_loss_loss: 1.2003\n",
      "Epoch 468/1500\n",
      "204/204 [==============================] - 0s 625us/step - loss: 2.3866 - predict_loss: 0.0000e+00 - loss_loss: 2.3866 - val_loss: 1.3315 - val_predict_loss: 0.0000e+00 - val_loss_loss: 1.3315\n",
      "Epoch 469/1500\n",
      "204/204 [==============================] - 0s 580us/step - loss: 2.2003 - predict_loss: 0.0000e+00 - loss_loss: 2.2003 - val_loss: 1.2336 - val_predict_loss: 0.0000e+00 - val_loss_loss: 1.2336\n",
      "Epoch 470/1500\n",
      "204/204 [==============================] - 0s 652us/step - loss: 2.6148 - predict_loss: 0.0000e+00 - loss_loss: 2.6148 - val_loss: 1.2144 - val_predict_loss: 0.0000e+00 - val_loss_loss: 1.2144\n",
      "Epoch 471/1500\n",
      "204/204 [==============================] - 0s 537us/step - loss: 2.4516 - predict_loss: 0.0000e+00 - loss_loss: 2.4516 - val_loss: 1.3055 - val_predict_loss: 0.0000e+00 - val_loss_loss: 1.3055\n",
      "Epoch 472/1500\n",
      "204/204 [==============================] - 0s 591us/step - loss: 1.7982 - predict_loss: 0.0000e+00 - loss_loss: 1.7982 - val_loss: 1.2142 - val_predict_loss: 0.0000e+00 - val_loss_loss: 1.2142\n",
      "Epoch 473/1500\n",
      "204/204 [==============================] - 0s 608us/step - loss: 2.8016 - predict_loss: 0.0000e+00 - loss_loss: 2.8016 - val_loss: 1.3200 - val_predict_loss: 0.0000e+00 - val_loss_loss: 1.3200\n",
      "Epoch 474/1500\n",
      "204/204 [==============================] - 0s 542us/step - loss: 2.2172 - predict_loss: 0.0000e+00 - loss_loss: 2.2172 - val_loss: 1.1827 - val_predict_loss: 0.0000e+00 - val_loss_loss: 1.1827\n",
      "Epoch 475/1500\n",
      "204/204 [==============================] - 0s 499us/step - loss: 1.8576 - predict_loss: 0.0000e+00 - loss_loss: 1.8576 - val_loss: 1.3471 - val_predict_loss: 0.0000e+00 - val_loss_loss: 1.3471\n",
      "Epoch 476/1500\n",
      "204/204 [==============================] - 0s 597us/step - loss: 2.8291 - predict_loss: 0.0000e+00 - loss_loss: 2.8291 - val_loss: 1.8624 - val_predict_loss: 0.0000e+00 - val_loss_loss: 1.8624\n",
      "Epoch 477/1500\n",
      "204/204 [==============================] - 0s 577us/step - loss: 2.3649 - predict_loss: 0.0000e+00 - loss_loss: 2.3649 - val_loss: 1.2044 - val_predict_loss: 0.0000e+00 - val_loss_loss: 1.2044\n",
      "Epoch 478/1500\n",
      "204/204 [==============================] - 0s 634us/step - loss: 2.5195 - predict_loss: 0.0000e+00 - loss_loss: 2.5195 - val_loss: 1.0874 - val_predict_loss: 0.0000e+00 - val_loss_loss: 1.0874\n",
      "Epoch 479/1500\n",
      "204/204 [==============================] - 0s 667us/step - loss: 2.1972 - predict_loss: 0.0000e+00 - loss_loss: 2.1972 - val_loss: 1.3718 - val_predict_loss: 0.0000e+00 - val_loss_loss: 1.3718\n",
      "Epoch 480/1500\n",
      "204/204 [==============================] - 0s 599us/step - loss: 2.7294 - predict_loss: 0.0000e+00 - loss_loss: 2.7294 - val_loss: 1.6549 - val_predict_loss: 0.0000e+00 - val_loss_loss: 1.6549\n",
      "Epoch 481/1500\n",
      "204/204 [==============================] - 0s 640us/step - loss: 2.2081 - predict_loss: 0.0000e+00 - loss_loss: 2.2081 - val_loss: 1.1119 - val_predict_loss: 0.0000e+00 - val_loss_loss: 1.1119\n",
      "Epoch 482/1500\n",
      "204/204 [==============================] - 0s 586us/step - loss: 2.1103 - predict_loss: 0.0000e+00 - loss_loss: 2.1103 - val_loss: 1.0487 - val_predict_loss: 0.0000e+00 - val_loss_loss: 1.0487\n",
      "Epoch 483/1500\n",
      "204/204 [==============================] - 0s 583us/step - loss: 1.9913 - predict_loss: 0.0000e+00 - loss_loss: 1.9913 - val_loss: 1.1002 - val_predict_loss: 0.0000e+00 - val_loss_loss: 1.1002\n",
      "Epoch 484/1500\n",
      "204/204 [==============================] - 0s 601us/step - loss: 2.6744 - predict_loss: 0.0000e+00 - loss_loss: 2.6744 - val_loss: 1.4200 - val_predict_loss: 0.0000e+00 - val_loss_loss: 1.4200\n",
      "Epoch 485/1500\n",
      "204/204 [==============================] - 0s 630us/step - loss: 2.4248 - predict_loss: 0.0000e+00 - loss_loss: 2.4248 - val_loss: 1.0058 - val_predict_loss: 0.0000e+00 - val_loss_loss: 1.0058\n",
      "Epoch 486/1500\n",
      "204/204 [==============================] - 0s 641us/step - loss: 2.2841 - predict_loss: 0.0000e+00 - loss_loss: 2.2841 - val_loss: 1.0531 - val_predict_loss: 0.0000e+00 - val_loss_loss: 1.0531\n",
      "Epoch 487/1500\n",
      "204/204 [==============================] - 0s 719us/step - loss: 1.6252 - predict_loss: 0.0000e+00 - loss_loss: 1.6252 - val_loss: 1.0015 - val_predict_loss: 0.0000e+00 - val_loss_loss: 1.0015\n",
      "Epoch 488/1500\n",
      "204/204 [==============================] - 0s 588us/step - loss: 3.2178 - predict_loss: 0.0000e+00 - loss_loss: 3.2178 - val_loss: 1.2886 - val_predict_loss: 0.0000e+00 - val_loss_loss: 1.2886\n",
      "Epoch 489/1500\n",
      "204/204 [==============================] - 0s 640us/step - loss: 1.4387 - predict_loss: 0.0000e+00 - loss_loss: 1.4387 - val_loss: 1.4564 - val_predict_loss: 0.0000e+00 - val_loss_loss: 1.4564\n",
      "Epoch 490/1500\n",
      "204/204 [==============================] - 0s 679us/step - loss: 2.8260 - predict_loss: 0.0000e+00 - loss_loss: 2.8260 - val_loss: 1.1448 - val_predict_loss: 0.0000e+00 - val_loss_loss: 1.1448\n",
      "Epoch 491/1500\n",
      "204/204 [==============================] - 0s 626us/step - loss: 2.4920 - predict_loss: 0.0000e+00 - loss_loss: 2.4920 - val_loss: 1.1682 - val_predict_loss: 0.0000e+00 - val_loss_loss: 1.1682\n",
      "Epoch 492/1500\n",
      "204/204 [==============================] - 0s 668us/step - loss: 2.0743 - predict_loss: 0.0000e+00 - loss_loss: 2.0743 - val_loss: 1.4171 - val_predict_loss: 0.0000e+00 - val_loss_loss: 1.4171\n",
      "Epoch 493/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "204/204 [==============================] - 0s 598us/step - loss: 2.7634 - predict_loss: 0.0000e+00 - loss_loss: 2.7634 - val_loss: 0.9990 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.9990\n",
      "Epoch 494/1500\n",
      "204/204 [==============================] - 0s 522us/step - loss: 1.8765 - predict_loss: 0.0000e+00 - loss_loss: 1.8765 - val_loss: 1.1047 - val_predict_loss: 0.0000e+00 - val_loss_loss: 1.1047\n",
      "Epoch 495/1500\n",
      "204/204 [==============================] - 0s 545us/step - loss: 2.2953 - predict_loss: 0.0000e+00 - loss_loss: 2.2953 - val_loss: 0.9956 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.9956\n",
      "Epoch 496/1500\n",
      "204/204 [==============================] - 0s 625us/step - loss: 2.4695 - predict_loss: 0.0000e+00 - loss_loss: 2.4695 - val_loss: 0.9734 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.9734\n",
      "Epoch 497/1500\n",
      "204/204 [==============================] - 0s 616us/step - loss: 2.2613 - predict_loss: 0.0000e+00 - loss_loss: 2.2613 - val_loss: 0.9605 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.9605\n",
      "Epoch 498/1500\n",
      "204/204 [==============================] - 0s 536us/step - loss: 2.5308 - predict_loss: 0.0000e+00 - loss_loss: 2.5308 - val_loss: 1.1331 - val_predict_loss: 0.0000e+00 - val_loss_loss: 1.1331\n",
      "Epoch 499/1500\n",
      "204/204 [==============================] - 0s 665us/step - loss: 2.2653 - predict_loss: 0.0000e+00 - loss_loss: 2.2653 - val_loss: 1.1392 - val_predict_loss: 0.0000e+00 - val_loss_loss: 1.1392\n",
      "Epoch 500/1500\n",
      "204/204 [==============================] - 0s 588us/step - loss: 2.9414 - predict_loss: 0.0000e+00 - loss_loss: 2.9414 - val_loss: 1.0001 - val_predict_loss: 0.0000e+00 - val_loss_loss: 1.0001\n",
      "Epoch 501/1500\n",
      "204/204 [==============================] - 0s 607us/step - loss: 1.5193 - predict_loss: 0.0000e+00 - loss_loss: 1.5193 - val_loss: 0.8781 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.8781\n",
      "Epoch 502/1500\n",
      "204/204 [==============================] - 0s 575us/step - loss: 2.7926 - predict_loss: 0.0000e+00 - loss_loss: 2.7926 - val_loss: 0.9610 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.9610\n",
      "Epoch 503/1500\n",
      "204/204 [==============================] - 0s 569us/step - loss: 2.3972 - predict_loss: 0.0000e+00 - loss_loss: 2.3972 - val_loss: 1.2033 - val_predict_loss: 0.0000e+00 - val_loss_loss: 1.2033\n",
      "Epoch 504/1500\n",
      "204/204 [==============================] - 0s 607us/step - loss: 1.7902 - predict_loss: 0.0000e+00 - loss_loss: 1.7902 - val_loss: 0.9283 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.9283\n",
      "Epoch 505/1500\n",
      "204/204 [==============================] - 0s 570us/step - loss: 2.5513 - predict_loss: 0.0000e+00 - loss_loss: 2.5513 - val_loss: 1.0905 - val_predict_loss: 0.0000e+00 - val_loss_loss: 1.0905\n",
      "Epoch 506/1500\n",
      "204/204 [==============================] - 0s 585us/step - loss: 2.2634 - predict_loss: 0.0000e+00 - loss_loss: 2.2634 - val_loss: 1.0477 - val_predict_loss: 0.0000e+00 - val_loss_loss: 1.0477\n",
      "Epoch 507/1500\n",
      "204/204 [==============================] - 0s 592us/step - loss: 2.1513 - predict_loss: 0.0000e+00 - loss_loss: 2.1513 - val_loss: 1.1056 - val_predict_loss: 0.0000e+00 - val_loss_loss: 1.1056\n",
      "Epoch 508/1500\n",
      "204/204 [==============================] - 0s 600us/step - loss: 1.7992 - predict_loss: 0.0000e+00 - loss_loss: 1.7992 - val_loss: 1.0024 - val_predict_loss: 0.0000e+00 - val_loss_loss: 1.0024\n",
      "Epoch 509/1500\n",
      "204/204 [==============================] - 0s 618us/step - loss: 2.6434 - predict_loss: 0.0000e+00 - loss_loss: 2.6434 - val_loss: 1.4262 - val_predict_loss: 0.0000e+00 - val_loss_loss: 1.4262\n",
      "Epoch 510/1500\n",
      "204/204 [==============================] - 0s 624us/step - loss: 2.0862 - predict_loss: 0.0000e+00 - loss_loss: 2.0862 - val_loss: 0.9077 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.9077\n",
      "Epoch 511/1500\n",
      "204/204 [==============================] - 0s 585us/step - loss: 2.4638 - predict_loss: 0.0000e+00 - loss_loss: 2.4638 - val_loss: 0.9969 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.9969\n",
      "Epoch 512/1500\n",
      "204/204 [==============================] - 0s 585us/step - loss: 2.0928 - predict_loss: 0.0000e+00 - loss_loss: 2.0928 - val_loss: 0.9787 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.9787\n",
      "Epoch 513/1500\n",
      "204/204 [==============================] - 0s 630us/step - loss: 1.8704 - predict_loss: 0.0000e+00 - loss_loss: 1.8704 - val_loss: 0.8763 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.8763\n",
      "Epoch 514/1500\n",
      "204/204 [==============================] - 0s 550us/step - loss: 2.1208 - predict_loss: 0.0000e+00 - loss_loss: 2.1208 - val_loss: 1.2222 - val_predict_loss: 0.0000e+00 - val_loss_loss: 1.2222\n",
      "Epoch 515/1500\n",
      "204/204 [==============================] - 0s 617us/step - loss: 2.0182 - predict_loss: 0.0000e+00 - loss_loss: 2.0182 - val_loss: 1.0465 - val_predict_loss: 0.0000e+00 - val_loss_loss: 1.0465\n",
      "Epoch 516/1500\n",
      "204/204 [==============================] - 0s 635us/step - loss: 2.1972 - predict_loss: 0.0000e+00 - loss_loss: 2.1972 - val_loss: 1.0353 - val_predict_loss: 0.0000e+00 - val_loss_loss: 1.0353\n",
      "Epoch 517/1500\n",
      "204/204 [==============================] - 0s 602us/step - loss: 2.3314 - predict_loss: 0.0000e+00 - loss_loss: 2.3314 - val_loss: 0.9306 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.9306\n",
      "Epoch 518/1500\n",
      "204/204 [==============================] - 0s 611us/step - loss: 2.8195 - predict_loss: 0.0000e+00 - loss_loss: 2.8195 - val_loss: 1.0100 - val_predict_loss: 0.0000e+00 - val_loss_loss: 1.0100\n",
      "Epoch 519/1500\n",
      "204/204 [==============================] - 0s 656us/step - loss: 2.1823 - predict_loss: 0.0000e+00 - loss_loss: 2.1823 - val_loss: 1.1534 - val_predict_loss: 0.0000e+00 - val_loss_loss: 1.1534\n",
      "Epoch 520/1500\n",
      "204/204 [==============================] - 0s 621us/step - loss: 2.4621 - predict_loss: 0.0000e+00 - loss_loss: 2.4621 - val_loss: 1.0301 - val_predict_loss: 0.0000e+00 - val_loss_loss: 1.0301\n",
      "Epoch 521/1500\n",
      "204/204 [==============================] - 0s 566us/step - loss: 2.2115 - predict_loss: 0.0000e+00 - loss_loss: 2.2115 - val_loss: 1.1154 - val_predict_loss: 0.0000e+00 - val_loss_loss: 1.1154\n",
      "Epoch 522/1500\n",
      "204/204 [==============================] - 0s 606us/step - loss: 2.0979 - predict_loss: 0.0000e+00 - loss_loss: 2.0979 - val_loss: 0.8749 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.8749\n",
      "Epoch 523/1500\n",
      "204/204 [==============================] - 0s 670us/step - loss: 2.3875 - predict_loss: 0.0000e+00 - loss_loss: 2.3875 - val_loss: 1.0378 - val_predict_loss: 0.0000e+00 - val_loss_loss: 1.0378\n",
      "Epoch 524/1500\n",
      "204/204 [==============================] - 0s 630us/step - loss: 2.1603 - predict_loss: 0.0000e+00 - loss_loss: 2.1603 - val_loss: 0.9780 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.9780\n",
      "Epoch 525/1500\n",
      "204/204 [==============================] - 0s 600us/step - loss: 1.8944 - predict_loss: 0.0000e+00 - loss_loss: 1.8944 - val_loss: 1.3289 - val_predict_loss: 0.0000e+00 - val_loss_loss: 1.3289\n",
      "Epoch 526/1500\n",
      "204/204 [==============================] - 0s 631us/step - loss: 2.3105 - predict_loss: 0.0000e+00 - loss_loss: 2.3105 - val_loss: 1.0802 - val_predict_loss: 0.0000e+00 - val_loss_loss: 1.0802\n",
      "Epoch 527/1500\n",
      "204/204 [==============================] - 0s 611us/step - loss: 2.6827 - predict_loss: 0.0000e+00 - loss_loss: 2.6827 - val_loss: 1.5313 - val_predict_loss: 0.0000e+00 - val_loss_loss: 1.5313\n",
      "Epoch 528/1500\n",
      "204/204 [==============================] - 0s 649us/step - loss: 1.6934 - predict_loss: 0.0000e+00 - loss_loss: 1.6934 - val_loss: 1.6329 - val_predict_loss: 0.0000e+00 - val_loss_loss: 1.6329\n",
      "Epoch 529/1500\n",
      "204/204 [==============================] - 0s 560us/step - loss: 2.2891 - predict_loss: 0.0000e+00 - loss_loss: 2.2891 - val_loss: 1.3951 - val_predict_loss: 0.0000e+00 - val_loss_loss: 1.3951\n",
      "Epoch 530/1500\n",
      "204/204 [==============================] - 0s 571us/step - loss: 2.4797 - predict_loss: 0.0000e+00 - loss_loss: 2.4797 - val_loss: 0.9139 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.9139\n",
      "Epoch 531/1500\n",
      "204/204 [==============================] - 0s 625us/step - loss: 2.1314 - predict_loss: 0.0000e+00 - loss_loss: 2.1314 - val_loss: 1.0838 - val_predict_loss: 0.0000e+00 - val_loss_loss: 1.0838\n",
      "Epoch 532/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "204/204 [==============================] - 0s 673us/step - loss: 2.2373 - predict_loss: 0.0000e+00 - loss_loss: 2.2373 - val_loss: 1.0772 - val_predict_loss: 0.0000e+00 - val_loss_loss: 1.0772\n",
      "Epoch 533/1500\n",
      "204/204 [==============================] - 0s 522us/step - loss: 2.0437 - predict_loss: 0.0000e+00 - loss_loss: 2.0437 - val_loss: 0.9462 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.9462\n",
      "Epoch 534/1500\n",
      "204/204 [==============================] - 0s 588us/step - loss: 2.3690 - predict_loss: 0.0000e+00 - loss_loss: 2.3690 - val_loss: 0.9227 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.9227\n",
      "Epoch 535/1500\n",
      "204/204 [==============================] - 0s 490us/step - loss: 1.9044 - predict_loss: 0.0000e+00 - loss_loss: 1.9044 - val_loss: 1.1493 - val_predict_loss: 0.0000e+00 - val_loss_loss: 1.1493\n",
      "Epoch 536/1500\n",
      "204/204 [==============================] - 0s 508us/step - loss: 1.9623 - predict_loss: 0.0000e+00 - loss_loss: 1.9623 - val_loss: 1.0294 - val_predict_loss: 0.0000e+00 - val_loss_loss: 1.0294\n",
      "Epoch 537/1500\n",
      "204/204 [==============================] - 0s 615us/step - loss: 2.2844 - predict_loss: 0.0000e+00 - loss_loss: 2.2844 - val_loss: 1.0751 - val_predict_loss: 0.0000e+00 - val_loss_loss: 1.0751\n",
      "Epoch 538/1500\n",
      "204/204 [==============================] - 0s 602us/step - loss: 2.0075 - predict_loss: 0.0000e+00 - loss_loss: 2.0075 - val_loss: 0.7655 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.7655\n",
      "Epoch 539/1500\n",
      "204/204 [==============================] - 0s 664us/step - loss: 2.5080 - predict_loss: 0.0000e+00 - loss_loss: 2.5080 - val_loss: 0.9597 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.9597\n",
      "Epoch 540/1500\n",
      "204/204 [==============================] - 0s 678us/step - loss: 1.9377 - predict_loss: 0.0000e+00 - loss_loss: 1.9377 - val_loss: 0.9614 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.9614\n",
      "Epoch 541/1500\n",
      "204/204 [==============================] - 0s 566us/step - loss: 0.9303 - predict_loss: 0.0000e+00 - loss_loss: 0.9303 - val_loss: 0.8737 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.8737\n",
      "Epoch 542/1500\n",
      "204/204 [==============================] - 0s 572us/step - loss: 1.0474 - predict_loss: 0.0000e+00 - loss_loss: 1.0474 - val_loss: 0.8267 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.8267\n",
      "Epoch 543/1500\n",
      "204/204 [==============================] - 0s 590us/step - loss: 1.2306 - predict_loss: 0.0000e+00 - loss_loss: 1.2306 - val_loss: 0.8238 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.8238\n",
      "Epoch 544/1500\n",
      "204/204 [==============================] - 0s 690us/step - loss: 1.1488 - predict_loss: 0.0000e+00 - loss_loss: 1.1488 - val_loss: 0.9053 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.9053\n",
      "Epoch 545/1500\n",
      "204/204 [==============================] - 0s 678us/step - loss: 1.1184 - predict_loss: 0.0000e+00 - loss_loss: 1.1184 - val_loss: 0.8462 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.8462\n",
      "Epoch 546/1500\n",
      "204/204 [==============================] - 0s 597us/step - loss: 1.3862 - predict_loss: 0.0000e+00 - loss_loss: 1.3862 - val_loss: 0.7208 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.7208\n",
      "Epoch 547/1500\n",
      "204/204 [==============================] - 0s 589us/step - loss: 1.0623 - predict_loss: 0.0000e+00 - loss_loss: 1.0623 - val_loss: 0.7344 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.7344\n",
      "Epoch 548/1500\n",
      "204/204 [==============================] - 0s 620us/step - loss: 1.1843 - predict_loss: 0.0000e+00 - loss_loss: 1.1843 - val_loss: 0.7004 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.7004\n",
      "Epoch 549/1500\n",
      "204/204 [==============================] - 0s 595us/step - loss: 1.0761 - predict_loss: 0.0000e+00 - loss_loss: 1.0761 - val_loss: 0.7744 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.7744\n",
      "Epoch 550/1500\n",
      "204/204 [==============================] - 0s 584us/step - loss: 1.0417 - predict_loss: 0.0000e+00 - loss_loss: 1.0417 - val_loss: 0.7842 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.7842\n",
      "Epoch 551/1500\n",
      "204/204 [==============================] - 0s 561us/step - loss: 1.3516 - predict_loss: 0.0000e+00 - loss_loss: 1.3516 - val_loss: 0.6833 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.6833\n",
      "Epoch 552/1500\n",
      "204/204 [==============================] - 0s 622us/step - loss: 1.1164 - predict_loss: 0.0000e+00 - loss_loss: 1.1164 - val_loss: 0.7837 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.7837\n",
      "Epoch 553/1500\n",
      "204/204 [==============================] - 0s 545us/step - loss: 1.2968 - predict_loss: 0.0000e+00 - loss_loss: 1.2968 - val_loss: 0.6591 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.6591\n",
      "Epoch 554/1500\n",
      "204/204 [==============================] - 0s 613us/step - loss: 1.0131 - predict_loss: 0.0000e+00 - loss_loss: 1.0131 - val_loss: 0.7165 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.7165\n",
      "Epoch 555/1500\n",
      "204/204 [==============================] - 0s 655us/step - loss: 1.1915 - predict_loss: 0.0000e+00 - loss_loss: 1.1915 - val_loss: 0.7319 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.7319\n",
      "Epoch 556/1500\n",
      "204/204 [==============================] - 0s 611us/step - loss: 1.0574 - predict_loss: 0.0000e+00 - loss_loss: 1.0574 - val_loss: 0.6618 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.6618\n",
      "Epoch 557/1500\n",
      "204/204 [==============================] - 0s 644us/step - loss: 1.2344 - predict_loss: 0.0000e+00 - loss_loss: 1.2344 - val_loss: 0.8278 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.8278\n",
      "Epoch 558/1500\n",
      "204/204 [==============================] - 0s 598us/step - loss: 0.9828 - predict_loss: 0.0000e+00 - loss_loss: 0.9828 - val_loss: 0.7430 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.7430\n",
      "Epoch 559/1500\n",
      "204/204 [==============================] - 0s 608us/step - loss: 1.2198 - predict_loss: 0.0000e+00 - loss_loss: 1.2198 - val_loss: 0.7956 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.7956\n",
      "Epoch 560/1500\n",
      "204/204 [==============================] - 0s 604us/step - loss: 0.9356 - predict_loss: 0.0000e+00 - loss_loss: 0.9356 - val_loss: 0.8219 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.8219\n",
      "Epoch 561/1500\n",
      "204/204 [==============================] - 0s 621us/step - loss: 1.1704 - predict_loss: 0.0000e+00 - loss_loss: 1.1704 - val_loss: 0.6648 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.6648\n",
      "Epoch 562/1500\n",
      "204/204 [==============================] - 0s 606us/step - loss: 0.8083 - predict_loss: 0.0000e+00 - loss_loss: 0.8083 - val_loss: 0.6473 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.6473\n",
      "Epoch 563/1500\n",
      "204/204 [==============================] - 0s 556us/step - loss: 1.4164 - predict_loss: 0.0000e+00 - loss_loss: 1.4164 - val_loss: 0.7581 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.7581\n",
      "Epoch 564/1500\n",
      "204/204 [==============================] - 0s 597us/step - loss: 1.0129 - predict_loss: 0.0000e+00 - loss_loss: 1.0129 - val_loss: 0.7392 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.7392\n",
      "Epoch 565/1500\n",
      "204/204 [==============================] - 0s 586us/step - loss: 1.1343 - predict_loss: 0.0000e+00 - loss_loss: 1.1343 - val_loss: 0.6621 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.6621\n",
      "Epoch 566/1500\n",
      "204/204 [==============================] - 0s 591us/step - loss: 0.9812 - predict_loss: 0.0000e+00 - loss_loss: 0.9812 - val_loss: 0.7186 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.7186\n",
      "Epoch 567/1500\n",
      "204/204 [==============================] - 0s 593us/step - loss: 0.7706 - predict_loss: 0.0000e+00 - loss_loss: 0.7706 - val_loss: 0.7406 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.7406\n",
      "Epoch 568/1500\n",
      "204/204 [==============================] - 0s 666us/step - loss: 1.4602 - predict_loss: 0.0000e+00 - loss_loss: 1.4602 - val_loss: 0.6583 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.6583\n",
      "Epoch 569/1500\n",
      "204/204 [==============================] - 0s 607us/step - loss: 0.8595 - predict_loss: 0.0000e+00 - loss_loss: 0.8595 - val_loss: 0.9288 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.9288\n",
      "Epoch 570/1500\n",
      "204/204 [==============================] - 0s 644us/step - loss: 1.2846 - predict_loss: 0.0000e+00 - loss_loss: 1.2846 - val_loss: 0.6980 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.6980\n",
      "Epoch 571/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "204/204 [==============================] - 0s 657us/step - loss: 1.1798 - predict_loss: 0.0000e+00 - loss_loss: 1.1798 - val_loss: 0.6704 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.6704\n",
      "Epoch 572/1500\n",
      "204/204 [==============================] - 0s 528us/step - loss: 1.0458 - predict_loss: 0.0000e+00 - loss_loss: 1.0458 - val_loss: 0.7143 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.7143\n",
      "Epoch 573/1500\n",
      "204/204 [==============================] - 0s 555us/step - loss: 0.8560 - predict_loss: 0.0000e+00 - loss_loss: 0.8560 - val_loss: 0.6534 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.6534\n",
      "Epoch 574/1500\n",
      "204/204 [==============================] - 0s 582us/step - loss: 1.3722 - predict_loss: 0.0000e+00 - loss_loss: 1.3722 - val_loss: 0.7767 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.7767\n",
      "Epoch 575/1500\n",
      "204/204 [==============================] - 0s 598us/step - loss: 1.0713 - predict_loss: 0.0000e+00 - loss_loss: 1.0713 - val_loss: 0.6425 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.6425\n",
      "Epoch 576/1500\n",
      "204/204 [==============================] - 0s 576us/step - loss: 1.0339 - predict_loss: 0.0000e+00 - loss_loss: 1.0339 - val_loss: 0.7932 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.7932\n",
      "Epoch 577/1500\n",
      "204/204 [==============================] - 0s 621us/step - loss: 1.1650 - predict_loss: 0.0000e+00 - loss_loss: 1.1650 - val_loss: 0.7917 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.7917\n",
      "Epoch 578/1500\n",
      "204/204 [==============================] - 0s 652us/step - loss: 1.1358 - predict_loss: 0.0000e+00 - loss_loss: 1.1358 - val_loss: 0.6949 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.6949\n",
      "Epoch 579/1500\n",
      "204/204 [==============================] - 0s 637us/step - loss: 0.8628 - predict_loss: 0.0000e+00 - loss_loss: 0.8628 - val_loss: 0.6434 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.6434\n",
      "Epoch 580/1500\n",
      "204/204 [==============================] - 0s 580us/step - loss: 0.9046 - predict_loss: 0.0000e+00 - loss_loss: 0.9046 - val_loss: 0.6283 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.6283\n",
      "Epoch 581/1500\n",
      "204/204 [==============================] - 0s 633us/step - loss: 1.1130 - predict_loss: 0.0000e+00 - loss_loss: 1.1130 - val_loss: 0.6020 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.6020\n",
      "Epoch 582/1500\n",
      "204/204 [==============================] - 0s 637us/step - loss: 0.7885 - predict_loss: 0.0000e+00 - loss_loss: 0.7885 - val_loss: 0.6882 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.6882\n",
      "Epoch 583/1500\n",
      "204/204 [==============================] - 0s 658us/step - loss: 1.4171 - predict_loss: 0.0000e+00 - loss_loss: 1.4171 - val_loss: 0.6712 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.6712\n",
      "Epoch 584/1500\n",
      "204/204 [==============================] - 0s 593us/step - loss: 0.7482 - predict_loss: 0.0000e+00 - loss_loss: 0.7482 - val_loss: 0.6642 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.6642\n",
      "Epoch 585/1500\n",
      "204/204 [==============================] - 0s 504us/step - loss: 1.3147 - predict_loss: 0.0000e+00 - loss_loss: 1.3147 - val_loss: 0.7348 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.7348\n",
      "Epoch 586/1500\n",
      "204/204 [==============================] - 0s 540us/step - loss: 0.9988 - predict_loss: 0.0000e+00 - loss_loss: 0.9988 - val_loss: 0.7173 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.7173\n",
      "Epoch 587/1500\n",
      "204/204 [==============================] - 0s 574us/step - loss: 0.8909 - predict_loss: 0.0000e+00 - loss_loss: 0.8909 - val_loss: 0.7574 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.7574\n",
      "Epoch 588/1500\n",
      "204/204 [==============================] - 0s 628us/step - loss: 1.1773 - predict_loss: 0.0000e+00 - loss_loss: 1.1773 - val_loss: 0.5863 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.5863\n",
      "Epoch 589/1500\n",
      "204/204 [==============================] - 0s 637us/step - loss: 0.9097 - predict_loss: 0.0000e+00 - loss_loss: 0.9097 - val_loss: 0.7232 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.7232\n",
      "Epoch 590/1500\n",
      "204/204 [==============================] - 0s 594us/step - loss: 1.1454 - predict_loss: 0.0000e+00 - loss_loss: 1.1454 - val_loss: 0.5880 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.5880\n",
      "Epoch 591/1500\n",
      "204/204 [==============================] - 0s 624us/step - loss: 0.7282 - predict_loss: 0.0000e+00 - loss_loss: 0.7282 - val_loss: 0.5712 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.5712\n",
      "Epoch 592/1500\n",
      "204/204 [==============================] - 0s 601us/step - loss: 1.4225 - predict_loss: 0.0000e+00 - loss_loss: 1.4225 - val_loss: 0.5628 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.5628\n",
      "Epoch 593/1500\n",
      "204/204 [==============================] - 0s 609us/step - loss: 0.8487 - predict_loss: 0.0000e+00 - loss_loss: 0.8487 - val_loss: 0.6292 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.6292\n",
      "Epoch 594/1500\n",
      "204/204 [==============================] - 0s 693us/step - loss: 1.0481 - predict_loss: 0.0000e+00 - loss_loss: 1.0481 - val_loss: 0.5640 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.5640\n",
      "Epoch 595/1500\n",
      "204/204 [==============================] - 0s 574us/step - loss: 0.9834 - predict_loss: 0.0000e+00 - loss_loss: 0.9834 - val_loss: 0.6162 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.6162\n",
      "Epoch 596/1500\n",
      "204/204 [==============================] - 0s 615us/step - loss: 0.8082 - predict_loss: 0.0000e+00 - loss_loss: 0.8082 - val_loss: 0.6348 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.6348\n",
      "Epoch 597/1500\n",
      "204/204 [==============================] - 0s 658us/step - loss: 1.0817 - predict_loss: 0.0000e+00 - loss_loss: 1.0817 - val_loss: 0.7575 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.7575\n",
      "Epoch 598/1500\n",
      "204/204 [==============================] - 0s 711us/step - loss: 1.0291 - predict_loss: 0.0000e+00 - loss_loss: 1.0291 - val_loss: 0.6729 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.6729\n",
      "Epoch 599/1500\n",
      "204/204 [==============================] - 0s 613us/step - loss: 0.9192 - predict_loss: 0.0000e+00 - loss_loss: 0.9192 - val_loss: 0.6356 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.6356\n",
      "Epoch 600/1500\n",
      "204/204 [==============================] - 0s 632us/step - loss: 1.1777 - predict_loss: 0.0000e+00 - loss_loss: 1.1777 - val_loss: 0.5690 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.5690\n",
      "Epoch 601/1500\n",
      "204/204 [==============================] - 0s 626us/step - loss: 1.0415 - predict_loss: 0.0000e+00 - loss_loss: 1.0415 - val_loss: 0.6325 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.6325\n",
      "Epoch 602/1500\n",
      "204/204 [==============================] - 0s 617us/step - loss: 0.8155 - predict_loss: 0.0000e+00 - loss_loss: 0.8155 - val_loss: 0.6145 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.6145\n",
      "Epoch 603/1500\n",
      "204/204 [==============================] - 0s 645us/step - loss: 0.8093 - predict_loss: 0.0000e+00 - loss_loss: 0.8093 - val_loss: 0.7142 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.7142\n",
      "Epoch 604/1500\n",
      "204/204 [==============================] - 0s 719us/step - loss: 1.2192 - predict_loss: 0.0000e+00 - loss_loss: 1.2192 - val_loss: 0.6138 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.6138\n",
      "Epoch 605/1500\n",
      "204/204 [==============================] - 0s 700us/step - loss: 0.8390 - predict_loss: 0.0000e+00 - loss_loss: 0.8390 - val_loss: 0.5317 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.5317\n",
      "Epoch 606/1500\n",
      "204/204 [==============================] - 0s 647us/step - loss: 0.9627 - predict_loss: 0.0000e+00 - loss_loss: 0.9627 - val_loss: 0.9026 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.9026\n",
      "Epoch 607/1500\n",
      "204/204 [==============================] - 0s 600us/step - loss: 1.0460 - predict_loss: 0.0000e+00 - loss_loss: 1.0460 - val_loss: 0.5569 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.5569\n",
      "Epoch 608/1500\n",
      "204/204 [==============================] - 0s 705us/step - loss: 0.7296 - predict_loss: 0.0000e+00 - loss_loss: 0.7296 - val_loss: 0.5539 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.5539\n",
      "Epoch 609/1500\n",
      "204/204 [==============================] - 0s 593us/step - loss: 1.0480 - predict_loss: 0.0000e+00 - loss_loss: 1.0480 - val_loss: 0.5928 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.5928\n",
      "Epoch 610/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "204/204 [==============================] - 0s 666us/step - loss: 0.9752 - predict_loss: 0.0000e+00 - loss_loss: 0.9752 - val_loss: 0.5530 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.5530\n",
      "Epoch 611/1500\n",
      "204/204 [==============================] - 0s 623us/step - loss: 1.0185 - predict_loss: 0.0000e+00 - loss_loss: 1.0185 - val_loss: 0.6405 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.6405\n",
      "Epoch 612/1500\n",
      "204/204 [==============================] - 0s 606us/step - loss: 1.0947 - predict_loss: 0.0000e+00 - loss_loss: 1.0947 - val_loss: 0.5197 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.5197\n",
      "Epoch 613/1500\n",
      "204/204 [==============================] - 0s 586us/step - loss: 1.0808 - predict_loss: 0.0000e+00 - loss_loss: 1.0808 - val_loss: 0.5272 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.5272\n",
      "Epoch 614/1500\n",
      "204/204 [==============================] - 0s 531us/step - loss: 0.8075 - predict_loss: 0.0000e+00 - loss_loss: 0.8075 - val_loss: 0.5528 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.5528\n",
      "Epoch 615/1500\n",
      "204/204 [==============================] - 0s 579us/step - loss: 0.7893 - predict_loss: 0.0000e+00 - loss_loss: 0.7893 - val_loss: 0.5631 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.5631\n",
      "Epoch 616/1500\n",
      "204/204 [==============================] - 0s 566us/step - loss: 1.1390 - predict_loss: 0.0000e+00 - loss_loss: 1.1390 - val_loss: 0.7033 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.7033\n",
      "Epoch 617/1500\n",
      "204/204 [==============================] - 0s 541us/step - loss: 1.1989 - predict_loss: 0.0000e+00 - loss_loss: 1.1989 - val_loss: 0.6741 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.6741\n",
      "Epoch 618/1500\n",
      "204/204 [==============================] - 0s 621us/step - loss: 0.7508 - predict_loss: 0.0000e+00 - loss_loss: 0.7508 - val_loss: 0.6673 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.6673\n",
      "Epoch 619/1500\n",
      "204/204 [==============================] - 0s 591us/step - loss: 1.0143 - predict_loss: 0.0000e+00 - loss_loss: 1.0143 - val_loss: 0.5427 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.5427\n",
      "Epoch 620/1500\n",
      "204/204 [==============================] - 0s 610us/step - loss: 1.0488 - predict_loss: 0.0000e+00 - loss_loss: 1.0488 - val_loss: 0.5016 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.5016\n",
      "Epoch 621/1500\n",
      "204/204 [==============================] - 0s 620us/step - loss: 1.2255 - predict_loss: 0.0000e+00 - loss_loss: 1.2255 - val_loss: 0.5506 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.5506\n",
      "Epoch 622/1500\n",
      "204/204 [==============================] - 0s 598us/step - loss: 1.0310 - predict_loss: 0.0000e+00 - loss_loss: 1.0310 - val_loss: 0.5332 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.5332\n",
      "Epoch 623/1500\n",
      "204/204 [==============================] - 0s 632us/step - loss: 0.9757 - predict_loss: 0.0000e+00 - loss_loss: 0.9757 - val_loss: 0.6434 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.6434\n",
      "Epoch 624/1500\n",
      "204/204 [==============================] - 0s 656us/step - loss: 0.8932 - predict_loss: 0.0000e+00 - loss_loss: 0.8932 - val_loss: 0.6120 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.6120\n",
      "Epoch 625/1500\n",
      "204/204 [==============================] - 0s 585us/step - loss: 1.0742 - predict_loss: 0.0000e+00 - loss_loss: 1.0742 - val_loss: 0.4921 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.4921\n",
      "Epoch 626/1500\n",
      "204/204 [==============================] - 0s 624us/step - loss: 0.7922 - predict_loss: 0.0000e+00 - loss_loss: 0.7922 - val_loss: 0.5745 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.5745\n",
      "Epoch 627/1500\n",
      "204/204 [==============================] - 0s 588us/step - loss: 1.1008 - predict_loss: 0.0000e+00 - loss_loss: 1.1008 - val_loss: 0.5278 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.5278\n",
      "Epoch 628/1500\n",
      "204/204 [==============================] - 0s 581us/step - loss: 0.8387 - predict_loss: 0.0000e+00 - loss_loss: 0.8387 - val_loss: 0.7138 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.7138\n",
      "Epoch 629/1500\n",
      "204/204 [==============================] - 0s 623us/step - loss: 0.9145 - predict_loss: 0.0000e+00 - loss_loss: 0.9145 - val_loss: 0.5885 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.5885\n",
      "Epoch 630/1500\n",
      "204/204 [==============================] - 0s 585us/step - loss: 1.0882 - predict_loss: 0.0000e+00 - loss_loss: 1.0882 - val_loss: 0.4545 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.4545\n",
      "Epoch 631/1500\n",
      "204/204 [==============================] - 0s 659us/step - loss: 0.8684 - predict_loss: 0.0000e+00 - loss_loss: 0.8684 - val_loss: 0.4895 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.4895\n",
      "Epoch 632/1500\n",
      "204/204 [==============================] - 0s 619us/step - loss: 0.8732 - predict_loss: 0.0000e+00 - loss_loss: 0.8732 - val_loss: 0.7112 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.7112\n",
      "Epoch 633/1500\n",
      "204/204 [==============================] - 0s 666us/step - loss: 1.0845 - predict_loss: 0.0000e+00 - loss_loss: 1.0845 - val_loss: 0.5505 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.5505\n",
      "Epoch 634/1500\n",
      "204/204 [==============================] - 0s 692us/step - loss: 0.7827 - predict_loss: 0.0000e+00 - loss_loss: 0.7827 - val_loss: 0.5339 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.5339\n",
      "Epoch 635/1500\n",
      "204/204 [==============================] - 0s 602us/step - loss: 0.9505 - predict_loss: 0.0000e+00 - loss_loss: 0.9505 - val_loss: 0.6205 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.6205\n",
      "Epoch 636/1500\n",
      "204/204 [==============================] - 0s 623us/step - loss: 1.0893 - predict_loss: 0.0000e+00 - loss_loss: 1.0893 - val_loss: 0.5450 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.5450\n",
      "Epoch 637/1500\n",
      "204/204 [==============================] - 0s 638us/step - loss: 0.7424 - predict_loss: 0.0000e+00 - loss_loss: 0.7424 - val_loss: 0.6974 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.6974\n",
      "Epoch 638/1500\n",
      "204/204 [==============================] - 0s 697us/step - loss: 1.1092 - predict_loss: 0.0000e+00 - loss_loss: 1.1092 - val_loss: 0.5062 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.5062\n",
      "Epoch 639/1500\n",
      "204/204 [==============================] - 0s 641us/step - loss: 0.7978 - predict_loss: 0.0000e+00 - loss_loss: 0.7978 - val_loss: 0.6733 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.6733\n",
      "Epoch 640/1500\n",
      "204/204 [==============================] - 0s 597us/step - loss: 0.9629 - predict_loss: 0.0000e+00 - loss_loss: 0.9629 - val_loss: 0.6680 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.6680\n",
      "Epoch 641/1500\n",
      "204/204 [==============================] - 0s 610us/step - loss: 1.1769 - predict_loss: 0.0000e+00 - loss_loss: 1.1769 - val_loss: 0.5298 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.5298\n",
      "Epoch 642/1500\n",
      "204/204 [==============================] - 0s 607us/step - loss: 0.7961 - predict_loss: 0.0000e+00 - loss_loss: 0.7961 - val_loss: 0.5174 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.5174\n",
      "Epoch 643/1500\n",
      "204/204 [==============================] - 0s 617us/step - loss: 0.5612 - predict_loss: 0.0000e+00 - loss_loss: 0.5612 - val_loss: 0.4906 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.4906\n",
      "Epoch 644/1500\n",
      "204/204 [==============================] - 0s 587us/step - loss: 0.5349 - predict_loss: 0.0000e+00 - loss_loss: 0.5349 - val_loss: 0.5223 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.5223\n",
      "Epoch 645/1500\n",
      "204/204 [==============================] - 0s 630us/step - loss: 0.6039 - predict_loss: 0.0000e+00 - loss_loss: 0.6039 - val_loss: 0.4944 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.4944\n",
      "Epoch 646/1500\n",
      "204/204 [==============================] - 0s 606us/step - loss: 0.6217 - predict_loss: 0.0000e+00 - loss_loss: 0.6217 - val_loss: 0.4571 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.4571\n",
      "Epoch 647/1500\n",
      "204/204 [==============================] - 0s 683us/step - loss: 0.6298 - predict_loss: 0.0000e+00 - loss_loss: 0.6298 - val_loss: 0.5098 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.5098\n",
      "Epoch 648/1500\n",
      "204/204 [==============================] - 0s 643us/step - loss: 0.5456 - predict_loss: 0.0000e+00 - loss_loss: 0.5456 - val_loss: 0.5224 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.5224\n",
      "Epoch 649/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "204/204 [==============================] - 0s 633us/step - loss: 0.6922 - predict_loss: 0.0000e+00 - loss_loss: 0.6922 - val_loss: 0.4852 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.4852\n",
      "Epoch 650/1500\n",
      "204/204 [==============================] - 0s 602us/step - loss: 0.5833 - predict_loss: 0.0000e+00 - loss_loss: 0.5833 - val_loss: 0.6009 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.6009\n",
      "Epoch 651/1500\n",
      "204/204 [==============================] - 0s 534us/step - loss: 0.5647 - predict_loss: 0.0000e+00 - loss_loss: 0.5647 - val_loss: 0.4706 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.4706\n",
      "Epoch 652/1500\n",
      "204/204 [==============================] - 0s 527us/step - loss: 0.5514 - predict_loss: 0.0000e+00 - loss_loss: 0.5514 - val_loss: 0.4412 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.4412\n",
      "Epoch 653/1500\n",
      "204/204 [==============================] - 0s 632us/step - loss: 0.5721 - predict_loss: 0.0000e+00 - loss_loss: 0.5721 - val_loss: 0.5113 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.5113\n",
      "Epoch 654/1500\n",
      "204/204 [==============================] - 0s 597us/step - loss: 0.6331 - predict_loss: 0.0000e+00 - loss_loss: 0.6331 - val_loss: 0.4733 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.4733\n",
      "Epoch 655/1500\n",
      "204/204 [==============================] - 0s 651us/step - loss: 0.5795 - predict_loss: 0.0000e+00 - loss_loss: 0.5795 - val_loss: 0.4991 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.4991\n",
      "Epoch 656/1500\n",
      "204/204 [==============================] - 0s 612us/step - loss: 0.5372 - predict_loss: 0.0000e+00 - loss_loss: 0.5372 - val_loss: 0.5056 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.5056\n",
      "Epoch 657/1500\n",
      "204/204 [==============================] - 0s 624us/step - loss: 0.6146 - predict_loss: 0.0000e+00 - loss_loss: 0.6146 - val_loss: 0.4777 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.4777\n",
      "Epoch 658/1500\n",
      "204/204 [==============================] - 0s 607us/step - loss: 0.5344 - predict_loss: 0.0000e+00 - loss_loss: 0.5344 - val_loss: 0.5144 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.5144\n",
      "Epoch 659/1500\n",
      "204/204 [==============================] - 0s 634us/step - loss: 0.6466 - predict_loss: 0.0000e+00 - loss_loss: 0.6466 - val_loss: 0.4866 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.4866\n",
      "Epoch 660/1500\n",
      "204/204 [==============================] - 0s 630us/step - loss: 0.5981 - predict_loss: 0.0000e+00 - loss_loss: 0.5981 - val_loss: 0.4289 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.4289\n",
      "Epoch 661/1500\n",
      "204/204 [==============================] - 0s 607us/step - loss: 0.5688 - predict_loss: 0.0000e+00 - loss_loss: 0.5688 - val_loss: 0.4488 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.4488\n",
      "Epoch 662/1500\n",
      "204/204 [==============================] - 0s 603us/step - loss: 0.5410 - predict_loss: 0.0000e+00 - loss_loss: 0.5410 - val_loss: 0.4816 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.4816\n",
      "Epoch 663/1500\n",
      "204/204 [==============================] - 0s 580us/step - loss: 0.6632 - predict_loss: 0.0000e+00 - loss_loss: 0.6632 - val_loss: 0.4724 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.4724\n",
      "Epoch 664/1500\n",
      "204/204 [==============================] - 0s 580us/step - loss: 0.5240 - predict_loss: 0.0000e+00 - loss_loss: 0.5240 - val_loss: 0.4926 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.4926\n",
      "Epoch 665/1500\n",
      "204/204 [==============================] - 0s 613us/step - loss: 0.6226 - predict_loss: 0.0000e+00 - loss_loss: 0.6226 - val_loss: 0.4675 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.4675\n",
      "Epoch 666/1500\n",
      "204/204 [==============================] - 0s 550us/step - loss: 0.4497 - predict_loss: 0.0000e+00 - loss_loss: 0.4497 - val_loss: 0.4508 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.4508\n",
      "Epoch 667/1500\n",
      "204/204 [==============================] - 0s 664us/step - loss: 0.5459 - predict_loss: 0.0000e+00 - loss_loss: 0.5459 - val_loss: 0.5626 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.5626\n",
      "Epoch 668/1500\n",
      "204/204 [==============================] - 0s 645us/step - loss: 0.6322 - predict_loss: 0.0000e+00 - loss_loss: 0.6322 - val_loss: 0.4565 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.4565\n",
      "Epoch 669/1500\n",
      "204/204 [==============================] - 0s 655us/step - loss: 0.5821 - predict_loss: 0.0000e+00 - loss_loss: 0.5821 - val_loss: 0.4549 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.4549\n",
      "Epoch 670/1500\n",
      "204/204 [==============================] - 0s 586us/step - loss: 0.5650 - predict_loss: 0.0000e+00 - loss_loss: 0.5650 - val_loss: 0.4446 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.4446\n",
      "Epoch 671/1500\n",
      "204/204 [==============================] - 0s 608us/step - loss: 0.5004 - predict_loss: 0.0000e+00 - loss_loss: 0.5004 - val_loss: 0.4448 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.4448\n",
      "Epoch 672/1500\n",
      "204/204 [==============================] - 0s 562us/step - loss: 0.5715 - predict_loss: 0.0000e+00 - loss_loss: 0.5715 - val_loss: 0.4243 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.4243\n",
      "Epoch 673/1500\n",
      "204/204 [==============================] - 0s 605us/step - loss: 0.4939 - predict_loss: 0.0000e+00 - loss_loss: 0.4939 - val_loss: 0.4907 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.4907\n",
      "Epoch 674/1500\n",
      "204/204 [==============================] - 0s 596us/step - loss: 0.5937 - predict_loss: 0.0000e+00 - loss_loss: 0.5937 - val_loss: 0.4465 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.4465\n",
      "Epoch 675/1500\n",
      "204/204 [==============================] - 0s 582us/step - loss: 0.5181 - predict_loss: 0.0000e+00 - loss_loss: 0.5181 - val_loss: 0.5164 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.5164\n",
      "Epoch 676/1500\n",
      "204/204 [==============================] - 0s 673us/step - loss: 0.5691 - predict_loss: 0.0000e+00 - loss_loss: 0.5691 - val_loss: 0.4344 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.4344\n",
      "Epoch 677/1500\n",
      "204/204 [==============================] - 0s 642us/step - loss: 0.5794 - predict_loss: 0.0000e+00 - loss_loss: 0.5794 - val_loss: 0.4370 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.4370\n",
      "Epoch 678/1500\n",
      "204/204 [==============================] - 0s 570us/step - loss: 0.5600 - predict_loss: 0.0000e+00 - loss_loss: 0.5600 - val_loss: 0.4204 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.4204\n",
      "Epoch 679/1500\n",
      "204/204 [==============================] - 0s 619us/step - loss: 0.4705 - predict_loss: 0.0000e+00 - loss_loss: 0.4705 - val_loss: 0.4807 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.4807\n",
      "Epoch 680/1500\n",
      "204/204 [==============================] - 0s 647us/step - loss: 0.5842 - predict_loss: 0.0000e+00 - loss_loss: 0.5842 - val_loss: 0.4185 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.4185\n",
      "Epoch 681/1500\n",
      "204/204 [==============================] - 0s 613us/step - loss: 0.5083 - predict_loss: 0.0000e+00 - loss_loss: 0.5083 - val_loss: 0.4538 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.4538\n",
      "Epoch 682/1500\n",
      "204/204 [==============================] - 0s 675us/step - loss: 0.6080 - predict_loss: 0.0000e+00 - loss_loss: 0.6080 - val_loss: 0.3893 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3893\n",
      "Epoch 683/1500\n",
      "204/204 [==============================] - 0s 711us/step - loss: 0.4529 - predict_loss: 0.0000e+00 - loss_loss: 0.4529 - val_loss: 0.4360 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.4360\n",
      "Epoch 684/1500\n",
      "204/204 [==============================] - 0s 678us/step - loss: 0.5580 - predict_loss: 0.0000e+00 - loss_loss: 0.5580 - val_loss: 0.3916 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3916\n",
      "Epoch 685/1500\n",
      "204/204 [==============================] - 0s 620us/step - loss: 0.5999 - predict_loss: 0.0000e+00 - loss_loss: 0.5999 - val_loss: 0.4916 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.4916\n",
      "Epoch 686/1500\n",
      "204/204 [==============================] - 0s 634us/step - loss: 0.4403 - predict_loss: 0.0000e+00 - loss_loss: 0.4403 - val_loss: 0.4677 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.4677\n",
      "Epoch 687/1500\n",
      "204/204 [==============================] - 0s 617us/step - loss: 0.5095 - predict_loss: 0.0000e+00 - loss_loss: 0.5095 - val_loss: 0.3969 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3969\n",
      "Epoch 688/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "204/204 [==============================] - 0s 648us/step - loss: 0.5333 - predict_loss: 0.0000e+00 - loss_loss: 0.5333 - val_loss: 0.4517 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.4517\n",
      "Epoch 689/1500\n",
      "204/204 [==============================] - 0s 620us/step - loss: 0.5478 - predict_loss: 0.0000e+00 - loss_loss: 0.5478 - val_loss: 0.4108 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.4108\n",
      "Epoch 690/1500\n",
      "204/204 [==============================] - 0s 603us/step - loss: 0.6026 - predict_loss: 0.0000e+00 - loss_loss: 0.6026 - val_loss: 0.4377 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.4377\n",
      "Epoch 691/1500\n",
      "204/204 [==============================] - 0s 511us/step - loss: 0.4700 - predict_loss: 0.0000e+00 - loss_loss: 0.4700 - val_loss: 0.4301 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.4301\n",
      "Epoch 692/1500\n",
      "204/204 [==============================] - 0s 476us/step - loss: 0.4633 - predict_loss: 0.0000e+00 - loss_loss: 0.4633 - val_loss: 0.3919 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3919\n",
      "Epoch 693/1500\n",
      "204/204 [==============================] - 0s 613us/step - loss: 0.5112 - predict_loss: 0.0000e+00 - loss_loss: 0.5112 - val_loss: 0.4082 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.4082\n",
      "Epoch 694/1500\n",
      "204/204 [==============================] - 0s 520us/step - loss: 0.6548 - predict_loss: 0.0000e+00 - loss_loss: 0.6548 - val_loss: 0.3858 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3858\n",
      "Epoch 695/1500\n",
      "204/204 [==============================] - 0s 549us/step - loss: 0.4467 - predict_loss: 0.0000e+00 - loss_loss: 0.4467 - val_loss: 0.4861 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.4861\n",
      "Epoch 696/1500\n",
      "204/204 [==============================] - 0s 590us/step - loss: 0.5602 - predict_loss: 0.0000e+00 - loss_loss: 0.5602 - val_loss: 0.4272 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.4272\n",
      "Epoch 697/1500\n",
      "204/204 [==============================] - 0s 628us/step - loss: 0.4601 - predict_loss: 0.0000e+00 - loss_loss: 0.4601 - val_loss: 0.3917 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3917\n",
      "Epoch 698/1500\n",
      "204/204 [==============================] - 0s 600us/step - loss: 0.5494 - predict_loss: 0.0000e+00 - loss_loss: 0.5494 - val_loss: 0.4103 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.4103\n",
      "Epoch 699/1500\n",
      "204/204 [==============================] - 0s 606us/step - loss: 0.4309 - predict_loss: 0.0000e+00 - loss_loss: 0.4309 - val_loss: 0.4391 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.4391\n",
      "Epoch 700/1500\n",
      "204/204 [==============================] - 0s 682us/step - loss: 0.5138 - predict_loss: 0.0000e+00 - loss_loss: 0.5138 - val_loss: 0.3803 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3803\n",
      "Epoch 701/1500\n",
      "204/204 [==============================] - 0s 613us/step - loss: 0.4316 - predict_loss: 0.0000e+00 - loss_loss: 0.4316 - val_loss: 0.4060 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.4060\n",
      "Epoch 702/1500\n",
      "204/204 [==============================] - 0s 662us/step - loss: 0.6219 - predict_loss: 0.0000e+00 - loss_loss: 0.6219 - val_loss: 0.4377 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.4377\n",
      "Epoch 703/1500\n",
      "204/204 [==============================] - 0s 625us/step - loss: 0.5266 - predict_loss: 0.0000e+00 - loss_loss: 0.5266 - val_loss: 0.5134 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.5134\n",
      "Epoch 704/1500\n",
      "204/204 [==============================] - 0s 594us/step - loss: 0.4474 - predict_loss: 0.0000e+00 - loss_loss: 0.4474 - val_loss: 0.4001 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.4001\n",
      "Epoch 705/1500\n",
      "204/204 [==============================] - 0s 613us/step - loss: 0.5449 - predict_loss: 0.0000e+00 - loss_loss: 0.5449 - val_loss: 0.3974 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3974\n",
      "Epoch 706/1500\n",
      "204/204 [==============================] - 0s 575us/step - loss: 0.4700 - predict_loss: 0.0000e+00 - loss_loss: 0.4700 - val_loss: 0.3877 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3877\n",
      "Epoch 707/1500\n",
      "204/204 [==============================] - 0s 603us/step - loss: 0.5726 - predict_loss: 0.0000e+00 - loss_loss: 0.5726 - val_loss: 0.3906 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3906\n",
      "Epoch 708/1500\n",
      "204/204 [==============================] - 0s 670us/step - loss: 0.4383 - predict_loss: 0.0000e+00 - loss_loss: 0.4383 - val_loss: 0.4063 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.4063\n",
      "Epoch 709/1500\n",
      "204/204 [==============================] - 0s 619us/step - loss: 0.5384 - predict_loss: 0.0000e+00 - loss_loss: 0.5384 - val_loss: 0.3899 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3899\n",
      "Epoch 710/1500\n",
      "204/204 [==============================] - 0s 591us/step - loss: 0.3833 - predict_loss: 0.0000e+00 - loss_loss: 0.3833 - val_loss: 0.4307 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.4307\n",
      "Epoch 711/1500\n",
      "204/204 [==============================] - 0s 588us/step - loss: 0.6259 - predict_loss: 0.0000e+00 - loss_loss: 0.6259 - val_loss: 0.3675 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3675\n",
      "Epoch 712/1500\n",
      "204/204 [==============================] - 0s 621us/step - loss: 0.4712 - predict_loss: 0.0000e+00 - loss_loss: 0.4712 - val_loss: 0.4352 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.4352\n",
      "Epoch 713/1500\n",
      "204/204 [==============================] - 0s 575us/step - loss: 0.5199 - predict_loss: 0.0000e+00 - loss_loss: 0.5199 - val_loss: 0.3952 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3952\n",
      "Epoch 714/1500\n",
      "204/204 [==============================] - 0s 580us/step - loss: 0.4074 - predict_loss: 0.0000e+00 - loss_loss: 0.4074 - val_loss: 0.4169 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.4169\n",
      "Epoch 715/1500\n",
      "204/204 [==============================] - 0s 639us/step - loss: 0.6618 - predict_loss: 0.0000e+00 - loss_loss: 0.6618 - val_loss: 0.3721 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3721\n",
      "Epoch 716/1500\n",
      "204/204 [==============================] - 0s 599us/step - loss: 0.3782 - predict_loss: 0.0000e+00 - loss_loss: 0.3782 - val_loss: 0.4165 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.4165\n",
      "Epoch 717/1500\n",
      "204/204 [==============================] - 0s 607us/step - loss: 0.5460 - predict_loss: 0.0000e+00 - loss_loss: 0.5460 - val_loss: 0.3805 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3805\n",
      "Epoch 718/1500\n",
      "204/204 [==============================] - 0s 656us/step - loss: 0.4453 - predict_loss: 0.0000e+00 - loss_loss: 0.4453 - val_loss: 0.5114 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.5114\n",
      "Epoch 719/1500\n",
      "204/204 [==============================] - 0s 622us/step - loss: 0.4459 - predict_loss: 0.0000e+00 - loss_loss: 0.4459 - val_loss: 0.4504 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.4504\n",
      "Epoch 720/1500\n",
      "204/204 [==============================] - 0s 543us/step - loss: 0.5298 - predict_loss: 0.0000e+00 - loss_loss: 0.5298 - val_loss: 0.3496 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3496\n",
      "Epoch 721/1500\n",
      "204/204 [==============================] - 0s 648us/step - loss: 0.5119 - predict_loss: 0.0000e+00 - loss_loss: 0.5119 - val_loss: 0.3804 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3804\n",
      "Epoch 722/1500\n",
      "204/204 [==============================] - 0s 617us/step - loss: 0.4020 - predict_loss: 0.0000e+00 - loss_loss: 0.4020 - val_loss: 0.3692 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3692\n",
      "Epoch 723/1500\n",
      "204/204 [==============================] - 0s 518us/step - loss: 0.5482 - predict_loss: 0.0000e+00 - loss_loss: 0.5482 - val_loss: 0.3634 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3634\n",
      "Epoch 724/1500\n",
      "204/204 [==============================] - 0s 652us/step - loss: 0.5620 - predict_loss: 0.0000e+00 - loss_loss: 0.5620 - val_loss: 0.3890 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3890\n",
      "Epoch 725/1500\n",
      "204/204 [==============================] - 0s 603us/step - loss: 0.3579 - predict_loss: 0.0000e+00 - loss_loss: 0.3579 - val_loss: 0.3783 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3783\n",
      "Epoch 726/1500\n",
      "204/204 [==============================] - 0s 528us/step - loss: 0.6097 - predict_loss: 0.0000e+00 - loss_loss: 0.6097 - val_loss: 0.3794 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3794\n",
      "Epoch 727/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "204/204 [==============================] - 0s 562us/step - loss: 0.5316 - predict_loss: 0.0000e+00 - loss_loss: 0.5316 - val_loss: 0.3488 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3488\n",
      "Epoch 728/1500\n",
      "204/204 [==============================] - 0s 616us/step - loss: 0.4723 - predict_loss: 0.0000e+00 - loss_loss: 0.4723 - val_loss: 0.3725 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3725\n",
      "Epoch 729/1500\n",
      "204/204 [==============================] - 0s 536us/step - loss: 0.4940 - predict_loss: 0.0000e+00 - loss_loss: 0.4940 - val_loss: 0.3356 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3356\n",
      "Epoch 730/1500\n",
      "204/204 [==============================] - 0s 563us/step - loss: 0.4168 - predict_loss: 0.0000e+00 - loss_loss: 0.4168 - val_loss: 0.4002 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.4002\n",
      "Epoch 731/1500\n",
      "204/204 [==============================] - 0s 606us/step - loss: 0.5216 - predict_loss: 0.0000e+00 - loss_loss: 0.5216 - val_loss: 0.3627 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3627\n",
      "Epoch 732/1500\n",
      "204/204 [==============================] - 0s 588us/step - loss: 0.4857 - predict_loss: 0.0000e+00 - loss_loss: 0.4857 - val_loss: 0.3507 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3507\n",
      "Epoch 733/1500\n",
      "204/204 [==============================] - 0s 635us/step - loss: 0.4928 - predict_loss: 0.0000e+00 - loss_loss: 0.4928 - val_loss: 0.3904 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3904\n",
      "Epoch 734/1500\n",
      "204/204 [==============================] - 0s 564us/step - loss: 0.5003 - predict_loss: 0.0000e+00 - loss_loss: 0.5003 - val_loss: 0.3646 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3646\n",
      "Epoch 735/1500\n",
      "204/204 [==============================] - 0s 544us/step - loss: 0.4541 - predict_loss: 0.0000e+00 - loss_loss: 0.4541 - val_loss: 0.3656 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3656\n",
      "Epoch 736/1500\n",
      "204/204 [==============================] - 0s 649us/step - loss: 0.4772 - predict_loss: 0.0000e+00 - loss_loss: 0.4772 - val_loss: 0.3540 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3540\n",
      "Epoch 737/1500\n",
      "204/204 [==============================] - 0s 624us/step - loss: 0.4515 - predict_loss: 0.0000e+00 - loss_loss: 0.4515 - val_loss: 0.3811 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3811\n",
      "Epoch 738/1500\n",
      "204/204 [==============================] - 0s 556us/step - loss: 0.5022 - predict_loss: 0.0000e+00 - loss_loss: 0.5022 - val_loss: 0.3588 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3588\n",
      "Epoch 739/1500\n",
      "204/204 [==============================] - 0s 548us/step - loss: 0.4340 - predict_loss: 0.0000e+00 - loss_loss: 0.4340 - val_loss: 0.3811 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3811\n",
      "Epoch 740/1500\n",
      "204/204 [==============================] - 0s 588us/step - loss: 0.5722 - predict_loss: 0.0000e+00 - loss_loss: 0.5722 - val_loss: 0.3669 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3669\n",
      "Epoch 741/1500\n",
      "204/204 [==============================] - 0s 646us/step - loss: 0.4238 - predict_loss: 0.0000e+00 - loss_loss: 0.4238 - val_loss: 0.3452 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3452\n",
      "Epoch 742/1500\n",
      "204/204 [==============================] - 0s 508us/step - loss: 0.4479 - predict_loss: 0.0000e+00 - loss_loss: 0.4479 - val_loss: 0.3651 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3651\n",
      "Epoch 743/1500\n",
      "204/204 [==============================] - 0s 537us/step - loss: 0.3730 - predict_loss: 0.0000e+00 - loss_loss: 0.3730 - val_loss: 0.3981 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3981\n",
      "Epoch 744/1500\n",
      "204/204 [==============================] - 0s 596us/step - loss: 0.4881 - predict_loss: 0.0000e+00 - loss_loss: 0.4881 - val_loss: 0.3479 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3479\n",
      "Epoch 745/1500\n",
      "204/204 [==============================] - 0s 614us/step - loss: 0.5220 - predict_loss: 0.0000e+00 - loss_loss: 0.5220 - val_loss: 0.3417 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3417\n",
      "Epoch 746/1500\n",
      "204/204 [==============================] - 0s 730us/step - loss: 0.4921 - predict_loss: 0.0000e+00 - loss_loss: 0.4921 - val_loss: 0.3998 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3998\n",
      "Epoch 747/1500\n",
      "204/204 [==============================] - 0s 624us/step - loss: 0.3984 - predict_loss: 0.0000e+00 - loss_loss: 0.3984 - val_loss: 0.3839 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3839\n",
      "Epoch 748/1500\n",
      "204/204 [==============================] - 0s 591us/step - loss: 0.5005 - predict_loss: 0.0000e+00 - loss_loss: 0.5005 - val_loss: 0.4527 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.4527\n",
      "Epoch 749/1500\n",
      "204/204 [==============================] - 0s 604us/step - loss: 0.5137 - predict_loss: 0.0000e+00 - loss_loss: 0.5137 - val_loss: 0.4145 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.4145\n",
      "Epoch 750/1500\n",
      "204/204 [==============================] - 0s 623us/step - loss: 0.4296 - predict_loss: 0.0000e+00 - loss_loss: 0.4296 - val_loss: 0.3549 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3549\n",
      "Epoch 751/1500\n",
      "204/204 [==============================] - 0s 579us/step - loss: 0.4889 - predict_loss: 0.0000e+00 - loss_loss: 0.4889 - val_loss: 0.3996 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3996\n",
      "Epoch 752/1500\n",
      "204/204 [==============================] - 0s 626us/step - loss: 0.4649 - predict_loss: 0.0000e+00 - loss_loss: 0.4649 - val_loss: 0.3670 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3670\n",
      "Epoch 753/1500\n",
      "204/204 [==============================] - 0s 634us/step - loss: 0.6067 - predict_loss: 0.0000e+00 - loss_loss: 0.6067 - val_loss: 0.3669 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3669\n",
      "Epoch 754/1500\n",
      "204/204 [==============================] - 0s 616us/step - loss: 0.3591 - predict_loss: 0.0000e+00 - loss_loss: 0.3591 - val_loss: 0.4033 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.4033\n",
      "Epoch 755/1500\n",
      "204/204 [==============================] - 0s 587us/step - loss: 0.5562 - predict_loss: 0.0000e+00 - loss_loss: 0.5562 - val_loss: 0.3931 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3931\n",
      "Epoch 756/1500\n",
      "204/204 [==============================] - 0s 598us/step - loss: 0.3848 - predict_loss: 0.0000e+00 - loss_loss: 0.3848 - val_loss: 0.3966 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3966\n",
      "Epoch 757/1500\n",
      "204/204 [==============================] - 0s 591us/step - loss: 0.5104 - predict_loss: 0.0000e+00 - loss_loss: 0.5104 - val_loss: 0.3421 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3421\n",
      "Epoch 758/1500\n",
      "204/204 [==============================] - 0s 562us/step - loss: 0.4036 - predict_loss: 0.0000e+00 - loss_loss: 0.4036 - val_loss: 0.3720 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3720\n",
      "Epoch 759/1500\n",
      "204/204 [==============================] - 0s 614us/step - loss: 0.5076 - predict_loss: 0.0000e+00 - loss_loss: 0.5076 - val_loss: 0.3496 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3496\n",
      "Epoch 760/1500\n",
      "204/204 [==============================] - 0s 685us/step - loss: 0.4702 - predict_loss: 0.0000e+00 - loss_loss: 0.4702 - val_loss: 0.3386 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3386\n",
      "Epoch 761/1500\n",
      "204/204 [==============================] - 0s 667us/step - loss: 0.4381 - predict_loss: 0.0000e+00 - loss_loss: 0.4381 - val_loss: 0.3859 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3859\n",
      "Epoch 762/1500\n",
      "204/204 [==============================] - 0s 660us/step - loss: 0.5130 - predict_loss: 0.0000e+00 - loss_loss: 0.5130 - val_loss: 0.3547 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3547\n",
      "Epoch 763/1500\n",
      "204/204 [==============================] - 0s 648us/step - loss: 0.4781 - predict_loss: 0.0000e+00 - loss_loss: 0.4781 - val_loss: 0.3459 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3459\n",
      "Epoch 764/1500\n",
      "204/204 [==============================] - 0s 633us/step - loss: 0.3418 - predict_loss: 0.0000e+00 - loss_loss: 0.3418 - val_loss: 0.3627 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3627\n",
      "Epoch 765/1500\n",
      "204/204 [==============================] - 0s 651us/step - loss: 0.5860 - predict_loss: 0.0000e+00 - loss_loss: 0.5860 - val_loss: 0.3343 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3343\n",
      "Epoch 766/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "204/204 [==============================] - 0s 709us/step - loss: 0.4581 - predict_loss: 0.0000e+00 - loss_loss: 0.4581 - val_loss: 0.3831 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3831\n",
      "Epoch 767/1500\n",
      "204/204 [==============================] - 0s 608us/step - loss: 0.3917 - predict_loss: 0.0000e+00 - loss_loss: 0.3917 - val_loss: 0.3573 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3573\n",
      "Epoch 768/1500\n",
      "204/204 [==============================] - 0s 519us/step - loss: 0.4959 - predict_loss: 0.0000e+00 - loss_loss: 0.4959 - val_loss: 0.3865 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3865\n",
      "Epoch 769/1500\n",
      "204/204 [==============================] - 0s 546us/step - loss: 0.5062 - predict_loss: 0.0000e+00 - loss_loss: 0.5062 - val_loss: 0.3715 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3715\n",
      "Epoch 770/1500\n",
      "204/204 [==============================] - 0s 651us/step - loss: 0.4268 - predict_loss: 0.0000e+00 - loss_loss: 0.4268 - val_loss: 0.3326 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3326\n",
      "Epoch 771/1500\n",
      "204/204 [==============================] - 0s 664us/step - loss: 0.4564 - predict_loss: 0.0000e+00 - loss_loss: 0.4564 - val_loss: 0.3669 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3669\n",
      "Epoch 772/1500\n",
      "204/204 [==============================] - 0s 729us/step - loss: 0.3477 - predict_loss: 0.0000e+00 - loss_loss: 0.3477 - val_loss: 0.3432 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3432\n",
      "Epoch 773/1500\n",
      "204/204 [==============================] - 0s 575us/step - loss: 0.5887 - predict_loss: 0.0000e+00 - loss_loss: 0.5887 - val_loss: 0.4010 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.4010\n",
      "Epoch 774/1500\n",
      "204/204 [==============================] - 0s 650us/step - loss: 0.4422 - predict_loss: 0.0000e+00 - loss_loss: 0.4422 - val_loss: 0.3707 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3707\n",
      "Epoch 775/1500\n",
      "204/204 [==============================] - 0s 608us/step - loss: 0.4334 - predict_loss: 0.0000e+00 - loss_loss: 0.4334 - val_loss: 0.3538 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3538\n",
      "Epoch 776/1500\n",
      "204/204 [==============================] - 0s 624us/step - loss: 0.4369 - predict_loss: 0.0000e+00 - loss_loss: 0.4369 - val_loss: 0.3485 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3485\n",
      "Epoch 777/1500\n",
      "204/204 [==============================] - 0s 633us/step - loss: 0.5045 - predict_loss: 0.0000e+00 - loss_loss: 0.5045 - val_loss: 0.3275 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3275\n",
      "Epoch 778/1500\n",
      "204/204 [==============================] - 0s 651us/step - loss: 0.4012 - predict_loss: 0.0000e+00 - loss_loss: 0.4012 - val_loss: 0.3368 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3368\n",
      "Epoch 779/1500\n",
      "204/204 [==============================] - 0s 622us/step - loss: 0.4683 - predict_loss: 0.0000e+00 - loss_loss: 0.4683 - val_loss: 0.3678 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3678\n",
      "Epoch 780/1500\n",
      "204/204 [==============================] - 0s 611us/step - loss: 0.5026 - predict_loss: 0.0000e+00 - loss_loss: 0.5026 - val_loss: 0.3274 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3274\n",
      "Epoch 781/1500\n",
      "204/204 [==============================] - 0s 755us/step - loss: 0.4253 - predict_loss: 0.0000e+00 - loss_loss: 0.4253 - val_loss: 0.3594 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3594\n",
      "Epoch 782/1500\n",
      "204/204 [==============================] - 0s 593us/step - loss: 0.4793 - predict_loss: 0.0000e+00 - loss_loss: 0.4793 - val_loss: 0.3189 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3189\n",
      "Epoch 783/1500\n",
      "204/204 [==============================] - 0s 603us/step - loss: 0.4142 - predict_loss: 0.0000e+00 - loss_loss: 0.4142 - val_loss: 0.3188 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3188\n",
      "Epoch 784/1500\n",
      "204/204 [==============================] - 0s 625us/step - loss: 0.5090 - predict_loss: 0.0000e+00 - loss_loss: 0.5090 - val_loss: 0.3704 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3704\n",
      "Epoch 785/1500\n",
      "204/204 [==============================] - 0s 506us/step - loss: 0.3836 - predict_loss: 0.0000e+00 - loss_loss: 0.3836 - val_loss: 0.3635 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3635\n",
      "Epoch 786/1500\n",
      "204/204 [==============================] - 0s 585us/step - loss: 0.5100 - predict_loss: 0.0000e+00 - loss_loss: 0.5100 - val_loss: 0.3429 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3429\n",
      "Epoch 787/1500\n",
      "204/204 [==============================] - 0s 702us/step - loss: 0.5155 - predict_loss: 0.0000e+00 - loss_loss: 0.5155 - val_loss: 0.3829 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3829\n",
      "Epoch 788/1500\n",
      "204/204 [==============================] - 0s 618us/step - loss: 0.3866 - predict_loss: 0.0000e+00 - loss_loss: 0.3866 - val_loss: 0.3450 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3450\n",
      "Epoch 789/1500\n",
      "204/204 [==============================] - 0s 562us/step - loss: 0.4901 - predict_loss: 0.0000e+00 - loss_loss: 0.4901 - val_loss: 0.3704 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3704\n",
      "Epoch 790/1500\n",
      "204/204 [==============================] - 0s 610us/step - loss: 0.3500 - predict_loss: 0.0000e+00 - loss_loss: 0.3500 - val_loss: 0.3626 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3626\n",
      "Epoch 791/1500\n",
      "204/204 [==============================] - 0s 660us/step - loss: 0.5400 - predict_loss: 0.0000e+00 - loss_loss: 0.5400 - val_loss: 0.3402 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3402\n",
      "Epoch 792/1500\n",
      "204/204 [==============================] - 0s 612us/step - loss: 0.4260 - predict_loss: 0.0000e+00 - loss_loss: 0.4260 - val_loss: 0.3917 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3917\n",
      "Epoch 793/1500\n",
      "204/204 [==============================] - 0s 587us/step - loss: 0.3740 - predict_loss: 0.0000e+00 - loss_loss: 0.3740 - val_loss: 0.3466 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3466\n",
      "Epoch 794/1500\n",
      "204/204 [==============================] - 0s 595us/step - loss: 0.5196 - predict_loss: 0.0000e+00 - loss_loss: 0.5196 - val_loss: 0.3841 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3841\n",
      "Epoch 795/1500\n",
      "204/204 [==============================] - 0s 593us/step - loss: 0.4221 - predict_loss: 0.0000e+00 - loss_loss: 0.4221 - val_loss: 0.3561 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3561\n",
      "Epoch 796/1500\n",
      "204/204 [==============================] - 0s 598us/step - loss: 0.4265 - predict_loss: 0.0000e+00 - loss_loss: 0.4265 - val_loss: 0.3230 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3230\n",
      "Epoch 797/1500\n",
      "204/204 [==============================] - 0s 623us/step - loss: 0.5448 - predict_loss: 0.0000e+00 - loss_loss: 0.5448 - val_loss: 0.3367 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3367\n",
      "Epoch 798/1500\n",
      "204/204 [==============================] - 0s 578us/step - loss: 0.3530 - predict_loss: 0.0000e+00 - loss_loss: 0.3530 - val_loss: 0.3517 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3517\n",
      "Epoch 799/1500\n",
      "204/204 [==============================] - 0s 601us/step - loss: 0.4898 - predict_loss: 0.0000e+00 - loss_loss: 0.4898 - val_loss: 0.3273 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3273\n",
      "Epoch 800/1500\n",
      "204/204 [==============================] - 0s 602us/step - loss: 0.4152 - predict_loss: 0.0000e+00 - loss_loss: 0.4152 - val_loss: 0.3438 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3438\n",
      "Epoch 801/1500\n",
      "204/204 [==============================] - 0s 618us/step - loss: 0.4854 - predict_loss: 0.0000e+00 - loss_loss: 0.4854 - val_loss: 0.3467 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3467\n",
      "Epoch 802/1500\n",
      "204/204 [==============================] - 0s 602us/step - loss: 0.3966 - predict_loss: 0.0000e+00 - loss_loss: 0.3966 - val_loss: 0.3309 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3309\n",
      "Epoch 803/1500\n",
      "204/204 [==============================] - 0s 573us/step - loss: 0.4315 - predict_loss: 0.0000e+00 - loss_loss: 0.4315 - val_loss: 0.3399 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3399\n",
      "Epoch 804/1500\n",
      "204/204 [==============================] - 0s 551us/step - loss: 0.4252 - predict_loss: 0.0000e+00 - loss_loss: 0.4252 - val_loss: 0.3568 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3568\n",
      "Epoch 805/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "204/204 [==============================] - 0s 608us/step - loss: 0.5332 - predict_loss: 0.0000e+00 - loss_loss: 0.5332 - val_loss: 0.3202 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3202\n",
      "Epoch 806/1500\n",
      "204/204 [==============================] - 0s 607us/step - loss: 0.3644 - predict_loss: 0.0000e+00 - loss_loss: 0.3644 - val_loss: 0.3616 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3616\n",
      "Epoch 807/1500\n",
      "204/204 [==============================] - 0s 596us/step - loss: 0.5271 - predict_loss: 0.0000e+00 - loss_loss: 0.5271 - val_loss: 0.3508 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3508\n",
      "Epoch 808/1500\n",
      "204/204 [==============================] - 0s 625us/step - loss: 0.4110 - predict_loss: 0.0000e+00 - loss_loss: 0.4110 - val_loss: 0.3769 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3769\n",
      "Epoch 809/1500\n",
      "204/204 [==============================] - 0s 557us/step - loss: 0.3733 - predict_loss: 0.0000e+00 - loss_loss: 0.3733 - val_loss: 0.3281 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3281\n",
      "Epoch 810/1500\n",
      "204/204 [==============================] - 0s 574us/step - loss: 0.4725 - predict_loss: 0.0000e+00 - loss_loss: 0.4725 - val_loss: 0.3457 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3457\n",
      "Epoch 811/1500\n",
      "204/204 [==============================] - 0s 566us/step - loss: 0.5458 - predict_loss: 0.0000e+00 - loss_loss: 0.5458 - val_loss: 0.3603 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3603\n",
      "Epoch 812/1500\n",
      "204/204 [==============================] - 0s 621us/step - loss: 0.4673 - predict_loss: 0.0000e+00 - loss_loss: 0.4673 - val_loss: 0.3209 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3209\n",
      "Epoch 813/1500\n",
      "204/204 [==============================] - 0s 602us/step - loss: 0.2996 - predict_loss: 0.0000e+00 - loss_loss: 0.2996 - val_loss: 0.4045 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.4045\n",
      "Epoch 814/1500\n",
      "204/204 [==============================] - 0s 574us/step - loss: 0.4883 - predict_loss: 0.0000e+00 - loss_loss: 0.4883 - val_loss: 0.3315 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3315\n",
      "Epoch 815/1500\n",
      "204/204 [==============================] - 0s 547us/step - loss: 0.3534 - predict_loss: 0.0000e+00 - loss_loss: 0.3534 - val_loss: 0.3297 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3297\n",
      "Epoch 816/1500\n",
      "204/204 [==============================] - 0s 598us/step - loss: 0.5304 - predict_loss: 0.0000e+00 - loss_loss: 0.5304 - val_loss: 0.3411 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3411\n",
      "Epoch 817/1500\n",
      "204/204 [==============================] - 0s 565us/step - loss: 0.4196 - predict_loss: 0.0000e+00 - loss_loss: 0.4196 - val_loss: 0.3311 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3311\n",
      "Epoch 818/1500\n",
      "204/204 [==============================] - 0s 600us/step - loss: 0.5010 - predict_loss: 0.0000e+00 - loss_loss: 0.5010 - val_loss: 0.3403 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3403\n",
      "Epoch 819/1500\n",
      "204/204 [==============================] - 0s 579us/step - loss: 0.3756 - predict_loss: 0.0000e+00 - loss_loss: 0.3756 - val_loss: 0.3072 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3072\n",
      "Epoch 820/1500\n",
      "204/204 [==============================] - 0s 601us/step - loss: 0.4786 - predict_loss: 0.0000e+00 - loss_loss: 0.4786 - val_loss: 0.3072 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3072\n",
      "Epoch 821/1500\n",
      "204/204 [==============================] - 0s 579us/step - loss: 0.3944 - predict_loss: 0.0000e+00 - loss_loss: 0.3944 - val_loss: 0.4090 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.4090\n",
      "Epoch 822/1500\n",
      "204/204 [==============================] - 0s 563us/step - loss: 0.5721 - predict_loss: 0.0000e+00 - loss_loss: 0.5721 - val_loss: 0.2908 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2908\n",
      "Epoch 823/1500\n",
      "204/204 [==============================] - 0s 549us/step - loss: 0.3041 - predict_loss: 0.0000e+00 - loss_loss: 0.3041 - val_loss: 0.3347 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3347\n",
      "Epoch 824/1500\n",
      "204/204 [==============================] - 0s 603us/step - loss: 0.4682 - predict_loss: 0.0000e+00 - loss_loss: 0.4682 - val_loss: 0.3146 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3146\n",
      "Epoch 825/1500\n",
      "204/204 [==============================] - 0s 610us/step - loss: 0.3455 - predict_loss: 0.0000e+00 - loss_loss: 0.3455 - val_loss: 0.3165 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3165\n",
      "Epoch 826/1500\n",
      "204/204 [==============================] - 0s 576us/step - loss: 0.4888 - predict_loss: 0.0000e+00 - loss_loss: 0.4888 - val_loss: 0.3170 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3170\n",
      "Epoch 827/1500\n",
      "204/204 [==============================] - 0s 546us/step - loss: 0.4695 - predict_loss: 0.0000e+00 - loss_loss: 0.4695 - val_loss: 0.3573 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3573\n",
      "Epoch 828/1500\n",
      "204/204 [==============================] - 0s 613us/step - loss: 0.4788 - predict_loss: 0.0000e+00 - loss_loss: 0.4788 - val_loss: 0.3376 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3376\n",
      "Epoch 829/1500\n",
      "204/204 [==============================] - 0s 660us/step - loss: 0.5400 - predict_loss: 0.0000e+00 - loss_loss: 0.5400 - val_loss: 0.3745 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3745\n",
      "Epoch 830/1500\n",
      "204/204 [==============================] - 0s 656us/step - loss: 0.3833 - predict_loss: 0.0000e+00 - loss_loss: 0.3833 - val_loss: 0.2921 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2921\n",
      "Epoch 831/1500\n",
      "204/204 [==============================] - 0s 579us/step - loss: 0.3962 - predict_loss: 0.0000e+00 - loss_loss: 0.3962 - val_loss: 0.3066 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3066\n",
      "Epoch 832/1500\n",
      "204/204 [==============================] - 0s 665us/step - loss: 0.4470 - predict_loss: 0.0000e+00 - loss_loss: 0.4470 - val_loss: 0.3062 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3062\n",
      "Epoch 833/1500\n",
      "204/204 [==============================] - 0s 618us/step - loss: 0.4077 - predict_loss: 0.0000e+00 - loss_loss: 0.4077 - val_loss: 0.3766 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3766\n",
      "Epoch 834/1500\n",
      "204/204 [==============================] - 0s 593us/step - loss: 0.3903 - predict_loss: 0.0000e+00 - loss_loss: 0.3903 - val_loss: 0.3477 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3477\n",
      "Epoch 835/1500\n",
      "204/204 [==============================] - 0s 589us/step - loss: 0.5486 - predict_loss: 0.0000e+00 - loss_loss: 0.5486 - val_loss: 0.3028 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3028\n",
      "Epoch 836/1500\n",
      "204/204 [==============================] - 0s 630us/step - loss: 0.4028 - predict_loss: 0.0000e+00 - loss_loss: 0.4028 - val_loss: 0.3650 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3650\n",
      "Epoch 837/1500\n",
      "204/204 [==============================] - 0s 619us/step - loss: 0.3939 - predict_loss: 0.0000e+00 - loss_loss: 0.3939 - val_loss: 0.3264 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3264\n",
      "Epoch 838/1500\n",
      "204/204 [==============================] - 0s 488us/step - loss: 0.3892 - predict_loss: 0.0000e+00 - loss_loss: 0.3892 - val_loss: 0.4277 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.4277\n",
      "Epoch 839/1500\n",
      "204/204 [==============================] - 0s 437us/step - loss: 0.4323 - predict_loss: 0.0000e+00 - loss_loss: 0.4323 - val_loss: 0.3567 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3567\n",
      "Epoch 840/1500\n",
      "204/204 [==============================] - 0s 590us/step - loss: 0.4417 - predict_loss: 0.0000e+00 - loss_loss: 0.4417 - val_loss: 0.3282 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3282\n",
      "Epoch 841/1500\n",
      "204/204 [==============================] - 0s 672us/step - loss: 0.3650 - predict_loss: 0.0000e+00 - loss_loss: 0.3650 - val_loss: 0.3456 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3456\n",
      "Epoch 842/1500\n",
      "204/204 [==============================] - 0s 574us/step - loss: 0.4841 - predict_loss: 0.0000e+00 - loss_loss: 0.4841 - val_loss: 0.3125 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3125\n",
      "Epoch 843/1500\n",
      "204/204 [==============================] - 0s 689us/step - loss: 0.3390 - predict_loss: 0.0000e+00 - loss_loss: 0.3390 - val_loss: 0.4246 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.4246\n",
      "Epoch 844/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "204/204 [==============================] - 0s 633us/step - loss: 0.3877 - predict_loss: 0.0000e+00 - loss_loss: 0.3877 - val_loss: 0.3257 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3257\n",
      "Epoch 845/1500\n",
      "204/204 [==============================] - 0s 647us/step - loss: 0.4864 - predict_loss: 0.0000e+00 - loss_loss: 0.4864 - val_loss: 0.3056 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3056\n",
      "Epoch 846/1500\n",
      "204/204 [==============================] - 0s 603us/step - loss: 0.3305 - predict_loss: 0.0000e+00 - loss_loss: 0.3305 - val_loss: 0.3336 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3336\n",
      "Epoch 847/1500\n",
      "204/204 [==============================] - 0s 587us/step - loss: 0.4369 - predict_loss: 0.0000e+00 - loss_loss: 0.4369 - val_loss: 0.3357 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3357\n",
      "Epoch 848/1500\n",
      "204/204 [==============================] - 0s 683us/step - loss: 0.3241 - predict_loss: 0.0000e+00 - loss_loss: 0.3241 - val_loss: 0.3895 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3895\n",
      "Epoch 849/1500\n",
      "204/204 [==============================] - 0s 557us/step - loss: 0.4888 - predict_loss: 0.0000e+00 - loss_loss: 0.4888 - val_loss: 0.2949 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2949\n",
      "Epoch 850/1500\n",
      "204/204 [==============================] - 0s 569us/step - loss: 0.4258 - predict_loss: 0.0000e+00 - loss_loss: 0.4258 - val_loss: 0.3843 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3843\n",
      "Epoch 851/1500\n",
      "204/204 [==============================] - 0s 608us/step - loss: 0.3289 - predict_loss: 0.0000e+00 - loss_loss: 0.3289 - val_loss: 0.3478 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3478\n",
      "Epoch 852/1500\n",
      "204/204 [==============================] - 0s 547us/step - loss: 0.5654 - predict_loss: 0.0000e+00 - loss_loss: 0.5654 - val_loss: 0.3478 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3478\n",
      "Epoch 853/1500\n",
      "204/204 [==============================] - 0s 484us/step - loss: 0.3323 - predict_loss: 0.0000e+00 - loss_loss: 0.3323 - val_loss: 0.3045 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3045\n",
      "Epoch 854/1500\n",
      "204/204 [==============================] - 0s 514us/step - loss: 0.5225 - predict_loss: 0.0000e+00 - loss_loss: 0.5225 - val_loss: 0.3301 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3301\n",
      "Epoch 855/1500\n",
      "204/204 [==============================] - 0s 489us/step - loss: 0.3439 - predict_loss: 0.0000e+00 - loss_loss: 0.3439 - val_loss: 0.3036 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3036\n",
      "Epoch 856/1500\n",
      "204/204 [==============================] - 0s 549us/step - loss: 0.4212 - predict_loss: 0.0000e+00 - loss_loss: 0.4212 - val_loss: 0.3518 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3518\n",
      "Epoch 857/1500\n",
      "204/204 [==============================] - 0s 595us/step - loss: 0.3787 - predict_loss: 0.0000e+00 - loss_loss: 0.3787 - val_loss: 0.3017 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3017\n",
      "Epoch 858/1500\n",
      "204/204 [==============================] - 0s 615us/step - loss: 0.5190 - predict_loss: 0.0000e+00 - loss_loss: 0.5190 - val_loss: 0.3304 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3304\n",
      "Epoch 859/1500\n",
      "204/204 [==============================] - 0s 566us/step - loss: 0.4355 - predict_loss: 0.0000e+00 - loss_loss: 0.4355 - val_loss: 0.3324 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3324\n",
      "Epoch 860/1500\n",
      "204/204 [==============================] - 0s 535us/step - loss: 0.3807 - predict_loss: 0.0000e+00 - loss_loss: 0.3807 - val_loss: 0.2999 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2999\n",
      "Epoch 861/1500\n",
      "204/204 [==============================] - 0s 539us/step - loss: 0.3644 - predict_loss: 0.0000e+00 - loss_loss: 0.3644 - val_loss: 0.3054 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3054\n",
      "Epoch 862/1500\n",
      "204/204 [==============================] - 0s 590us/step - loss: 0.3892 - predict_loss: 0.0000e+00 - loss_loss: 0.3892 - val_loss: 0.3472 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3472\n",
      "Epoch 863/1500\n",
      "204/204 [==============================] - 0s 597us/step - loss: 0.3309 - predict_loss: 0.0000e+00 - loss_loss: 0.3309 - val_loss: 0.3620 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3620\n",
      "Epoch 864/1500\n",
      "204/204 [==============================] - 0s 536us/step - loss: 0.5216 - predict_loss: 0.0000e+00 - loss_loss: 0.5216 - val_loss: 0.4247 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.4247\n",
      "Epoch 865/1500\n",
      "204/204 [==============================] - 0s 600us/step - loss: 0.3090 - predict_loss: 0.0000e+00 - loss_loss: 0.3090 - val_loss: 0.3166 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3166\n",
      "Epoch 866/1500\n",
      "204/204 [==============================] - 0s 591us/step - loss: 0.2961 - predict_loss: 0.0000e+00 - loss_loss: 0.2961 - val_loss: 0.2946 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2946\n",
      "Epoch 867/1500\n",
      "204/204 [==============================] - 0s 569us/step - loss: 0.2944 - predict_loss: 0.0000e+00 - loss_loss: 0.2944 - val_loss: 0.2912 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2912\n",
      "Epoch 868/1500\n",
      "204/204 [==============================] - 0s 613us/step - loss: 0.2961 - predict_loss: 0.0000e+00 - loss_loss: 0.2961 - val_loss: 0.3162 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3162\n",
      "Epoch 869/1500\n",
      "204/204 [==============================] - 0s 631us/step - loss: 0.2794 - predict_loss: 0.0000e+00 - loss_loss: 0.2794 - val_loss: 0.3565 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3565\n",
      "Epoch 870/1500\n",
      "204/204 [==============================] - 0s 625us/step - loss: 0.3339 - predict_loss: 0.0000e+00 - loss_loss: 0.3339 - val_loss: 0.2925 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2925\n",
      "Epoch 871/1500\n",
      "204/204 [==============================] - 0s 539us/step - loss: 0.2875 - predict_loss: 0.0000e+00 - loss_loss: 0.2875 - val_loss: 0.3131 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3131\n",
      "Epoch 872/1500\n",
      "204/204 [==============================] - 0s 508us/step - loss: 0.3325 - predict_loss: 0.0000e+00 - loss_loss: 0.3325 - val_loss: 0.3196 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3196\n",
      "Epoch 873/1500\n",
      "204/204 [==============================] - 0s 513us/step - loss: 0.2741 - predict_loss: 0.0000e+00 - loss_loss: 0.2741 - val_loss: 0.2969 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2969\n",
      "Epoch 874/1500\n",
      "204/204 [==============================] - 0s 596us/step - loss: 0.2739 - predict_loss: 0.0000e+00 - loss_loss: 0.2739 - val_loss: 0.3385 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3385\n",
      "Epoch 875/1500\n",
      "204/204 [==============================] - 0s 652us/step - loss: 0.3636 - predict_loss: 0.0000e+00 - loss_loss: 0.3636 - val_loss: 0.2868 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2868\n",
      "Epoch 876/1500\n",
      "204/204 [==============================] - 0s 649us/step - loss: 0.2956 - predict_loss: 0.0000e+00 - loss_loss: 0.2956 - val_loss: 0.3010 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3010\n",
      "Epoch 877/1500\n",
      "204/204 [==============================] - 0s 587us/step - loss: 0.3051 - predict_loss: 0.0000e+00 - loss_loss: 0.3051 - val_loss: 0.2905 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2905\n",
      "Epoch 878/1500\n",
      "204/204 [==============================] - 0s 554us/step - loss: 0.2881 - predict_loss: 0.0000e+00 - loss_loss: 0.2881 - val_loss: 0.3108 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3108\n",
      "Epoch 879/1500\n",
      "204/204 [==============================] - 0s 617us/step - loss: 0.2994 - predict_loss: 0.0000e+00 - loss_loss: 0.2994 - val_loss: 0.3170 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3170\n",
      "Epoch 880/1500\n",
      "204/204 [==============================] - 0s 589us/step - loss: 0.3023 - predict_loss: 0.0000e+00 - loss_loss: 0.3023 - val_loss: 0.3181 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3181\n",
      "Epoch 881/1500\n",
      "204/204 [==============================] - 0s 637us/step - loss: 0.2891 - predict_loss: 0.0000e+00 - loss_loss: 0.2891 - val_loss: 0.3454 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3454\n",
      "Epoch 882/1500\n",
      "204/204 [==============================] - 0s 637us/step - loss: 0.2844 - predict_loss: 0.0000e+00 - loss_loss: 0.2844 - val_loss: 0.2998 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2998\n",
      "Epoch 883/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "204/204 [==============================] - 0s 596us/step - loss: 0.2930 - predict_loss: 0.0000e+00 - loss_loss: 0.2930 - val_loss: 0.2996 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2996\n",
      "Epoch 884/1500\n",
      "204/204 [==============================] - 0s 788us/step - loss: 0.3278 - predict_loss: 0.0000e+00 - loss_loss: 0.3278 - val_loss: 0.3026 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3026\n",
      "Epoch 885/1500\n",
      "204/204 [==============================] - 0s 773us/step - loss: 0.2748 - predict_loss: 0.0000e+00 - loss_loss: 0.2748 - val_loss: 0.3075 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3075\n",
      "Epoch 886/1500\n",
      "204/204 [==============================] - 0s 532us/step - loss: 0.2798 - predict_loss: 0.0000e+00 - loss_loss: 0.2798 - val_loss: 0.3338 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3338\n",
      "Epoch 887/1500\n",
      "204/204 [==============================] - 0s 693us/step - loss: 0.3115 - predict_loss: 0.0000e+00 - loss_loss: 0.3115 - val_loss: 0.2802 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2802\n",
      "Epoch 888/1500\n",
      "204/204 [==============================] - 0s 733us/step - loss: 0.2720 - predict_loss: 0.0000e+00 - loss_loss: 0.2720 - val_loss: 0.3182 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3182\n",
      "Epoch 889/1500\n",
      "204/204 [==============================] - 0s 729us/step - loss: 0.3018 - predict_loss: 0.0000e+00 - loss_loss: 0.3018 - val_loss: 0.2956 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2956\n",
      "Epoch 890/1500\n",
      "204/204 [==============================] - 0s 561us/step - loss: 0.2753 - predict_loss: 0.0000e+00 - loss_loss: 0.2753 - val_loss: 0.3046 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3046\n",
      "Epoch 891/1500\n",
      "204/204 [==============================] - 0s 488us/step - loss: 0.2690 - predict_loss: 0.0000e+00 - loss_loss: 0.2690 - val_loss: 0.2812 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2812\n",
      "Epoch 892/1500\n",
      "204/204 [==============================] - 0s 674us/step - loss: 0.3083 - predict_loss: 0.0000e+00 - loss_loss: 0.3083 - val_loss: 0.2893 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2893\n",
      "Epoch 893/1500\n",
      "204/204 [==============================] - 0s 839us/step - loss: 0.2988 - predict_loss: 0.0000e+00 - loss_loss: 0.2988 - val_loss: 0.2856 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2856\n",
      "Epoch 894/1500\n",
      "204/204 [==============================] - 0s 689us/step - loss: 0.3123 - predict_loss: 0.0000e+00 - loss_loss: 0.3123 - val_loss: 0.2878 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2878\n",
      "Epoch 895/1500\n",
      "204/204 [==============================] - 0s 615us/step - loss: 0.2793 - predict_loss: 0.0000e+00 - loss_loss: 0.2793 - val_loss: 0.2817 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2817\n",
      "Epoch 896/1500\n",
      "204/204 [==============================] - 0s 645us/step - loss: 0.3395 - predict_loss: 0.0000e+00 - loss_loss: 0.3395 - val_loss: 0.2824 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2824\n",
      "Epoch 897/1500\n",
      "204/204 [==============================] - 0s 850us/step - loss: 0.2729 - predict_loss: 0.0000e+00 - loss_loss: 0.2729 - val_loss: 0.2813 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2813\n",
      "Epoch 898/1500\n",
      "204/204 [==============================] - 0s 713us/step - loss: 0.3274 - predict_loss: 0.0000e+00 - loss_loss: 0.3274 - val_loss: 0.2856 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2856\n",
      "Epoch 899/1500\n",
      "204/204 [==============================] - 0s 608us/step - loss: 0.2606 - predict_loss: 0.0000e+00 - loss_loss: 0.2606 - val_loss: 0.2766 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2766\n",
      "Epoch 900/1500\n",
      "204/204 [==============================] - 0s 604us/step - loss: 0.3310 - predict_loss: 0.0000e+00 - loss_loss: 0.3310 - val_loss: 0.2984 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2984\n",
      "Epoch 901/1500\n",
      "204/204 [==============================] - 0s 536us/step - loss: 0.2727 - predict_loss: 0.0000e+00 - loss_loss: 0.2727 - val_loss: 0.3233 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3233\n",
      "Epoch 902/1500\n",
      "204/204 [==============================] - 0s 561us/step - loss: 0.2975 - predict_loss: 0.0000e+00 - loss_loss: 0.2975 - val_loss: 0.2914 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2914\n",
      "Epoch 903/1500\n",
      "204/204 [==============================] - 0s 494us/step - loss: 0.2809 - predict_loss: 0.0000e+00 - loss_loss: 0.2809 - val_loss: 0.2900 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2900\n",
      "Epoch 904/1500\n",
      "204/204 [==============================] - 0s 570us/step - loss: 0.3075 - predict_loss: 0.0000e+00 - loss_loss: 0.3075 - val_loss: 0.2915 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2915\n",
      "Epoch 905/1500\n",
      "204/204 [==============================] - 0s 714us/step - loss: 0.3004 - predict_loss: 0.0000e+00 - loss_loss: 0.3004 - val_loss: 0.2768 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2768\n",
      "Epoch 906/1500\n",
      "204/204 [==============================] - 0s 799us/step - loss: 0.3117 - predict_loss: 0.0000e+00 - loss_loss: 0.3117 - val_loss: 0.2851 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2851\n",
      "Epoch 907/1500\n",
      "204/204 [==============================] - 0s 647us/step - loss: 0.2661 - predict_loss: 0.0000e+00 - loss_loss: 0.2661 - val_loss: 0.2955 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2955\n",
      "Epoch 908/1500\n",
      "204/204 [==============================] - 0s 593us/step - loss: 0.2722 - predict_loss: 0.0000e+00 - loss_loss: 0.2722 - val_loss: 0.2749 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2749\n",
      "Epoch 909/1500\n",
      "204/204 [==============================] - 0s 532us/step - loss: 0.2657 - predict_loss: 0.0000e+00 - loss_loss: 0.2657 - val_loss: 0.2974 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2974\n",
      "Epoch 910/1500\n",
      "204/204 [==============================] - 0s 611us/step - loss: 0.2826 - predict_loss: 0.0000e+00 - loss_loss: 0.2826 - val_loss: 0.3112 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3112\n",
      "Epoch 911/1500\n",
      "204/204 [==============================] - 0s 592us/step - loss: 0.2729 - predict_loss: 0.0000e+00 - loss_loss: 0.2729 - val_loss: 0.2958 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2958\n",
      "Epoch 912/1500\n",
      "204/204 [==============================] - 0s 618us/step - loss: 0.3208 - predict_loss: 0.0000e+00 - loss_loss: 0.3208 - val_loss: 0.2897 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2897\n",
      "Epoch 913/1500\n",
      "204/204 [==============================] - 0s 557us/step - loss: 0.2442 - predict_loss: 0.0000e+00 - loss_loss: 0.2442 - val_loss: 0.3220 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3220\n",
      "Epoch 914/1500\n",
      "204/204 [==============================] - 0s 600us/step - loss: 0.3246 - predict_loss: 0.0000e+00 - loss_loss: 0.3246 - val_loss: 0.2891 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2891\n",
      "Epoch 915/1500\n",
      "204/204 [==============================] - 0s 584us/step - loss: 0.2538 - predict_loss: 0.0000e+00 - loss_loss: 0.2538 - val_loss: 0.2743 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2743\n",
      "Epoch 916/1500\n",
      "204/204 [==============================] - 0s 645us/step - loss: 0.3194 - predict_loss: 0.0000e+00 - loss_loss: 0.3194 - val_loss: 0.2920 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2920\n",
      "Epoch 917/1500\n",
      "204/204 [==============================] - 0s 748us/step - loss: 0.2816 - predict_loss: 0.0000e+00 - loss_loss: 0.2816 - val_loss: 0.2783 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2783\n",
      "Epoch 918/1500\n",
      "204/204 [==============================] - 0s 632us/step - loss: 0.2572 - predict_loss: 0.0000e+00 - loss_loss: 0.2572 - val_loss: 0.2855 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2855\n",
      "Epoch 919/1500\n",
      "204/204 [==============================] - 0s 556us/step - loss: 0.2896 - predict_loss: 0.0000e+00 - loss_loss: 0.2896 - val_loss: 0.3080 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3080\n",
      "Epoch 920/1500\n",
      "204/204 [==============================] - 0s 520us/step - loss: 0.2584 - predict_loss: 0.0000e+00 - loss_loss: 0.2584 - val_loss: 0.2739 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2739\n",
      "Epoch 921/1500\n",
      "204/204 [==============================] - 0s 667us/step - loss: 0.3233 - predict_loss: 0.0000e+00 - loss_loss: 0.3233 - val_loss: 0.3054 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3054\n",
      "Epoch 922/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "204/204 [==============================] - 0s 585us/step - loss: 0.2547 - predict_loss: 0.0000e+00 - loss_loss: 0.2547 - val_loss: 0.2879 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2879\n",
      "Epoch 923/1500\n",
      "204/204 [==============================] - 0s 599us/step - loss: 0.3089 - predict_loss: 0.0000e+00 - loss_loss: 0.3089 - val_loss: 0.2730 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2730\n",
      "Epoch 924/1500\n",
      "204/204 [==============================] - 0s 534us/step - loss: 0.2511 - predict_loss: 0.0000e+00 - loss_loss: 0.2511 - val_loss: 0.2998 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2998\n",
      "Epoch 925/1500\n",
      "204/204 [==============================] - 0s 631us/step - loss: 0.3014 - predict_loss: 0.0000e+00 - loss_loss: 0.3014 - val_loss: 0.3210 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3210\n",
      "Epoch 926/1500\n",
      "204/204 [==============================] - 0s 560us/step - loss: 0.2954 - predict_loss: 0.0000e+00 - loss_loss: 0.2954 - val_loss: 0.2818 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2818\n",
      "Epoch 927/1500\n",
      "204/204 [==============================] - 0s 509us/step - loss: 0.3246 - predict_loss: 0.0000e+00 - loss_loss: 0.3246 - val_loss: 0.3185 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3185\n",
      "Epoch 928/1500\n",
      "204/204 [==============================] - 0s 577us/step - loss: 0.2509 - predict_loss: 0.0000e+00 - loss_loss: 0.2509 - val_loss: 0.3050 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3050\n",
      "Epoch 929/1500\n",
      "204/204 [==============================] - 0s 486us/step - loss: 0.2650 - predict_loss: 0.0000e+00 - loss_loss: 0.2650 - val_loss: 0.2932 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2932\n",
      "Epoch 930/1500\n",
      "204/204 [==============================] - 0s 584us/step - loss: 0.2499 - predict_loss: 0.0000e+00 - loss_loss: 0.2499 - val_loss: 0.2856 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2856\n",
      "Epoch 931/1500\n",
      "204/204 [==============================] - 0s 572us/step - loss: 0.2887 - predict_loss: 0.0000e+00 - loss_loss: 0.2887 - val_loss: 0.2845 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2845\n",
      "Epoch 932/1500\n",
      "204/204 [==============================] - 0s 484us/step - loss: 0.3027 - predict_loss: 0.0000e+00 - loss_loss: 0.3027 - val_loss: 0.2966 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2966\n",
      "Epoch 933/1500\n",
      "204/204 [==============================] - 0s 531us/step - loss: 0.2495 - predict_loss: 0.0000e+00 - loss_loss: 0.2495 - val_loss: 0.3108 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3108\n",
      "Epoch 934/1500\n",
      "204/204 [==============================] - 0s 559us/step - loss: 0.3126 - predict_loss: 0.0000e+00 - loss_loss: 0.3126 - val_loss: 0.2992 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2992\n",
      "Epoch 935/1500\n",
      "204/204 [==============================] - 0s 543us/step - loss: 0.2591 - predict_loss: 0.0000e+00 - loss_loss: 0.2591 - val_loss: 0.2812 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2812\n",
      "Epoch 936/1500\n",
      "204/204 [==============================] - 0s 555us/step - loss: 0.2904 - predict_loss: 0.0000e+00 - loss_loss: 0.2904 - val_loss: 0.2764 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2764\n",
      "Epoch 937/1500\n",
      "204/204 [==============================] - 0s 614us/step - loss: 0.2963 - predict_loss: 0.0000e+00 - loss_loss: 0.2963 - val_loss: 0.2794 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2794\n",
      "Epoch 938/1500\n",
      "204/204 [==============================] - 0s 592us/step - loss: 0.2465 - predict_loss: 0.0000e+00 - loss_loss: 0.2465 - val_loss: 0.2816 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2816\n",
      "Epoch 939/1500\n",
      "204/204 [==============================] - 0s 596us/step - loss: 0.2767 - predict_loss: 0.0000e+00 - loss_loss: 0.2767 - val_loss: 0.2891 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2891\n",
      "Epoch 940/1500\n",
      "204/204 [==============================] - 0s 600us/step - loss: 0.2957 - predict_loss: 0.0000e+00 - loss_loss: 0.2957 - val_loss: 0.2972 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2972\n",
      "Epoch 941/1500\n",
      "204/204 [==============================] - 0s 643us/step - loss: 0.3051 - predict_loss: 0.0000e+00 - loss_loss: 0.3051 - val_loss: 0.2907 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2907\n",
      "Epoch 942/1500\n",
      "204/204 [==============================] - 0s 741us/step - loss: 0.2399 - predict_loss: 0.0000e+00 - loss_loss: 0.2399 - val_loss: 0.2654 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2654\n",
      "Epoch 943/1500\n",
      "204/204 [==============================] - 0s 804us/step - loss: 0.3056 - predict_loss: 0.0000e+00 - loss_loss: 0.3056 - val_loss: 0.2934 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2934\n",
      "Epoch 944/1500\n",
      "204/204 [==============================] - 0s 625us/step - loss: 0.2978 - predict_loss: 0.0000e+00 - loss_loss: 0.2978 - val_loss: 0.3154 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3154\n",
      "Epoch 945/1500\n",
      "204/204 [==============================] - 0s 625us/step - loss: 0.2834 - predict_loss: 0.0000e+00 - loss_loss: 0.2834 - val_loss: 0.2802 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2802\n",
      "Epoch 946/1500\n",
      "204/204 [==============================] - 0s 614us/step - loss: 0.2790 - predict_loss: 0.0000e+00 - loss_loss: 0.2790 - val_loss: 0.2696 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2696\n",
      "Epoch 947/1500\n",
      "204/204 [==============================] - 0s 595us/step - loss: 0.2443 - predict_loss: 0.0000e+00 - loss_loss: 0.2443 - val_loss: 0.2740 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2740\n",
      "Epoch 948/1500\n",
      "204/204 [==============================] - 0s 689us/step - loss: 0.3134 - predict_loss: 0.0000e+00 - loss_loss: 0.3134 - val_loss: 0.2826 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2826\n",
      "Epoch 949/1500\n",
      "204/204 [==============================] - 0s 589us/step - loss: 0.2596 - predict_loss: 0.0000e+00 - loss_loss: 0.2596 - val_loss: 0.2929 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2929\n",
      "Epoch 950/1500\n",
      "204/204 [==============================] - 0s 555us/step - loss: 0.2585 - predict_loss: 0.0000e+00 - loss_loss: 0.2585 - val_loss: 0.2658 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2658\n",
      "Epoch 951/1500\n",
      "204/204 [==============================] - 0s 529us/step - loss: 0.2896 - predict_loss: 0.0000e+00 - loss_loss: 0.2896 - val_loss: 0.2808 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2808\n",
      "Epoch 952/1500\n",
      "204/204 [==============================] - 0s 616us/step - loss: 0.2884 - predict_loss: 0.0000e+00 - loss_loss: 0.2884 - val_loss: 0.2937 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2937\n",
      "Epoch 953/1500\n",
      "204/204 [==============================] - 0s 636us/step - loss: 0.2682 - predict_loss: 0.0000e+00 - loss_loss: 0.2682 - val_loss: 0.2875 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2875\n",
      "Epoch 954/1500\n",
      "204/204 [==============================] - 0s 593us/step - loss: 0.2902 - predict_loss: 0.0000e+00 - loss_loss: 0.2902 - val_loss: 0.3344 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3344\n",
      "Epoch 955/1500\n",
      "204/204 [==============================] - 0s 566us/step - loss: 0.2446 - predict_loss: 0.0000e+00 - loss_loss: 0.2446 - val_loss: 0.2826 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2826\n",
      "Epoch 956/1500\n",
      "204/204 [==============================] - 0s 712us/step - loss: 0.3195 - predict_loss: 0.0000e+00 - loss_loss: 0.3195 - val_loss: 0.3033 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3033\n",
      "Epoch 957/1500\n",
      "204/204 [==============================] - 0s 598us/step - loss: 0.2633 - predict_loss: 0.0000e+00 - loss_loss: 0.2633 - val_loss: 0.3265 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3265\n",
      "Epoch 958/1500\n",
      "204/204 [==============================] - 0s 623us/step - loss: 0.2577 - predict_loss: 0.0000e+00 - loss_loss: 0.2577 - val_loss: 0.2797 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2797\n",
      "Epoch 959/1500\n",
      "204/204 [==============================] - 0s 564us/step - loss: 0.2964 - predict_loss: 0.0000e+00 - loss_loss: 0.2964 - val_loss: 0.2586 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2586\n",
      "Epoch 960/1500\n",
      "204/204 [==============================] - 0s 632us/step - loss: 0.2611 - predict_loss: 0.0000e+00 - loss_loss: 0.2611 - val_loss: 0.2640 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2640\n",
      "Epoch 961/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "204/204 [==============================] - 0s 635us/step - loss: 0.3042 - predict_loss: 0.0000e+00 - loss_loss: 0.3042 - val_loss: 0.2793 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2793\n",
      "Epoch 962/1500\n",
      "204/204 [==============================] - 0s 519us/step - loss: 0.2421 - predict_loss: 0.0000e+00 - loss_loss: 0.2421 - val_loss: 0.2788 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2788\n",
      "Epoch 963/1500\n",
      "204/204 [==============================] - 0s 531us/step - loss: 0.3116 - predict_loss: 0.0000e+00 - loss_loss: 0.3116 - val_loss: 0.3043 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3043\n",
      "Epoch 964/1500\n",
      "204/204 [==============================] - 0s 531us/step - loss: 0.2388 - predict_loss: 0.0000e+00 - loss_loss: 0.2388 - val_loss: 0.2760 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2760\n",
      "Epoch 965/1500\n",
      "204/204 [==============================] - 0s 819us/step - loss: 0.2974 - predict_loss: 0.0000e+00 - loss_loss: 0.2974 - val_loss: 0.2750 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2750\n",
      "Epoch 966/1500\n",
      "204/204 [==============================] - 0s 777us/step - loss: 0.2451 - predict_loss: 0.0000e+00 - loss_loss: 0.2451 - val_loss: 0.2700 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2700\n",
      "Epoch 967/1500\n",
      "204/204 [==============================] - 0s 535us/step - loss: 0.2701 - predict_loss: 0.0000e+00 - loss_loss: 0.2701 - val_loss: 0.2612 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2612\n",
      "Epoch 968/1500\n",
      "204/204 [==============================] - 0s 560us/step - loss: 0.2822 - predict_loss: 0.0000e+00 - loss_loss: 0.2822 - val_loss: 0.2784 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2784\n",
      "Epoch 969/1500\n",
      "204/204 [==============================] - 0s 572us/step - loss: 0.3136 - predict_loss: 0.0000e+00 - loss_loss: 0.3136 - val_loss: 0.2952 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2952\n",
      "Epoch 970/1500\n",
      "204/204 [==============================] - 0s 600us/step - loss: 0.2525 - predict_loss: 0.0000e+00 - loss_loss: 0.2525 - val_loss: 0.2813 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2813\n",
      "Epoch 971/1500\n",
      "204/204 [==============================] - 0s 586us/step - loss: 0.2694 - predict_loss: 0.0000e+00 - loss_loss: 0.2694 - val_loss: 0.2782 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2782\n",
      "Epoch 972/1500\n",
      "204/204 [==============================] - 0s 560us/step - loss: 0.2746 - predict_loss: 0.0000e+00 - loss_loss: 0.2746 - val_loss: 0.2901 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2901\n",
      "Epoch 973/1500\n",
      "204/204 [==============================] - 0s 560us/step - loss: 0.2474 - predict_loss: 0.0000e+00 - loss_loss: 0.2474 - val_loss: 0.2772 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2772\n",
      "Epoch 974/1500\n",
      "204/204 [==============================] - 0s 526us/step - loss: 0.2638 - predict_loss: 0.0000e+00 - loss_loss: 0.2638 - val_loss: 0.3053 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3053\n",
      "Epoch 975/1500\n",
      "204/204 [==============================] - 0s 550us/step - loss: 0.3165 - predict_loss: 0.0000e+00 - loss_loss: 0.3165 - val_loss: 0.2841 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2841\n",
      "Epoch 976/1500\n",
      "204/204 [==============================] - 0s 589us/step - loss: 0.2600 - predict_loss: 0.0000e+00 - loss_loss: 0.2600 - val_loss: 0.2776 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2776\n",
      "Epoch 977/1500\n",
      "204/204 [==============================] - 0s 804us/step - loss: 0.2657 - predict_loss: 0.0000e+00 - loss_loss: 0.2657 - val_loss: 0.2982 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2982\n",
      "Epoch 978/1500\n",
      "204/204 [==============================] - 0s 726us/step - loss: 0.2553 - predict_loss: 0.0000e+00 - loss_loss: 0.2553 - val_loss: 0.2807 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2807\n",
      "Epoch 979/1500\n",
      "204/204 [==============================] - 0s 635us/step - loss: 0.3257 - predict_loss: 0.0000e+00 - loss_loss: 0.3257 - val_loss: 0.2835 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2835\n",
      "Epoch 980/1500\n",
      "204/204 [==============================] - 0s 577us/step - loss: 0.2714 - predict_loss: 0.0000e+00 - loss_loss: 0.2714 - val_loss: 0.2766 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2766\n",
      "Epoch 981/1500\n",
      "204/204 [==============================] - 0s 494us/step - loss: 0.2512 - predict_loss: 0.0000e+00 - loss_loss: 0.2512 - val_loss: 0.2778 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2778\n",
      "Epoch 982/1500\n",
      "204/204 [==============================] - 0s 522us/step - loss: 0.2853 - predict_loss: 0.0000e+00 - loss_loss: 0.2853 - val_loss: 0.2810 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2810\n",
      "Epoch 983/1500\n",
      "204/204 [==============================] - 0s 564us/step - loss: 0.3061 - predict_loss: 0.0000e+00 - loss_loss: 0.3061 - val_loss: 0.3019 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3019\n",
      "Epoch 984/1500\n",
      "204/204 [==============================] - 0s 594us/step - loss: 0.2540 - predict_loss: 0.0000e+00 - loss_loss: 0.2540 - val_loss: 0.3034 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3034\n",
      "Epoch 985/1500\n",
      "204/204 [==============================] - 0s 569us/step - loss: 0.2467 - predict_loss: 0.0000e+00 - loss_loss: 0.2467 - val_loss: 0.2754 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2754\n",
      "Epoch 986/1500\n",
      "204/204 [==============================] - 0s 650us/step - loss: 0.2861 - predict_loss: 0.0000e+00 - loss_loss: 0.2861 - val_loss: 0.2761 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2761\n",
      "Epoch 987/1500\n",
      "204/204 [==============================] - 0s 605us/step - loss: 0.2212 - predict_loss: 0.0000e+00 - loss_loss: 0.2212 - val_loss: 0.2760 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2760\n",
      "Epoch 988/1500\n",
      "204/204 [==============================] - 0s 579us/step - loss: 0.3007 - predict_loss: 0.0000e+00 - loss_loss: 0.3007 - val_loss: 0.2610 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2610\n",
      "Epoch 989/1500\n",
      "204/204 [==============================] - 0s 590us/step - loss: 0.2360 - predict_loss: 0.0000e+00 - loss_loss: 0.2360 - val_loss: 0.2707 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2707\n",
      "Epoch 990/1500\n",
      "204/204 [==============================] - 0s 590us/step - loss: 0.2696 - predict_loss: 0.0000e+00 - loss_loss: 0.2696 - val_loss: 0.2740 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2740\n",
      "Epoch 991/1500\n",
      "204/204 [==============================] - 0s 588us/step - loss: 0.2494 - predict_loss: 0.0000e+00 - loss_loss: 0.2494 - val_loss: 0.2889 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2889\n",
      "Epoch 992/1500\n",
      "204/204 [==============================] - 0s 613us/step - loss: 0.2672 - predict_loss: 0.0000e+00 - loss_loss: 0.2672 - val_loss: 0.2607 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2607\n",
      "Epoch 993/1500\n",
      "204/204 [==============================] - 0s 561us/step - loss: 0.2339 - predict_loss: 0.0000e+00 - loss_loss: 0.2339 - val_loss: 0.2677 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2677\n",
      "Epoch 994/1500\n",
      "204/204 [==============================] - 0s 550us/step - loss: 0.3420 - predict_loss: 0.0000e+00 - loss_loss: 0.3420 - val_loss: 0.2759 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2759\n",
      "Epoch 995/1500\n",
      "204/204 [==============================] - 0s 617us/step - loss: 0.2580 - predict_loss: 0.0000e+00 - loss_loss: 0.2580 - val_loss: 0.3090 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3090\n",
      "Epoch 996/1500\n",
      "204/204 [==============================] - 0s 654us/step - loss: 0.2489 - predict_loss: 0.0000e+00 - loss_loss: 0.2489 - val_loss: 0.2854 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2854\n",
      "Epoch 997/1500\n",
      "204/204 [==============================] - 0s 642us/step - loss: 0.2462 - predict_loss: 0.0000e+00 - loss_loss: 0.2462 - val_loss: 0.2671 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2671\n",
      "Epoch 998/1500\n",
      "204/204 [==============================] - 0s 597us/step - loss: 0.3206 - predict_loss: 0.0000e+00 - loss_loss: 0.3206 - val_loss: 0.2672 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2672\n",
      "Epoch 999/1500\n",
      "204/204 [==============================] - 0s 595us/step - loss: 0.2704 - predict_loss: 0.0000e+00 - loss_loss: 0.2704 - val_loss: 0.2538 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2538\n",
      "Epoch 1000/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "204/204 [==============================] - 0s 608us/step - loss: 0.2650 - predict_loss: 0.0000e+00 - loss_loss: 0.2650 - val_loss: 0.2786 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2786\n",
      "Epoch 1001/1500\n",
      "204/204 [==============================] - 0s 597us/step - loss: 0.2886 - predict_loss: 0.0000e+00 - loss_loss: 0.2886 - val_loss: 0.2688 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2688\n",
      "Epoch 1002/1500\n",
      "204/204 [==============================] - 0s 519us/step - loss: 0.2362 - predict_loss: 0.0000e+00 - loss_loss: 0.2362 - val_loss: 0.2924 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2924\n",
      "Epoch 1003/1500\n",
      "204/204 [==============================] - 0s 478us/step - loss: 0.2845 - predict_loss: 0.0000e+00 - loss_loss: 0.2845 - val_loss: 0.2748 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2748\n",
      "Epoch 1004/1500\n",
      "204/204 [==============================] - 0s 556us/step - loss: 0.2469 - predict_loss: 0.0000e+00 - loss_loss: 0.2469 - val_loss: 0.2679 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2679\n",
      "Epoch 1005/1500\n",
      "204/204 [==============================] - 0s 753us/step - loss: 0.3058 - predict_loss: 0.0000e+00 - loss_loss: 0.3058 - val_loss: 0.2530 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2530\n",
      "Epoch 1006/1500\n",
      "204/204 [==============================] - 0s 736us/step - loss: 0.2587 - predict_loss: 0.0000e+00 - loss_loss: 0.2587 - val_loss: 0.2585 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2585\n",
      "Epoch 1007/1500\n",
      "204/204 [==============================] - 0s 592us/step - loss: 0.2760 - predict_loss: 0.0000e+00 - loss_loss: 0.2760 - val_loss: 0.2659 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2659\n",
      "Epoch 1008/1500\n",
      "204/204 [==============================] - 0s 590us/step - loss: 0.2347 - predict_loss: 0.0000e+00 - loss_loss: 0.2347 - val_loss: 0.2917 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2917\n",
      "Epoch 1009/1500\n",
      "204/204 [==============================] - 0s 650us/step - loss: 0.2904 - predict_loss: 0.0000e+00 - loss_loss: 0.2904 - val_loss: 0.2732 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2732\n",
      "Epoch 1010/1500\n",
      "204/204 [==============================] - 0s 818us/step - loss: 0.2722 - predict_loss: 0.0000e+00 - loss_loss: 0.2722 - val_loss: 0.2770 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2770\n",
      "Epoch 1011/1500\n",
      "204/204 [==============================] - 0s 687us/step - loss: 0.2785 - predict_loss: 0.0000e+00 - loss_loss: 0.2785 - val_loss: 0.2909 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2909\n",
      "Epoch 1012/1500\n",
      "204/204 [==============================] - 0s 746us/step - loss: 0.2385 - predict_loss: 0.0000e+00 - loss_loss: 0.2385 - val_loss: 0.2640 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2640\n",
      "Epoch 1013/1500\n",
      "204/204 [==============================] - 0s 799us/step - loss: 0.2958 - predict_loss: 0.0000e+00 - loss_loss: 0.2958 - val_loss: 0.2567 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2567\n",
      "Epoch 1014/1500\n",
      "204/204 [==============================] - 0s 531us/step - loss: 0.2499 - predict_loss: 0.0000e+00 - loss_loss: 0.2499 - val_loss: 0.2621 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2621\n",
      "Epoch 1015/1500\n",
      "204/204 [==============================] - 0s 775us/step - loss: 0.2724 - predict_loss: 0.0000e+00 - loss_loss: 0.2724 - val_loss: 0.2848 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2848\n",
      "Epoch 1016/1500\n",
      "204/204 [==============================] - 0s 719us/step - loss: 0.2942 - predict_loss: 0.0000e+00 - loss_loss: 0.2942 - val_loss: 0.2650 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2650\n",
      "Epoch 1017/1500\n",
      "204/204 [==============================] - 0s 650us/step - loss: 0.2374 - predict_loss: 0.0000e+00 - loss_loss: 0.2374 - val_loss: 0.2923 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2923\n",
      "Epoch 1018/1500\n",
      "204/204 [==============================] - 0s 810us/step - loss: 0.3077 - predict_loss: 0.0000e+00 - loss_loss: 0.3077 - val_loss: 0.2876 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2876\n",
      "Epoch 1019/1500\n",
      "204/204 [==============================] - 0s 624us/step - loss: 0.2455 - predict_loss: 0.0000e+00 - loss_loss: 0.2455 - val_loss: 0.2844 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2844\n",
      "Epoch 1020/1500\n",
      "204/204 [==============================] - 0s 687us/step - loss: 0.2712 - predict_loss: 0.0000e+00 - loss_loss: 0.2712 - val_loss: 0.2790 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2790\n",
      "Epoch 1021/1500\n",
      "204/204 [==============================] - 0s 700us/step - loss: 0.2855 - predict_loss: 0.0000e+00 - loss_loss: 0.2855 - val_loss: 0.2735 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2735\n",
      "Epoch 1022/1500\n",
      "204/204 [==============================] - 0s 652us/step - loss: 0.2466 - predict_loss: 0.0000e+00 - loss_loss: 0.2466 - val_loss: 0.2604 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2604\n",
      "Epoch 1023/1500\n",
      "204/204 [==============================] - 0s 688us/step - loss: 0.2588 - predict_loss: 0.0000e+00 - loss_loss: 0.2588 - val_loss: 0.2635 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2635\n",
      "Epoch 1024/1500\n",
      "204/204 [==============================] - 0s 681us/step - loss: 0.2540 - predict_loss: 0.0000e+00 - loss_loss: 0.2540 - val_loss: 0.2641 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2641\n",
      "Epoch 1025/1500\n",
      "204/204 [==============================] - 0s 713us/step - loss: 0.2658 - predict_loss: 0.0000e+00 - loss_loss: 0.2658 - val_loss: 0.2789 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2789\n",
      "Epoch 1026/1500\n",
      "204/204 [==============================] - 0s 731us/step - loss: 0.2503 - predict_loss: 0.0000e+00 - loss_loss: 0.2503 - val_loss: 0.2737 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2737\n",
      "Epoch 1027/1500\n",
      "204/204 [==============================] - 0s 741us/step - loss: 0.3308 - predict_loss: 0.0000e+00 - loss_loss: 0.3308 - val_loss: 0.3074 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3074\n",
      "Epoch 1028/1500\n",
      "204/204 [==============================] - 0s 545us/step - loss: 0.2523 - predict_loss: 0.0000e+00 - loss_loss: 0.2523 - val_loss: 0.2655 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2655\n",
      "Epoch 1029/1500\n",
      "204/204 [==============================] - 0s 600us/step - loss: 0.2616 - predict_loss: 0.0000e+00 - loss_loss: 0.2616 - val_loss: 0.2890 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2890\n",
      "Epoch 1030/1500\n",
      "204/204 [==============================] - 0s 682us/step - loss: 0.2761 - predict_loss: 0.0000e+00 - loss_loss: 0.2761 - val_loss: 0.2831 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2831\n",
      "Epoch 1031/1500\n",
      "204/204 [==============================] - 0s 603us/step - loss: 0.2507 - predict_loss: 0.0000e+00 - loss_loss: 0.2507 - val_loss: 0.2673 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2673\n",
      "Epoch 1032/1500\n",
      "204/204 [==============================] - 0s 628us/step - loss: 0.2876 - predict_loss: 0.0000e+00 - loss_loss: 0.2876 - val_loss: 0.2931 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2931\n",
      "Epoch 1033/1500\n",
      "204/204 [==============================] - 0s 662us/step - loss: 0.2467 - predict_loss: 0.0000e+00 - loss_loss: 0.2467 - val_loss: 0.2668 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2668\n",
      "Epoch 1034/1500\n",
      "204/204 [==============================] - 0s 607us/step - loss: 0.2953 - predict_loss: 0.0000e+00 - loss_loss: 0.2953 - val_loss: 0.3083 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3083\n",
      "Epoch 1035/1500\n",
      "204/204 [==============================] - 0s 547us/step - loss: 0.2680 - predict_loss: 0.0000e+00 - loss_loss: 0.2680 - val_loss: 0.3123 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.3123\n",
      "Epoch 1036/1500\n",
      "204/204 [==============================] - 0s 572us/step - loss: 0.2748 - predict_loss: 0.0000e+00 - loss_loss: 0.2748 - val_loss: 0.2600 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2600\n",
      "Epoch 1037/1500\n",
      "204/204 [==============================] - 0s 537us/step - loss: 0.2722 - predict_loss: 0.0000e+00 - loss_loss: 0.2722 - val_loss: 0.2528 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2528\n",
      "Epoch 1038/1500\n",
      "204/204 [==============================] - 0s 631us/step - loss: 0.2483 - predict_loss: 0.0000e+00 - loss_loss: 0.2483 - val_loss: 0.2558 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2558\n",
      "Epoch 1039/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "204/204 [==============================] - 0s 612us/step - loss: 0.2086 - predict_loss: 0.0000e+00 - loss_loss: 0.2086 - val_loss: 0.2611 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2611\n",
      "Epoch 1040/1500\n",
      "204/204 [==============================] - 0s 534us/step - loss: 0.2203 - predict_loss: 0.0000e+00 - loss_loss: 0.2203 - val_loss: 0.2600 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2600\n",
      "Epoch 1041/1500\n",
      "204/204 [==============================] - 0s 607us/step - loss: 0.2329 - predict_loss: 0.0000e+00 - loss_loss: 0.2329 - val_loss: 0.2689 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2689\n",
      "Epoch 1042/1500\n",
      "204/204 [==============================] - 0s 556us/step - loss: 0.2321 - predict_loss: 0.0000e+00 - loss_loss: 0.2321 - val_loss: 0.2591 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2591\n",
      "Epoch 1043/1500\n",
      "204/204 [==============================] - 0s 720us/step - loss: 0.2257 - predict_loss: 0.0000e+00 - loss_loss: 0.2257 - val_loss: 0.2542 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2542\n",
      "Epoch 1044/1500\n",
      "204/204 [==============================] - 0s 636us/step - loss: 0.2279 - predict_loss: 0.0000e+00 - loss_loss: 0.2279 - val_loss: 0.2506 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2506\n",
      "Epoch 1045/1500\n",
      "204/204 [==============================] - 0s 690us/step - loss: 0.2281 - predict_loss: 0.0000e+00 - loss_loss: 0.2281 - val_loss: 0.2633 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2633\n",
      "Epoch 1046/1500\n",
      "204/204 [==============================] - 0s 605us/step - loss: 0.2304 - predict_loss: 0.0000e+00 - loss_loss: 0.2304 - val_loss: 0.2556 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2556\n",
      "Epoch 1047/1500\n",
      "204/204 [==============================] - 0s 602us/step - loss: 0.2324 - predict_loss: 0.0000e+00 - loss_loss: 0.2324 - val_loss: 0.2603 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2603\n",
      "Epoch 1048/1500\n",
      "204/204 [==============================] - 0s 573us/step - loss: 0.2197 - predict_loss: 0.0000e+00 - loss_loss: 0.2197 - val_loss: 0.2527 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2527\n",
      "Epoch 1049/1500\n",
      "204/204 [==============================] - 0s 607us/step - loss: 0.2179 - predict_loss: 0.0000e+00 - loss_loss: 0.2179 - val_loss: 0.2654 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2654\n",
      "Epoch 1050/1500\n",
      "204/204 [==============================] - 0s 637us/step - loss: 0.2354 - predict_loss: 0.0000e+00 - loss_loss: 0.2354 - val_loss: 0.2525 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2525\n",
      "Epoch 1051/1500\n",
      "204/204 [==============================] - 0s 591us/step - loss: 0.2205 - predict_loss: 0.0000e+00 - loss_loss: 0.2205 - val_loss: 0.2513 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2513\n",
      "Epoch 1052/1500\n",
      "204/204 [==============================] - 0s 680us/step - loss: 0.2198 - predict_loss: 0.0000e+00 - loss_loss: 0.2198 - val_loss: 0.2517 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2517\n",
      "Epoch 1053/1500\n",
      "204/204 [==============================] - 0s 642us/step - loss: 0.2170 - predict_loss: 0.0000e+00 - loss_loss: 0.2170 - val_loss: 0.2515 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2515\n",
      "Epoch 1054/1500\n",
      "204/204 [==============================] - 0s 589us/step - loss: 0.2168 - predict_loss: 0.0000e+00 - loss_loss: 0.2168 - val_loss: 0.2672 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2672\n",
      "Epoch 1055/1500\n",
      "204/204 [==============================] - 0s 523us/step - loss: 0.2246 - predict_loss: 0.0000e+00 - loss_loss: 0.2246 - val_loss: 0.2631 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2631\n",
      "Epoch 1056/1500\n",
      "204/204 [==============================] - 0s 617us/step - loss: 0.2288 - predict_loss: 0.0000e+00 - loss_loss: 0.2288 - val_loss: 0.2534 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2534\n",
      "Epoch 1057/1500\n",
      "204/204 [==============================] - 0s 700us/step - loss: 0.2204 - predict_loss: 0.0000e+00 - loss_loss: 0.2204 - val_loss: 0.2511 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2511\n",
      "Epoch 1058/1500\n",
      "204/204 [==============================] - 0s 698us/step - loss: 0.2207 - predict_loss: 0.0000e+00 - loss_loss: 0.2207 - val_loss: 0.2532 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2532\n",
      "Epoch 1059/1500\n",
      "204/204 [==============================] - 0s 693us/step - loss: 0.2370 - predict_loss: 0.0000e+00 - loss_loss: 0.2370 - val_loss: 0.2599 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2599\n",
      "Epoch 1060/1500\n",
      "204/204 [==============================] - 0s 652us/step - loss: 0.2145 - predict_loss: 0.0000e+00 - loss_loss: 0.2145 - val_loss: 0.2691 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2691\n",
      "Epoch 1061/1500\n",
      "204/204 [==============================] - 0s 582us/step - loss: 0.2254 - predict_loss: 0.0000e+00 - loss_loss: 0.2254 - val_loss: 0.2516 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2516\n",
      "Epoch 1062/1500\n",
      "204/204 [==============================] - 0s 645us/step - loss: 0.2281 - predict_loss: 0.0000e+00 - loss_loss: 0.2281 - val_loss: 0.2612 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2612\n",
      "Epoch 1063/1500\n",
      "204/204 [==============================] - 0s 629us/step - loss: 0.2287 - predict_loss: 0.0000e+00 - loss_loss: 0.2287 - val_loss: 0.2565 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2565\n",
      "Epoch 1064/1500\n",
      "204/204 [==============================] - 0s 610us/step - loss: 0.2308 - predict_loss: 0.0000e+00 - loss_loss: 0.2308 - val_loss: 0.2582 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2582\n",
      "Epoch 1065/1500\n",
      "204/204 [==============================] - 0s 605us/step - loss: 0.2155 - predict_loss: 0.0000e+00 - loss_loss: 0.2155 - val_loss: 0.2644 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2644\n",
      "Epoch 1066/1500\n",
      "204/204 [==============================] - 0s 607us/step - loss: 0.2140 - predict_loss: 0.0000e+00 - loss_loss: 0.2140 - val_loss: 0.2475 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2475\n",
      "Epoch 1067/1500\n",
      "204/204 [==============================] - 0s 597us/step - loss: 0.2244 - predict_loss: 0.0000e+00 - loss_loss: 0.2244 - val_loss: 0.2555 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2555\n",
      "Epoch 1068/1500\n",
      "204/204 [==============================] - 0s 588us/step - loss: 0.2153 - predict_loss: 0.0000e+00 - loss_loss: 0.2153 - val_loss: 0.2506 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2506\n",
      "Epoch 1069/1500\n",
      "204/204 [==============================] - 0s 621us/step - loss: 0.2461 - predict_loss: 0.0000e+00 - loss_loss: 0.2461 - val_loss: 0.2479 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2479\n",
      "Epoch 1070/1500\n",
      "204/204 [==============================] - 0s 636us/step - loss: 0.2234 - predict_loss: 0.0000e+00 - loss_loss: 0.2234 - val_loss: 0.2619 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2619\n",
      "Epoch 1071/1500\n",
      "204/204 [==============================] - 0s 624us/step - loss: 0.2229 - predict_loss: 0.0000e+00 - loss_loss: 0.2229 - val_loss: 0.2513 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2513\n",
      "Epoch 1072/1500\n",
      "204/204 [==============================] - 0s 639us/step - loss: 0.2212 - predict_loss: 0.0000e+00 - loss_loss: 0.2212 - val_loss: 0.2484 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2484\n",
      "Epoch 1073/1500\n",
      "204/204 [==============================] - 0s 635us/step - loss: 0.2136 - predict_loss: 0.0000e+00 - loss_loss: 0.2136 - val_loss: 0.2498 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2498\n",
      "Epoch 1074/1500\n",
      "204/204 [==============================] - 0s 596us/step - loss: 0.2247 - predict_loss: 0.0000e+00 - loss_loss: 0.2247 - val_loss: 0.2501 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2501\n",
      "Epoch 1075/1500\n",
      "204/204 [==============================] - 0s 593us/step - loss: 0.2170 - predict_loss: 0.0000e+00 - loss_loss: 0.2170 - val_loss: 0.2566 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2566\n",
      "Epoch 1076/1500\n",
      "204/204 [==============================] - 0s 556us/step - loss: 0.2207 - predict_loss: 0.0000e+00 - loss_loss: 0.2207 - val_loss: 0.2499 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2499\n",
      "Epoch 1077/1500\n",
      "204/204 [==============================] - 0s 622us/step - loss: 0.2318 - predict_loss: 0.0000e+00 - loss_loss: 0.2318 - val_loss: 0.2534 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2534\n",
      "Epoch 1078/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "204/204 [==============================] - 0s 700us/step - loss: 0.2153 - predict_loss: 0.0000e+00 - loss_loss: 0.2153 - val_loss: 0.2565 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2565\n",
      "Epoch 1079/1500\n",
      "204/204 [==============================] - 0s 596us/step - loss: 0.2231 - predict_loss: 0.0000e+00 - loss_loss: 0.2231 - val_loss: 0.2621 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2621\n",
      "Epoch 1080/1500\n",
      "204/204 [==============================] - 0s 612us/step - loss: 0.2325 - predict_loss: 0.0000e+00 - loss_loss: 0.2325 - val_loss: 0.2528 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2528\n",
      "Epoch 1081/1500\n",
      "204/204 [==============================] - 0s 630us/step - loss: 0.2088 - predict_loss: 0.0000e+00 - loss_loss: 0.2088 - val_loss: 0.2698 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2698\n",
      "Epoch 1082/1500\n",
      "204/204 [==============================] - 0s 602us/step - loss: 0.2272 - predict_loss: 0.0000e+00 - loss_loss: 0.2272 - val_loss: 0.2518 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2518\n",
      "Epoch 1083/1500\n",
      "204/204 [==============================] - 0s 591us/step - loss: 0.2169 - predict_loss: 0.0000e+00 - loss_loss: 0.2169 - val_loss: 0.2538 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2538\n",
      "Epoch 1084/1500\n",
      "204/204 [==============================] - 0s 612us/step - loss: 0.2200 - predict_loss: 0.0000e+00 - loss_loss: 0.2200 - val_loss: 0.2580 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2580\n",
      "Epoch 1085/1500\n",
      "204/204 [==============================] - 0s 565us/step - loss: 0.2108 - predict_loss: 0.0000e+00 - loss_loss: 0.2108 - val_loss: 0.2671 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2671\n",
      "Epoch 1086/1500\n",
      "204/204 [==============================] - 0s 605us/step - loss: 0.2161 - predict_loss: 0.0000e+00 - loss_loss: 0.2161 - val_loss: 0.2622 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2622\n",
      "Epoch 1087/1500\n",
      "204/204 [==============================] - 0s 606us/step - loss: 0.2215 - predict_loss: 0.0000e+00 - loss_loss: 0.2215 - val_loss: 0.2646 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2646\n",
      "Epoch 1088/1500\n",
      "204/204 [==============================] - 0s 651us/step - loss: 0.2266 - predict_loss: 0.0000e+00 - loss_loss: 0.2266 - val_loss: 0.2522 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2522\n",
      "Epoch 1089/1500\n",
      "204/204 [==============================] - 0s 637us/step - loss: 0.2265 - predict_loss: 0.0000e+00 - loss_loss: 0.2265 - val_loss: 0.2523 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2523\n",
      "Epoch 1090/1500\n",
      "204/204 [==============================] - 0s 602us/step - loss: 0.2102 - predict_loss: 0.0000e+00 - loss_loss: 0.2102 - val_loss: 0.2609 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2609\n",
      "Epoch 1091/1500\n",
      "204/204 [==============================] - 0s 565us/step - loss: 0.2161 - predict_loss: 0.0000e+00 - loss_loss: 0.2161 - val_loss: 0.2491 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2491\n",
      "Epoch 1092/1500\n",
      "204/204 [==============================] - 0s 593us/step - loss: 0.2099 - predict_loss: 0.0000e+00 - loss_loss: 0.2099 - val_loss: 0.2497 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2497\n",
      "Epoch 1093/1500\n",
      "204/204 [==============================] - 0s 605us/step - loss: 0.2051 - predict_loss: 0.0000e+00 - loss_loss: 0.2051 - val_loss: 0.2525 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2525\n",
      "Epoch 1094/1500\n",
      "204/204 [==============================] - 0s 601us/step - loss: 0.2117 - predict_loss: 0.0000e+00 - loss_loss: 0.2117 - val_loss: 0.2537 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2537\n",
      "Epoch 1095/1500\n",
      "204/204 [==============================] - 0s 640us/step - loss: 0.2106 - predict_loss: 0.0000e+00 - loss_loss: 0.2106 - val_loss: 0.2492 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2492\n",
      "Epoch 1096/1500\n",
      "204/204 [==============================] - 0s 603us/step - loss: 0.2102 - predict_loss: 0.0000e+00 - loss_loss: 0.2102 - val_loss: 0.2496 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2496\n",
      "Epoch 1097/1500\n",
      "204/204 [==============================] - 0s 605us/step - loss: 0.2041 - predict_loss: 0.0000e+00 - loss_loss: 0.2041 - val_loss: 0.2446 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2446\n",
      "Epoch 1098/1500\n",
      "204/204 [==============================] - 0s 638us/step - loss: 0.1911 - predict_loss: 0.0000e+00 - loss_loss: 0.1911 - val_loss: 0.2461 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2461\n",
      "Epoch 1099/1500\n",
      "204/204 [==============================] - 0s 608us/step - loss: 0.2105 - predict_loss: 0.0000e+00 - loss_loss: 0.2105 - val_loss: 0.2474 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2474\n",
      "Epoch 1100/1500\n",
      "204/204 [==============================] - 0s 619us/step - loss: 0.2068 - predict_loss: 0.0000e+00 - loss_loss: 0.2068 - val_loss: 0.2512 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2512\n",
      "Epoch 1101/1500\n",
      "204/204 [==============================] - 0s 563us/step - loss: 0.2097 - predict_loss: 0.0000e+00 - loss_loss: 0.2097 - val_loss: 0.2474 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2474\n",
      "Epoch 1102/1500\n",
      "204/204 [==============================] - 0s 626us/step - loss: 0.2116 - predict_loss: 0.0000e+00 - loss_loss: 0.2116 - val_loss: 0.2482 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2482\n",
      "Epoch 1103/1500\n",
      "204/204 [==============================] - 0s 614us/step - loss: 0.2031 - predict_loss: 0.0000e+00 - loss_loss: 0.2031 - val_loss: 0.2619 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2619\n",
      "Epoch 1104/1500\n",
      "204/204 [==============================] - 0s 667us/step - loss: 0.2140 - predict_loss: 0.0000e+00 - loss_loss: 0.2140 - val_loss: 0.2453 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2453\n",
      "Epoch 1105/1500\n",
      "204/204 [==============================] - 0s 610us/step - loss: 0.2051 - predict_loss: 0.0000e+00 - loss_loss: 0.2051 - val_loss: 0.2544 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2544\n",
      "Epoch 1106/1500\n",
      "204/204 [==============================] - 0s 602us/step - loss: 0.2131 - predict_loss: 0.0000e+00 - loss_loss: 0.2131 - val_loss: 0.2465 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2465\n",
      "Epoch 1107/1500\n",
      "204/204 [==============================] - 0s 584us/step - loss: 0.2061 - predict_loss: 0.0000e+00 - loss_loss: 0.2061 - val_loss: 0.2451 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2451\n",
      "Epoch 1108/1500\n",
      "204/204 [==============================] - 0s 563us/step - loss: 0.2120 - predict_loss: 0.0000e+00 - loss_loss: 0.2120 - val_loss: 0.2492 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2492\n",
      "Epoch 1109/1500\n",
      "204/204 [==============================] - 0s 619us/step - loss: 0.2110 - predict_loss: 0.0000e+00 - loss_loss: 0.2110 - val_loss: 0.2468 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2468\n",
      "Epoch 1110/1500\n",
      "204/204 [==============================] - 0s 677us/step - loss: 0.2101 - predict_loss: 0.0000e+00 - loss_loss: 0.2101 - val_loss: 0.2516 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2516\n",
      "Epoch 1111/1500\n",
      "204/204 [==============================] - 0s 616us/step - loss: 0.2085 - predict_loss: 0.0000e+00 - loss_loss: 0.2085 - val_loss: 0.2446 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2446\n",
      "Epoch 1112/1500\n",
      "204/204 [==============================] - 0s 586us/step - loss: 0.2089 - predict_loss: 0.0000e+00 - loss_loss: 0.2089 - val_loss: 0.2442 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2442\n",
      "Epoch 1113/1500\n",
      "204/204 [==============================] - 0s 612us/step - loss: 0.2097 - predict_loss: 0.0000e+00 - loss_loss: 0.2097 - val_loss: 0.2509 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2509\n",
      "Epoch 1114/1500\n",
      "204/204 [==============================] - 0s 591us/step - loss: 0.2032 - predict_loss: 0.0000e+00 - loss_loss: 0.2032 - val_loss: 0.2582 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2582\n",
      "Epoch 1115/1500\n",
      "204/204 [==============================] - 0s 608us/step - loss: 0.2196 - predict_loss: 0.0000e+00 - loss_loss: 0.2196 - val_loss: 0.2494 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2494\n",
      "Epoch 1116/1500\n",
      "204/204 [==============================] - 0s 600us/step - loss: 0.2083 - predict_loss: 0.0000e+00 - loss_loss: 0.2083 - val_loss: 0.2470 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2470\n",
      "Epoch 1117/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "204/204 [==============================] - 0s 594us/step - loss: 0.2022 - predict_loss: 0.0000e+00 - loss_loss: 0.2022 - val_loss: 0.2464 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2464\n",
      "Epoch 1118/1500\n",
      "204/204 [==============================] - 0s 627us/step - loss: 0.2096 - predict_loss: 0.0000e+00 - loss_loss: 0.2096 - val_loss: 0.2490 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2490\n",
      "Epoch 1119/1500\n",
      "204/204 [==============================] - 0s 649us/step - loss: 0.2042 - predict_loss: 0.0000e+00 - loss_loss: 0.2042 - val_loss: 0.2582 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2582\n",
      "Epoch 1120/1500\n",
      "204/204 [==============================] - 0s 579us/step - loss: 0.2133 - predict_loss: 0.0000e+00 - loss_loss: 0.2133 - val_loss: 0.2526 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2526\n",
      "Epoch 1121/1500\n",
      "204/204 [==============================] - 0s 625us/step - loss: 0.2040 - predict_loss: 0.0000e+00 - loss_loss: 0.2040 - val_loss: 0.2462 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2462\n",
      "Epoch 1122/1500\n",
      "204/204 [==============================] - 0s 576us/step - loss: 0.2070 - predict_loss: 0.0000e+00 - loss_loss: 0.2070 - val_loss: 0.2469 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2469\n",
      "Epoch 1123/1500\n",
      "204/204 [==============================] - 0s 645us/step - loss: 0.2107 - predict_loss: 0.0000e+00 - loss_loss: 0.2107 - val_loss: 0.2483 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2483\n",
      "Epoch 1124/1500\n",
      "204/204 [==============================] - 0s 520us/step - loss: 0.2121 - predict_loss: 0.0000e+00 - loss_loss: 0.2121 - val_loss: 0.2468 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2468\n",
      "Epoch 1125/1500\n",
      "204/204 [==============================] - 0s 604us/step - loss: 0.2048 - predict_loss: 0.0000e+00 - loss_loss: 0.2048 - val_loss: 0.2444 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2444\n",
      "Epoch 1126/1500\n",
      "204/204 [==============================] - 0s 569us/step - loss: 0.2087 - predict_loss: 0.0000e+00 - loss_loss: 0.2087 - val_loss: 0.2622 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2622\n",
      "Epoch 1127/1500\n",
      "204/204 [==============================] - 0s 623us/step - loss: 0.2170 - predict_loss: 0.0000e+00 - loss_loss: 0.2170 - val_loss: 0.2469 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2469\n",
      "Epoch 1128/1500\n",
      "204/204 [==============================] - 0s 536us/step - loss: 0.2064 - predict_loss: 0.0000e+00 - loss_loss: 0.2064 - val_loss: 0.2416 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2416\n",
      "Epoch 1129/1500\n",
      "204/204 [==============================] - 0s 608us/step - loss: 0.1996 - predict_loss: 0.0000e+00 - loss_loss: 0.1996 - val_loss: 0.2484 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2484\n",
      "Epoch 1130/1500\n",
      "204/204 [==============================] - 0s 633us/step - loss: 0.2080 - predict_loss: 0.0000e+00 - loss_loss: 0.2080 - val_loss: 0.2459 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2459\n",
      "Epoch 1131/1500\n",
      "204/204 [==============================] - 0s 577us/step - loss: 0.1935 - predict_loss: 0.0000e+00 - loss_loss: 0.1935 - val_loss: 0.2426 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2426\n",
      "Epoch 1132/1500\n",
      "204/204 [==============================] - 0s 646us/step - loss: 0.2121 - predict_loss: 0.0000e+00 - loss_loss: 0.2121 - val_loss: 0.2465 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2465\n",
      "Epoch 1133/1500\n",
      "204/204 [==============================] - 0s 624us/step - loss: 0.2079 - predict_loss: 0.0000e+00 - loss_loss: 0.2079 - val_loss: 0.2492 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2492\n",
      "Epoch 1134/1500\n",
      "204/204 [==============================] - 0s 668us/step - loss: 0.1997 - predict_loss: 0.0000e+00 - loss_loss: 0.1997 - val_loss: 0.2500 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2500\n",
      "Epoch 1135/1500\n",
      "204/204 [==============================] - 0s 635us/step - loss: 0.2035 - predict_loss: 0.0000e+00 - loss_loss: 0.2035 - val_loss: 0.2616 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2616\n",
      "Epoch 1136/1500\n",
      "204/204 [==============================] - 0s 589us/step - loss: 0.2096 - predict_loss: 0.0000e+00 - loss_loss: 0.2096 - val_loss: 0.2559 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2559\n",
      "Epoch 1137/1500\n",
      "204/204 [==============================] - 0s 626us/step - loss: 0.1932 - predict_loss: 0.0000e+00 - loss_loss: 0.1932 - val_loss: 0.2471 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2471\n",
      "Epoch 1138/1500\n",
      "204/204 [==============================] - 0s 683us/step - loss: 0.2088 - predict_loss: 0.0000e+00 - loss_loss: 0.2088 - val_loss: 0.2426 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2426\n",
      "Epoch 1139/1500\n",
      "204/204 [==============================] - 0s 582us/step - loss: 0.1974 - predict_loss: 0.0000e+00 - loss_loss: 0.1974 - val_loss: 0.2555 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2555\n",
      "Epoch 1140/1500\n",
      "204/204 [==============================] - 0s 603us/step - loss: 0.2034 - predict_loss: 0.0000e+00 - loss_loss: 0.2034 - val_loss: 0.2511 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2511\n",
      "Epoch 1141/1500\n",
      "204/204 [==============================] - 0s 616us/step - loss: 0.2003 - predict_loss: 0.0000e+00 - loss_loss: 0.2003 - val_loss: 0.2453 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2453\n",
      "Epoch 1142/1500\n",
      "204/204 [==============================] - 0s 591us/step - loss: 0.2020 - predict_loss: 0.0000e+00 - loss_loss: 0.2020 - val_loss: 0.2543 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2543\n",
      "Epoch 1143/1500\n",
      "204/204 [==============================] - 0s 646us/step - loss: 0.2029 - predict_loss: 0.0000e+00 - loss_loss: 0.2029 - val_loss: 0.2538 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2538\n",
      "Epoch 1144/1500\n",
      "204/204 [==============================] - 0s 560us/step - loss: 0.2016 - predict_loss: 0.0000e+00 - loss_loss: 0.2016 - val_loss: 0.2500 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2500\n",
      "Epoch 1145/1500\n",
      "204/204 [==============================] - 0s 565us/step - loss: 0.2091 - predict_loss: 0.0000e+00 - loss_loss: 0.2091 - val_loss: 0.2556 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2556\n",
      "Epoch 1146/1500\n",
      "204/204 [==============================] - 0s 609us/step - loss: 0.2112 - predict_loss: 0.0000e+00 - loss_loss: 0.2112 - val_loss: 0.2461 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2461\n",
      "Epoch 1147/1500\n",
      "204/204 [==============================] - 0s 590us/step - loss: 0.2045 - predict_loss: 0.0000e+00 - loss_loss: 0.2045 - val_loss: 0.2407 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2407\n",
      "Epoch 1148/1500\n",
      "204/204 [==============================] - 0s 689us/step - loss: 0.2056 - predict_loss: 0.0000e+00 - loss_loss: 0.2056 - val_loss: 0.2444 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2444\n",
      "Epoch 1149/1500\n",
      "204/204 [==============================] - 0s 585us/step - loss: 0.2090 - predict_loss: 0.0000e+00 - loss_loss: 0.2090 - val_loss: 0.2450 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2450\n",
      "Epoch 1150/1500\n",
      "204/204 [==============================] - 0s 600us/step - loss: 0.1975 - predict_loss: 0.0000e+00 - loss_loss: 0.1975 - val_loss: 0.2455 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2455\n",
      "Epoch 1151/1500\n",
      "204/204 [==============================] - 0s 636us/step - loss: 0.2012 - predict_loss: 0.0000e+00 - loss_loss: 0.2012 - val_loss: 0.2497 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2497\n",
      "Epoch 1152/1500\n",
      "204/204 [==============================] - 0s 631us/step - loss: 0.1997 - predict_loss: 0.0000e+00 - loss_loss: 0.1997 - val_loss: 0.2472 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2472\n",
      "Epoch 1153/1500\n",
      "204/204 [==============================] - 0s 634us/step - loss: 0.2047 - predict_loss: 0.0000e+00 - loss_loss: 0.2047 - val_loss: 0.2437 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2437\n",
      "Epoch 1154/1500\n",
      "204/204 [==============================] - 0s 568us/step - loss: 0.2037 - predict_loss: 0.0000e+00 - loss_loss: 0.2037 - val_loss: 0.2473 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2473\n",
      "Epoch 1155/1500\n",
      "204/204 [==============================] - 0s 545us/step - loss: 0.1997 - predict_loss: 0.0000e+00 - loss_loss: 0.1997 - val_loss: 0.2450 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2450\n",
      "Epoch 1156/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "204/204 [==============================] - 0s 590us/step - loss: 0.2023 - predict_loss: 0.0000e+00 - loss_loss: 0.2023 - val_loss: 0.2477 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2477\n",
      "Epoch 1157/1500\n",
      "204/204 [==============================] - 0s 601us/step - loss: 0.2030 - predict_loss: 0.0000e+00 - loss_loss: 0.2030 - val_loss: 0.2484 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2484\n",
      "Epoch 1158/1500\n",
      "204/204 [==============================] - 0s 561us/step - loss: 0.2045 - predict_loss: 0.0000e+00 - loss_loss: 0.2045 - val_loss: 0.2449 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2449\n",
      "Epoch 1159/1500\n",
      "204/204 [==============================] - 0s 577us/step - loss: 0.2023 - predict_loss: 0.0000e+00 - loss_loss: 0.2023 - val_loss: 0.2447 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2447\n",
      "Epoch 1160/1500\n",
      "204/204 [==============================] - 0s 629us/step - loss: 0.2013 - predict_loss: 0.0000e+00 - loss_loss: 0.2013 - val_loss: 0.2429 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2429\n",
      "Epoch 1161/1500\n",
      "204/204 [==============================] - 0s 548us/step - loss: 0.2005 - predict_loss: 0.0000e+00 - loss_loss: 0.2005 - val_loss: 0.2452 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2452\n",
      "Epoch 1162/1500\n",
      "204/204 [==============================] - 0s 584us/step - loss: 0.2016 - predict_loss: 0.0000e+00 - loss_loss: 0.2016 - val_loss: 0.2490 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2490\n",
      "Epoch 1163/1500\n",
      "204/204 [==============================] - 0s 580us/step - loss: 0.2000 - predict_loss: 0.0000e+00 - loss_loss: 0.2000 - val_loss: 0.2431 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2431\n",
      "Epoch 1164/1500\n",
      "204/204 [==============================] - 0s 563us/step - loss: 0.2052 - predict_loss: 0.0000e+00 - loss_loss: 0.2052 - val_loss: 0.2434 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2434\n",
      "Epoch 1165/1500\n",
      "204/204 [==============================] - 0s 586us/step - loss: 0.2012 - predict_loss: 0.0000e+00 - loss_loss: 0.2012 - val_loss: 0.2499 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2499\n",
      "Epoch 1166/1500\n",
      "204/204 [==============================] - 0s 611us/step - loss: 0.2003 - predict_loss: 0.0000e+00 - loss_loss: 0.2003 - val_loss: 0.2428 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2428\n",
      "Epoch 1167/1500\n",
      "204/204 [==============================] - 0s 589us/step - loss: 0.1993 - predict_loss: 0.0000e+00 - loss_loss: 0.1993 - val_loss: 0.2449 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2449\n",
      "Epoch 1168/1500\n",
      "204/204 [==============================] - 0s 466us/step - loss: 0.2013 - predict_loss: 0.0000e+00 - loss_loss: 0.2013 - val_loss: 0.2470 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2470\n",
      "Epoch 1169/1500\n",
      "204/204 [==============================] - 0s 581us/step - loss: 0.2001 - predict_loss: 0.0000e+00 - loss_loss: 0.2001 - val_loss: 0.2435 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2435\n",
      "Epoch 1170/1500\n",
      "204/204 [==============================] - 0s 619us/step - loss: 0.2014 - predict_loss: 0.0000e+00 - loss_loss: 0.2014 - val_loss: 0.2463 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2463\n",
      "Epoch 1171/1500\n",
      "204/204 [==============================] - 0s 549us/step - loss: 0.2034 - predict_loss: 0.0000e+00 - loss_loss: 0.2034 - val_loss: 0.2458 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2458\n",
      "Epoch 1172/1500\n",
      "204/204 [==============================] - 0s 577us/step - loss: 0.2020 - predict_loss: 0.0000e+00 - loss_loss: 0.2020 - val_loss: 0.2465 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2465\n",
      "Epoch 1173/1500\n",
      "204/204 [==============================] - 0s 588us/step - loss: 0.2059 - predict_loss: 0.0000e+00 - loss_loss: 0.2059 - val_loss: 0.2475 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2475\n",
      "Epoch 1174/1500\n",
      "204/204 [==============================] - 0s 614us/step - loss: 0.1962 - predict_loss: 0.0000e+00 - loss_loss: 0.1962 - val_loss: 0.2426 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2426\n",
      "Epoch 1175/1500\n",
      "204/204 [==============================] - 0s 601us/step - loss: 0.2008 - predict_loss: 0.0000e+00 - loss_loss: 0.2008 - val_loss: 0.2456 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2456\n",
      "Epoch 1176/1500\n",
      "204/204 [==============================] - 0s 642us/step - loss: 0.1999 - predict_loss: 0.0000e+00 - loss_loss: 0.1999 - val_loss: 0.2428 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2428\n",
      "Epoch 1177/1500\n",
      "204/204 [==============================] - 0s 598us/step - loss: 0.2018 - predict_loss: 0.0000e+00 - loss_loss: 0.2018 - val_loss: 0.2509 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2509\n",
      "Epoch 1178/1500\n",
      "204/204 [==============================] - 0s 651us/step - loss: 0.1865 - predict_loss: 0.0000e+00 - loss_loss: 0.1865 - val_loss: 0.2416 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2416\n",
      "Epoch 1179/1500\n",
      "204/204 [==============================] - 0s 640us/step - loss: 0.1968 - predict_loss: 0.0000e+00 - loss_loss: 0.1968 - val_loss: 0.2434 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2434\n",
      "Epoch 1180/1500\n",
      "204/204 [==============================] - 0s 641us/step - loss: 0.2027 - predict_loss: 0.0000e+00 - loss_loss: 0.2027 - val_loss: 0.2492 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2492\n",
      "Epoch 1181/1500\n",
      "204/204 [==============================] - 0s 590us/step - loss: 0.2056 - predict_loss: 0.0000e+00 - loss_loss: 0.2056 - val_loss: 0.2447 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2447\n",
      "Epoch 1182/1500\n",
      "204/204 [==============================] - 0s 654us/step - loss: 0.2008 - predict_loss: 0.0000e+00 - loss_loss: 0.2008 - val_loss: 0.2420 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2420\n",
      "Epoch 1183/1500\n",
      "204/204 [==============================] - 0s 593us/step - loss: 0.2034 - predict_loss: 0.0000e+00 - loss_loss: 0.2034 - val_loss: 0.2500 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2500\n",
      "Epoch 1184/1500\n",
      "204/204 [==============================] - 0s 650us/step - loss: 0.1957 - predict_loss: 0.0000e+00 - loss_loss: 0.1957 - val_loss: 0.2477 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2477\n",
      "Epoch 1185/1500\n",
      "204/204 [==============================] - 0s 578us/step - loss: 0.2015 - predict_loss: 0.0000e+00 - loss_loss: 0.2015 - val_loss: 0.2402 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2402\n",
      "Epoch 1186/1500\n",
      "204/204 [==============================] - 0s 591us/step - loss: 0.2026 - predict_loss: 0.0000e+00 - loss_loss: 0.2026 - val_loss: 0.2434 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2434\n",
      "Epoch 1187/1500\n",
      "204/204 [==============================] - 0s 568us/step - loss: 0.1962 - predict_loss: 0.0000e+00 - loss_loss: 0.1962 - val_loss: 0.2482 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2482\n",
      "Epoch 1188/1500\n",
      "204/204 [==============================] - 0s 564us/step - loss: 0.1981 - predict_loss: 0.0000e+00 - loss_loss: 0.1981 - val_loss: 0.2451 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2451\n",
      "Epoch 1189/1500\n",
      "204/204 [==============================] - 0s 657us/step - loss: 0.1984 - predict_loss: 0.0000e+00 - loss_loss: 0.1984 - val_loss: 0.2452 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2452\n",
      "Epoch 1190/1500\n",
      "204/204 [==============================] - 0s 697us/step - loss: 0.2011 - predict_loss: 0.0000e+00 - loss_loss: 0.2011 - val_loss: 0.2465 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2465\n",
      "Epoch 1191/1500\n",
      "204/204 [==============================] - 0s 653us/step - loss: 0.2019 - predict_loss: 0.0000e+00 - loss_loss: 0.2019 - val_loss: 0.2419 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2419\n",
      "Epoch 1192/1500\n",
      "204/204 [==============================] - 0s 612us/step - loss: 0.2015 - predict_loss: 0.0000e+00 - loss_loss: 0.2015 - val_loss: 0.2464 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2464\n",
      "Epoch 1193/1500\n",
      "204/204 [==============================] - 0s 638us/step - loss: 0.2027 - predict_loss: 0.0000e+00 - loss_loss: 0.2027 - val_loss: 0.2443 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2443\n",
      "Epoch 1194/1500\n",
      "204/204 [==============================] - 0s 613us/step - loss: 0.2031 - predict_loss: 0.0000e+00 - loss_loss: 0.2031 - val_loss: 0.2455 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2455\n",
      "Epoch 1195/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "204/204 [==============================] - 0s 601us/step - loss: 0.2007 - predict_loss: 0.0000e+00 - loss_loss: 0.2007 - val_loss: 0.2451 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2451\n",
      "Epoch 1196/1500\n",
      "204/204 [==============================] - 0s 580us/step - loss: 0.2002 - predict_loss: 0.0000e+00 - loss_loss: 0.2002 - val_loss: 0.2434 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2434\n",
      "Epoch 1197/1500\n",
      "204/204 [==============================] - 0s 619us/step - loss: 0.1986 - predict_loss: 0.0000e+00 - loss_loss: 0.1986 - val_loss: 0.2445 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2445\n",
      "Epoch 1198/1500\n",
      "204/204 [==============================] - 0s 583us/step - loss: 0.2001 - predict_loss: 0.0000e+00 - loss_loss: 0.2001 - val_loss: 0.2423 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2423\n",
      "Epoch 1199/1500\n",
      "204/204 [==============================] - 0s 555us/step - loss: 0.2020 - predict_loss: 0.0000e+00 - loss_loss: 0.2020 - val_loss: 0.2444 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2444\n",
      "Epoch 1200/1500\n",
      "204/204 [==============================] - 0s 532us/step - loss: 0.2014 - predict_loss: 0.0000e+00 - loss_loss: 0.2014 - val_loss: 0.2434 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2434\n",
      "Epoch 1201/1500\n",
      "204/204 [==============================] - 0s 515us/step - loss: 0.1999 - predict_loss: 0.0000e+00 - loss_loss: 0.1999 - val_loss: 0.2417 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2417\n",
      "Epoch 1202/1500\n",
      "204/204 [==============================] - 0s 546us/step - loss: 0.1964 - predict_loss: 0.0000e+00 - loss_loss: 0.1964 - val_loss: 0.2530 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2530\n",
      "Epoch 1203/1500\n",
      "204/204 [==============================] - 0s 535us/step - loss: 0.1985 - predict_loss: 0.0000e+00 - loss_loss: 0.1985 - val_loss: 0.2477 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2477\n",
      "Epoch 1204/1500\n",
      "204/204 [==============================] - 0s 533us/step - loss: 0.2030 - predict_loss: 0.0000e+00 - loss_loss: 0.2030 - val_loss: 0.2450 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2450\n",
      "Epoch 1205/1500\n",
      "204/204 [==============================] - 0s 521us/step - loss: 0.2004 - predict_loss: 0.0000e+00 - loss_loss: 0.2004 - val_loss: 0.2412 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2412\n",
      "Epoch 1206/1500\n",
      "204/204 [==============================] - 0s 540us/step - loss: 0.1947 - predict_loss: 0.0000e+00 - loss_loss: 0.1947 - val_loss: 0.2425 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2425\n",
      "Epoch 1207/1500\n",
      "204/204 [==============================] - 0s 624us/step - loss: 0.2012 - predict_loss: 0.0000e+00 - loss_loss: 0.2012 - val_loss: 0.2412 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2412\n",
      "Epoch 1208/1500\n",
      "204/204 [==============================] - 0s 640us/step - loss: 0.1978 - predict_loss: 0.0000e+00 - loss_loss: 0.1978 - val_loss: 0.2479 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2479\n",
      "Epoch 1209/1500\n",
      "204/204 [==============================] - 0s 550us/step - loss: 0.2048 - predict_loss: 0.0000e+00 - loss_loss: 0.2048 - val_loss: 0.2437 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2437\n",
      "Epoch 1210/1500\n",
      "204/204 [==============================] - 0s 617us/step - loss: 0.2005 - predict_loss: 0.0000e+00 - loss_loss: 0.2005 - val_loss: 0.2444 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2444\n",
      "Epoch 1211/1500\n",
      "204/204 [==============================] - 0s 551us/step - loss: 0.1944 - predict_loss: 0.0000e+00 - loss_loss: 0.1944 - val_loss: 0.2438 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2438\n",
      "Epoch 1212/1500\n",
      "204/204 [==============================] - 0s 570us/step - loss: 0.2010 - predict_loss: 0.0000e+00 - loss_loss: 0.2010 - val_loss: 0.2453 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2453\n",
      "Epoch 1213/1500\n",
      "204/204 [==============================] - 0s 660us/step - loss: 0.2012 - predict_loss: 0.0000e+00 - loss_loss: 0.2012 - val_loss: 0.2480 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2480\n",
      "Epoch 1214/1500\n",
      "204/204 [==============================] - 0s 690us/step - loss: 0.1946 - predict_loss: 0.0000e+00 - loss_loss: 0.1946 - val_loss: 0.2419 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2419\n",
      "Epoch 1215/1500\n",
      "204/204 [==============================] - 0s 608us/step - loss: 0.2017 - predict_loss: 0.0000e+00 - loss_loss: 0.2017 - val_loss: 0.2404 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2404\n",
      "Epoch 1216/1500\n",
      "204/204 [==============================] - 0s 637us/step - loss: 0.2039 - predict_loss: 0.0000e+00 - loss_loss: 0.2039 - val_loss: 0.2416 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2416\n",
      "Epoch 1217/1500\n",
      "204/204 [==============================] - 0s 616us/step - loss: 0.1959 - predict_loss: 0.0000e+00 - loss_loss: 0.1959 - val_loss: 0.2450 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2450\n",
      "Epoch 1218/1500\n",
      "204/204 [==============================] - 0s 629us/step - loss: 0.1912 - predict_loss: 0.0000e+00 - loss_loss: 0.1912 - val_loss: 0.2409 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2409\n",
      "Epoch 1219/1500\n",
      "204/204 [==============================] - 0s 632us/step - loss: 0.2047 - predict_loss: 0.0000e+00 - loss_loss: 0.2047 - val_loss: 0.2442 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2442\n",
      "Epoch 1220/1500\n",
      "204/204 [==============================] - 0s 633us/step - loss: 0.2008 - predict_loss: 0.0000e+00 - loss_loss: 0.2008 - val_loss: 0.2455 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2455\n",
      "Epoch 1221/1500\n",
      "204/204 [==============================] - 0s 489us/step - loss: 0.1972 - predict_loss: 0.0000e+00 - loss_loss: 0.1972 - val_loss: 0.2446 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2446\n",
      "Epoch 1222/1500\n",
      "204/204 [==============================] - 0s 567us/step - loss: 0.1892 - predict_loss: 0.0000e+00 - loss_loss: 0.1892 - val_loss: 0.2416 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2416\n",
      "Epoch 1223/1500\n",
      "204/204 [==============================] - 0s 613us/step - loss: 0.1962 - predict_loss: 0.0000e+00 - loss_loss: 0.1962 - val_loss: 0.2422 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2422\n",
      "Epoch 1224/1500\n",
      "204/204 [==============================] - 0s 613us/step - loss: 0.1984 - predict_loss: 0.0000e+00 - loss_loss: 0.1984 - val_loss: 0.2456 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2456\n",
      "Epoch 1225/1500\n",
      "204/204 [==============================] - 0s 591us/step - loss: 0.2004 - predict_loss: 0.0000e+00 - loss_loss: 0.2004 - val_loss: 0.2429 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2429\n",
      "Epoch 1226/1500\n",
      "204/204 [==============================] - 0s 618us/step - loss: 0.1941 - predict_loss: 0.0000e+00 - loss_loss: 0.1941 - val_loss: 0.2457 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2457\n",
      "Epoch 1227/1500\n",
      "204/204 [==============================] - 0s 594us/step - loss: 0.2008 - predict_loss: 0.0000e+00 - loss_loss: 0.2008 - val_loss: 0.2416 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2416\n",
      "Epoch 1228/1500\n",
      "204/204 [==============================] - 0s 648us/step - loss: 0.1977 - predict_loss: 0.0000e+00 - loss_loss: 0.1977 - val_loss: 0.2467 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2467\n",
      "Epoch 1229/1500\n",
      "204/204 [==============================] - 0s 652us/step - loss: 0.1969 - predict_loss: 0.0000e+00 - loss_loss: 0.1969 - val_loss: 0.2447 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2447\n",
      "Epoch 1230/1500\n",
      "204/204 [==============================] - 0s 618us/step - loss: 0.1971 - predict_loss: 0.0000e+00 - loss_loss: 0.1971 - val_loss: 0.2444 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2444\n",
      "Epoch 1231/1500\n",
      "204/204 [==============================] - 0s 592us/step - loss: 0.1954 - predict_loss: 0.0000e+00 - loss_loss: 0.1954 - val_loss: 0.2449 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2449\n",
      "Epoch 1232/1500\n",
      "204/204 [==============================] - 0s 647us/step - loss: 0.1994 - predict_loss: 0.0000e+00 - loss_loss: 0.1994 - val_loss: 0.2436 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2436\n",
      "Epoch 1233/1500\n",
      "204/204 [==============================] - 0s 629us/step - loss: 0.1948 - predict_loss: 0.0000e+00 - loss_loss: 0.1948 - val_loss: 0.2414 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2414\n",
      "Epoch 1234/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "204/204 [==============================] - 0s 637us/step - loss: 0.1962 - predict_loss: 0.0000e+00 - loss_loss: 0.1962 - val_loss: 0.2448 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2448\n",
      "Epoch 1235/1500\n",
      "204/204 [==============================] - 0s 587us/step - loss: 0.1936 - predict_loss: 0.0000e+00 - loss_loss: 0.1936 - val_loss: 0.2404 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2404\n",
      "Epoch 1236/1500\n",
      "204/204 [==============================] - 0s 544us/step - loss: 0.1977 - predict_loss: 0.0000e+00 - loss_loss: 0.1977 - val_loss: 0.2419 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2419\n",
      "Epoch 1237/1500\n",
      "204/204 [==============================] - 0s 584us/step - loss: 0.1985 - predict_loss: 0.0000e+00 - loss_loss: 0.1985 - val_loss: 0.2426 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2426\n",
      "Epoch 1238/1500\n",
      "204/204 [==============================] - 0s 547us/step - loss: 0.1978 - predict_loss: 0.0000e+00 - loss_loss: 0.1978 - val_loss: 0.2426 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2426\n",
      "Epoch 1239/1500\n",
      "204/204 [==============================] - 0s 561us/step - loss: 0.1991 - predict_loss: 0.0000e+00 - loss_loss: 0.1991 - val_loss: 0.2407 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2407\n",
      "Epoch 1240/1500\n",
      "204/204 [==============================] - 0s 620us/step - loss: 0.1991 - predict_loss: 0.0000e+00 - loss_loss: 0.1991 - val_loss: 0.2426 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2426\n",
      "Epoch 1241/1500\n",
      "204/204 [==============================] - 0s 614us/step - loss: 0.1984 - predict_loss: 0.0000e+00 - loss_loss: 0.1984 - val_loss: 0.2430 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2430\n",
      "Epoch 1242/1500\n",
      "204/204 [==============================] - 0s 621us/step - loss: 0.1944 - predict_loss: 0.0000e+00 - loss_loss: 0.1944 - val_loss: 0.2420 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2420\n",
      "Epoch 1243/1500\n",
      "204/204 [==============================] - 0s 577us/step - loss: 0.1976 - predict_loss: 0.0000e+00 - loss_loss: 0.1976 - val_loss: 0.2417 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2417\n",
      "Epoch 1244/1500\n",
      "204/204 [==============================] - 0s 644us/step - loss: 0.1954 - predict_loss: 0.0000e+00 - loss_loss: 0.1954 - val_loss: 0.2444 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2444\n",
      "Epoch 1245/1500\n",
      "204/204 [==============================] - 0s 573us/step - loss: 0.2004 - predict_loss: 0.0000e+00 - loss_loss: 0.2004 - val_loss: 0.2431 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2431\n",
      "Epoch 1246/1500\n",
      "204/204 [==============================] - 0s 548us/step - loss: 0.1998 - predict_loss: 0.0000e+00 - loss_loss: 0.1998 - val_loss: 0.2420 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2420\n",
      "Epoch 1247/1500\n",
      "204/204 [==============================] - 0s 603us/step - loss: 0.1966 - predict_loss: 0.0000e+00 - loss_loss: 0.1966 - val_loss: 0.2426 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2426\n",
      "Epoch 1248/1500\n",
      "204/204 [==============================] - 0s 609us/step - loss: 0.1975 - predict_loss: 0.0000e+00 - loss_loss: 0.1975 - val_loss: 0.2416 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2416\n",
      "Epoch 1249/1500\n",
      "204/204 [==============================] - 0s 654us/step - loss: 0.1985 - predict_loss: 0.0000e+00 - loss_loss: 0.1985 - val_loss: 0.2427 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2427\n",
      "Epoch 1250/1500\n",
      "204/204 [==============================] - 0s 572us/step - loss: 0.1795 - predict_loss: 0.0000e+00 - loss_loss: 0.1795 - val_loss: 0.2400 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2400\n",
      "Epoch 1251/1500\n",
      "204/204 [==============================] - 0s 625us/step - loss: 0.1898 - predict_loss: 0.0000e+00 - loss_loss: 0.1898 - val_loss: 0.2412 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2412\n",
      "Epoch 1252/1500\n",
      "204/204 [==============================] - 0s 558us/step - loss: 0.1933 - predict_loss: 0.0000e+00 - loss_loss: 0.1933 - val_loss: 0.2412 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2412\n",
      "Epoch 1253/1500\n",
      "204/204 [==============================] - 0s 560us/step - loss: 0.1982 - predict_loss: 0.0000e+00 - loss_loss: 0.1982 - val_loss: 0.2416 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2416\n",
      "Epoch 1254/1500\n",
      "204/204 [==============================] - 0s 636us/step - loss: 0.1991 - predict_loss: 0.0000e+00 - loss_loss: 0.1991 - val_loss: 0.2410 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2410\n",
      "Epoch 1255/1500\n",
      "204/204 [==============================] - 0s 585us/step - loss: 0.1926 - predict_loss: 0.0000e+00 - loss_loss: 0.1926 - val_loss: 0.2408 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2408\n",
      "Epoch 1256/1500\n",
      "204/204 [==============================] - 0s 634us/step - loss: 0.1978 - predict_loss: 0.0000e+00 - loss_loss: 0.1978 - val_loss: 0.2412 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2412\n",
      "Epoch 1257/1500\n",
      "204/204 [==============================] - 0s 559us/step - loss: 0.1995 - predict_loss: 0.0000e+00 - loss_loss: 0.1995 - val_loss: 0.2404 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2404\n",
      "Epoch 1258/1500\n",
      "204/204 [==============================] - 0s 624us/step - loss: 0.1964 - predict_loss: 0.0000e+00 - loss_loss: 0.1964 - val_loss: 0.2434 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2434\n",
      "Epoch 1259/1500\n",
      "204/204 [==============================] - 0s 616us/step - loss: 0.1954 - predict_loss: 0.0000e+00 - loss_loss: 0.1954 - val_loss: 0.2412 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2412\n",
      "Epoch 1260/1500\n",
      "204/204 [==============================] - 0s 726us/step - loss: 0.1889 - predict_loss: 0.0000e+00 - loss_loss: 0.1889 - val_loss: 0.2430 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2430\n",
      "Epoch 1261/1500\n",
      "204/204 [==============================] - 0s 566us/step - loss: 0.1904 - predict_loss: 0.0000e+00 - loss_loss: 0.1904 - val_loss: 0.2410 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2410\n",
      "Epoch 1262/1500\n",
      "204/204 [==============================] - 0s 576us/step - loss: 0.1901 - predict_loss: 0.0000e+00 - loss_loss: 0.1901 - val_loss: 0.2417 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2417\n",
      "Epoch 1263/1500\n",
      "204/204 [==============================] - 0s 612us/step - loss: 0.1979 - predict_loss: 0.0000e+00 - loss_loss: 0.1979 - val_loss: 0.2454 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2454\n",
      "Epoch 1264/1500\n",
      "204/204 [==============================] - 0s 564us/step - loss: 0.1984 - predict_loss: 0.0000e+00 - loss_loss: 0.1984 - val_loss: 0.2423 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2423\n",
      "Epoch 1265/1500\n",
      "204/204 [==============================] - 0s 641us/step - loss: 0.1981 - predict_loss: 0.0000e+00 - loss_loss: 0.1981 - val_loss: 0.2410 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2410\n",
      "Epoch 1266/1500\n",
      "204/204 [==============================] - 0s 627us/step - loss: 0.1900 - predict_loss: 0.0000e+00 - loss_loss: 0.1900 - val_loss: 0.2407 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2407\n",
      "Epoch 1267/1500\n",
      "204/204 [==============================] - 0s 523us/step - loss: 0.1930 - predict_loss: 0.0000e+00 - loss_loss: 0.1930 - val_loss: 0.2412 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2412\n",
      "Epoch 1268/1500\n",
      "204/204 [==============================] - 0s 590us/step - loss: 0.1974 - predict_loss: 0.0000e+00 - loss_loss: 0.1974 - val_loss: 0.2413 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2413\n",
      "Epoch 1269/1500\n",
      "204/204 [==============================] - 0s 533us/step - loss: 0.1933 - predict_loss: 0.0000e+00 - loss_loss: 0.1933 - val_loss: 0.2444 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2444\n",
      "Epoch 1270/1500\n",
      "204/204 [==============================] - 0s 632us/step - loss: 0.1951 - predict_loss: 0.0000e+00 - loss_loss: 0.1951 - val_loss: 0.2435 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2435\n",
      "Epoch 1271/1500\n",
      "204/204 [==============================] - 0s 601us/step - loss: 0.1979 - predict_loss: 0.0000e+00 - loss_loss: 0.1979 - val_loss: 0.2415 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2415\n",
      "Epoch 1272/1500\n",
      "204/204 [==============================] - 0s 588us/step - loss: 0.1943 - predict_loss: 0.0000e+00 - loss_loss: 0.1943 - val_loss: 0.2412 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2412\n",
      "Epoch 1273/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "204/204 [==============================] - 0s 617us/step - loss: 0.1997 - predict_loss: 0.0000e+00 - loss_loss: 0.1997 - val_loss: 0.2410 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2410\n",
      "Epoch 1274/1500\n",
      "204/204 [==============================] - 0s 535us/step - loss: 0.1937 - predict_loss: 0.0000e+00 - loss_loss: 0.1937 - val_loss: 0.2406 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2406\n",
      "Epoch 1275/1500\n",
      "204/204 [==============================] - 0s 640us/step - loss: 0.1961 - predict_loss: 0.0000e+00 - loss_loss: 0.1961 - val_loss: 0.2438 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2438\n",
      "Epoch 1276/1500\n",
      "204/204 [==============================] - 0s 552us/step - loss: 0.1919 - predict_loss: 0.0000e+00 - loss_loss: 0.1919 - val_loss: 0.2411 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2411\n",
      "Epoch 1277/1500\n",
      "204/204 [==============================] - 0s 621us/step - loss: 0.1879 - predict_loss: 0.0000e+00 - loss_loss: 0.1879 - val_loss: 0.2416 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2416\n",
      "Epoch 1278/1500\n",
      "204/204 [==============================] - 0s 582us/step - loss: 0.1937 - predict_loss: 0.0000e+00 - loss_loss: 0.1937 - val_loss: 0.2398 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2398\n",
      "Epoch 1279/1500\n",
      "204/204 [==============================] - 0s 553us/step - loss: 0.1941 - predict_loss: 0.0000e+00 - loss_loss: 0.1941 - val_loss: 0.2407 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2407\n",
      "Epoch 1280/1500\n",
      "204/204 [==============================] - 0s 595us/step - loss: 0.1984 - predict_loss: 0.0000e+00 - loss_loss: 0.1984 - val_loss: 0.2441 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2441\n",
      "Epoch 1281/1500\n",
      "204/204 [==============================] - 0s 650us/step - loss: 0.1951 - predict_loss: 0.0000e+00 - loss_loss: 0.1951 - val_loss: 0.2421 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2421\n",
      "Epoch 1282/1500\n",
      "204/204 [==============================] - 0s 623us/step - loss: 0.1968 - predict_loss: 0.0000e+00 - loss_loss: 0.1968 - val_loss: 0.2402 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2402\n",
      "Epoch 1283/1500\n",
      "204/204 [==============================] - 0s 606us/step - loss: 0.1954 - predict_loss: 0.0000e+00 - loss_loss: 0.1954 - val_loss: 0.2415 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2415\n",
      "Epoch 1284/1500\n",
      "204/204 [==============================] - 0s 596us/step - loss: 0.1980 - predict_loss: 0.0000e+00 - loss_loss: 0.1980 - val_loss: 0.2420 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2420\n",
      "Epoch 1285/1500\n",
      "204/204 [==============================] - 0s 600us/step - loss: 0.1874 - predict_loss: 0.0000e+00 - loss_loss: 0.1874 - val_loss: 0.2406 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2406\n",
      "Epoch 1286/1500\n",
      "204/204 [==============================] - 0s 495us/step - loss: 0.1986 - predict_loss: 0.0000e+00 - loss_loss: 0.1986 - val_loss: 0.2408 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2408\n",
      "Epoch 1287/1500\n",
      "204/204 [==============================] - 0s 568us/step - loss: 0.1974 - predict_loss: 0.0000e+00 - loss_loss: 0.1974 - val_loss: 0.2414 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2414\n",
      "Epoch 1288/1500\n",
      "204/204 [==============================] - 0s 624us/step - loss: 0.1928 - predict_loss: 0.0000e+00 - loss_loss: 0.1928 - val_loss: 0.2427 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2427\n",
      "Epoch 1289/1500\n",
      "204/204 [==============================] - 0s 624us/step - loss: 0.2009 - predict_loss: 0.0000e+00 - loss_loss: 0.2009 - val_loss: 0.2411 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2411\n",
      "Epoch 1290/1500\n",
      "204/204 [==============================] - 0s 618us/step - loss: 0.1938 - predict_loss: 0.0000e+00 - loss_loss: 0.1938 - val_loss: 0.2398 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2398\n",
      "Epoch 1291/1500\n",
      "204/204 [==============================] - 0s 571us/step - loss: 0.1973 - predict_loss: 0.0000e+00 - loss_loss: 0.1973 - val_loss: 0.2399 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2399\n",
      "Epoch 1292/1500\n",
      "204/204 [==============================] - 0s 586us/step - loss: 0.1965 - predict_loss: 0.0000e+00 - loss_loss: 0.1965 - val_loss: 0.2419 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2419\n",
      "Epoch 1293/1500\n",
      "204/204 [==============================] - 0s 605us/step - loss: 0.1951 - predict_loss: 0.0000e+00 - loss_loss: 0.1951 - val_loss: 0.2423 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2423\n",
      "Epoch 1294/1500\n",
      "204/204 [==============================] - 0s 595us/step - loss: 0.1936 - predict_loss: 0.0000e+00 - loss_loss: 0.1936 - val_loss: 0.2412 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2412\n",
      "Epoch 1295/1500\n",
      "204/204 [==============================] - 0s 599us/step - loss: 0.1977 - predict_loss: 0.0000e+00 - loss_loss: 0.1977 - val_loss: 0.2412 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2412\n",
      "Epoch 1296/1500\n",
      "204/204 [==============================] - 0s 581us/step - loss: 0.1925 - predict_loss: 0.0000e+00 - loss_loss: 0.1925 - val_loss: 0.2412 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2412\n",
      "Epoch 1297/1500\n",
      "204/204 [==============================] - 0s 604us/step - loss: 0.1956 - predict_loss: 0.0000e+00 - loss_loss: 0.1956 - val_loss: 0.2406 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2406\n",
      "Epoch 1298/1500\n",
      "204/204 [==============================] - 0s 630us/step - loss: 0.1972 - predict_loss: 0.0000e+00 - loss_loss: 0.1972 - val_loss: 0.2422 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2422\n",
      "Epoch 1299/1500\n",
      "204/204 [==============================] - 0s 624us/step - loss: 0.1932 - predict_loss: 0.0000e+00 - loss_loss: 0.1932 - val_loss: 0.2416 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2416\n",
      "Epoch 1300/1500\n",
      "204/204 [==============================] - 0s 590us/step - loss: 0.1890 - predict_loss: 0.0000e+00 - loss_loss: 0.1890 - val_loss: 0.2404 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2404\n",
      "Epoch 1301/1500\n",
      "204/204 [==============================] - 0s 615us/step - loss: 0.1982 - predict_loss: 0.0000e+00 - loss_loss: 0.1982 - val_loss: 0.2410 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2410\n",
      "Epoch 1302/1500\n",
      "204/204 [==============================] - 0s 520us/step - loss: 0.1954 - predict_loss: 0.0000e+00 - loss_loss: 0.1954 - val_loss: 0.2414 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2414\n",
      "Epoch 1303/1500\n",
      "204/204 [==============================] - 0s 634us/step - loss: 0.1917 - predict_loss: 0.0000e+00 - loss_loss: 0.1917 - val_loss: 0.2407 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2407\n",
      "Epoch 1304/1500\n",
      "204/204 [==============================] - 0s 588us/step - loss: 0.1933 - predict_loss: 0.0000e+00 - loss_loss: 0.1933 - val_loss: 0.2408 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2408\n",
      "Epoch 1305/1500\n",
      "204/204 [==============================] - 0s 677us/step - loss: 0.1851 - predict_loss: 0.0000e+00 - loss_loss: 0.1851 - val_loss: 0.2400 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2400\n",
      "Epoch 1306/1500\n",
      "204/204 [==============================] - 0s 572us/step - loss: 0.1953 - predict_loss: 0.0000e+00 - loss_loss: 0.1953 - val_loss: 0.2411 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2411\n",
      "Epoch 1307/1500\n",
      "204/204 [==============================] - 0s 641us/step - loss: 0.1927 - predict_loss: 0.0000e+00 - loss_loss: 0.1927 - val_loss: 0.2407 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2407\n",
      "Epoch 1308/1500\n",
      "204/204 [==============================] - 0s 625us/step - loss: 0.1971 - predict_loss: 0.0000e+00 - loss_loss: 0.1971 - val_loss: 0.2411 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2411\n",
      "Epoch 1309/1500\n",
      "204/204 [==============================] - 0s 614us/step - loss: 0.1926 - predict_loss: 0.0000e+00 - loss_loss: 0.1926 - val_loss: 0.2403 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2403\n",
      "Epoch 1310/1500\n",
      "204/204 [==============================] - 0s 653us/step - loss: 0.1955 - predict_loss: 0.0000e+00 - loss_loss: 0.1955 - val_loss: 0.2410 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2410\n",
      "Epoch 1311/1500\n",
      "204/204 [==============================] - 0s 628us/step - loss: 0.1870 - predict_loss: 0.0000e+00 - loss_loss: 0.1870 - val_loss: 0.2408 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2408\n",
      "Epoch 1312/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "204/204 [==============================] - 0s 614us/step - loss: 0.1935 - predict_loss: 0.0000e+00 - loss_loss: 0.1935 - val_loss: 0.2403 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2403\n",
      "Epoch 1313/1500\n",
      "204/204 [==============================] - 0s 545us/step - loss: 0.1940 - predict_loss: 0.0000e+00 - loss_loss: 0.1940 - val_loss: 0.2407 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2407\n",
      "Epoch 1314/1500\n",
      "204/204 [==============================] - 0s 595us/step - loss: 0.1968 - predict_loss: 0.0000e+00 - loss_loss: 0.1968 - val_loss: 0.2407 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2407\n",
      "Epoch 1315/1500\n",
      "204/204 [==============================] - 0s 449us/step - loss: 0.1955 - predict_loss: 0.0000e+00 - loss_loss: 0.1955 - val_loss: 0.2404 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2404\n",
      "Epoch 1316/1500\n",
      "204/204 [==============================] - 0s 496us/step - loss: 0.1976 - predict_loss: 0.0000e+00 - loss_loss: 0.1976 - val_loss: 0.2407 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2407\n",
      "Epoch 1317/1500\n",
      "204/204 [==============================] - 0s 479us/step - loss: 0.1953 - predict_loss: 0.0000e+00 - loss_loss: 0.1953 - val_loss: 0.2404 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2404\n",
      "Epoch 1318/1500\n",
      "204/204 [==============================] - 0s 434us/step - loss: 0.1960 - predict_loss: 0.0000e+00 - loss_loss: 0.1960 - val_loss: 0.2403 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2403\n",
      "Epoch 1319/1500\n",
      "204/204 [==============================] - 0s 467us/step - loss: 0.1950 - predict_loss: 0.0000e+00 - loss_loss: 0.1950 - val_loss: 0.2406 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2406\n",
      "Epoch 1320/1500\n",
      "204/204 [==============================] - 0s 655us/step - loss: 0.1915 - predict_loss: 0.0000e+00 - loss_loss: 0.1915 - val_loss: 0.2406 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2406\n",
      "Epoch 1321/1500\n",
      "204/204 [==============================] - 0s 525us/step - loss: 0.1762 - predict_loss: 0.0000e+00 - loss_loss: 0.1762 - val_loss: 0.2401 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2401\n",
      "Epoch 1322/1500\n",
      "204/204 [==============================] - 0s 622us/step - loss: 0.1952 - predict_loss: 0.0000e+00 - loss_loss: 0.1952 - val_loss: 0.2400 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2400\n",
      "Epoch 1323/1500\n",
      "204/204 [==============================] - 0s 595us/step - loss: 0.1855 - predict_loss: 0.0000e+00 - loss_loss: 0.1855 - val_loss: 0.2397 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2397\n",
      "Epoch 1324/1500\n",
      "204/204 [==============================] - 0s 648us/step - loss: 0.1939 - predict_loss: 0.0000e+00 - loss_loss: 0.1939 - val_loss: 0.2408 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2408\n",
      "Epoch 1325/1500\n",
      "204/204 [==============================] - 0s 576us/step - loss: 0.1929 - predict_loss: 0.0000e+00 - loss_loss: 0.1929 - val_loss: 0.2408 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2408\n",
      "Epoch 1326/1500\n",
      "204/204 [==============================] - 0s 602us/step - loss: 0.1964 - predict_loss: 0.0000e+00 - loss_loss: 0.1964 - val_loss: 0.2407 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2407\n",
      "Epoch 1327/1500\n",
      "204/204 [==============================] - 0s 600us/step - loss: 0.1968 - predict_loss: 0.0000e+00 - loss_loss: 0.1968 - val_loss: 0.2404 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2404\n",
      "Epoch 1328/1500\n",
      "204/204 [==============================] - 0s 580us/step - loss: 0.1928 - predict_loss: 0.0000e+00 - loss_loss: 0.1928 - val_loss: 0.2406 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2406\n",
      "Epoch 1329/1500\n",
      "204/204 [==============================] - 0s 576us/step - loss: 0.1911 - predict_loss: 0.0000e+00 - loss_loss: 0.1911 - val_loss: 0.2405 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2405\n",
      "Epoch 1330/1500\n",
      "204/204 [==============================] - 0s 587us/step - loss: 0.1940 - predict_loss: 0.0000e+00 - loss_loss: 0.1940 - val_loss: 0.2405 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2405\n",
      "Epoch 1331/1500\n",
      "204/204 [==============================] - 0s 611us/step - loss: 0.1955 - predict_loss: 0.0000e+00 - loss_loss: 0.1955 - val_loss: 0.2405 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2405\n",
      "Epoch 1332/1500\n",
      "204/204 [==============================] - 0s 593us/step - loss: 0.1827 - predict_loss: 0.0000e+00 - loss_loss: 0.1827 - val_loss: 0.2415 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2415\n",
      "Epoch 1333/1500\n",
      "204/204 [==============================] - 0s 577us/step - loss: 0.1933 - predict_loss: 0.0000e+00 - loss_loss: 0.1933 - val_loss: 0.2405 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2405\n",
      "Epoch 1334/1500\n",
      "204/204 [==============================] - 0s 588us/step - loss: 0.1916 - predict_loss: 0.0000e+00 - loss_loss: 0.1916 - val_loss: 0.2405 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2405\n",
      "Epoch 1335/1500\n",
      "204/204 [==============================] - 0s 585us/step - loss: 0.1955 - predict_loss: 0.0000e+00 - loss_loss: 0.1955 - val_loss: 0.2422 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2422\n",
      "Epoch 1336/1500\n",
      "204/204 [==============================] - 0s 550us/step - loss: 0.1868 - predict_loss: 0.0000e+00 - loss_loss: 0.1868 - val_loss: 0.2413 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2413\n",
      "Epoch 1337/1500\n",
      "204/204 [==============================] - 0s 567us/step - loss: 0.1858 - predict_loss: 0.0000e+00 - loss_loss: 0.1858 - val_loss: 0.2414 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2414\n",
      "Epoch 1338/1500\n",
      "204/204 [==============================] - 0s 590us/step - loss: 0.1946 - predict_loss: 0.0000e+00 - loss_loss: 0.1946 - val_loss: 0.2410 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2410\n",
      "Epoch 1339/1500\n",
      "204/204 [==============================] - 0s 557us/step - loss: 0.1947 - predict_loss: 0.0000e+00 - loss_loss: 0.1947 - val_loss: 0.2402 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2402\n",
      "Epoch 1340/1500\n",
      "204/204 [==============================] - 0s 532us/step - loss: 0.1927 - predict_loss: 0.0000e+00 - loss_loss: 0.1927 - val_loss: 0.2409 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2409\n",
      "Epoch 1341/1500\n",
      "204/204 [==============================] - 0s 623us/step - loss: 0.1967 - predict_loss: 0.0000e+00 - loss_loss: 0.1967 - val_loss: 0.2399 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2399\n",
      "Epoch 1342/1500\n",
      "204/204 [==============================] - 0s 611us/step - loss: 0.1974 - predict_loss: 0.0000e+00 - loss_loss: 0.1974 - val_loss: 0.2402 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2402\n",
      "Epoch 1343/1500\n",
      "204/204 [==============================] - 0s 708us/step - loss: 0.1923 - predict_loss: 0.0000e+00 - loss_loss: 0.1923 - val_loss: 0.2404 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2404\n",
      "Epoch 1344/1500\n",
      "204/204 [==============================] - 0s 631us/step - loss: 0.1928 - predict_loss: 0.0000e+00 - loss_loss: 0.1928 - val_loss: 0.2399 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2399\n",
      "Epoch 1345/1500\n",
      "204/204 [==============================] - 0s 656us/step - loss: 0.1944 - predict_loss: 0.0000e+00 - loss_loss: 0.1944 - val_loss: 0.2404 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2404\n",
      "Epoch 1346/1500\n",
      "204/204 [==============================] - 0s 678us/step - loss: 0.1929 - predict_loss: 0.0000e+00 - loss_loss: 0.1929 - val_loss: 0.2413 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2413\n",
      "Epoch 1347/1500\n",
      "204/204 [==============================] - 0s 654us/step - loss: 0.1912 - predict_loss: 0.0000e+00 - loss_loss: 0.1912 - val_loss: 0.2403 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2403\n",
      "Epoch 1348/1500\n",
      "204/204 [==============================] - 0s 653us/step - loss: 0.1941 - predict_loss: 0.0000e+00 - loss_loss: 0.1941 - val_loss: 0.2406 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2406\n",
      "Epoch 1349/1500\n",
      "204/204 [==============================] - 0s 629us/step - loss: 0.1947 - predict_loss: 0.0000e+00 - loss_loss: 0.1947 - val_loss: 0.2413 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2413\n",
      "Epoch 1350/1500\n",
      "204/204 [==============================] - 0s 617us/step - loss: 0.1943 - predict_loss: 0.0000e+00 - loss_loss: 0.1943 - val_loss: 0.2400 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2400\n",
      "Epoch 1351/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "204/204 [==============================] - 0s 563us/step - loss: 0.1918 - predict_loss: 0.0000e+00 - loss_loss: 0.1918 - val_loss: 0.2408 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2408\n",
      "Epoch 1352/1500\n",
      "204/204 [==============================] - 0s 610us/step - loss: 0.1953 - predict_loss: 0.0000e+00 - loss_loss: 0.1953 - val_loss: 0.2401 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2401\n",
      "Epoch 1353/1500\n",
      "204/204 [==============================] - 0s 571us/step - loss: 0.1945 - predict_loss: 0.0000e+00 - loss_loss: 0.1945 - val_loss: 0.2409 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2409\n",
      "Epoch 1354/1500\n",
      "204/204 [==============================] - 0s 590us/step - loss: 0.1932 - predict_loss: 0.0000e+00 - loss_loss: 0.1932 - val_loss: 0.2402 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2402\n",
      "Epoch 1355/1500\n",
      "204/204 [==============================] - 0s 605us/step - loss: 0.1925 - predict_loss: 0.0000e+00 - loss_loss: 0.1925 - val_loss: 0.2394 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2394\n",
      "Epoch 1356/1500\n",
      "204/204 [==============================] - 0s 610us/step - loss: 0.1831 - predict_loss: 0.0000e+00 - loss_loss: 0.1831 - val_loss: 0.2392 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2392\n",
      "Epoch 1357/1500\n",
      "204/204 [==============================] - 0s 575us/step - loss: 0.1943 - predict_loss: 0.0000e+00 - loss_loss: 0.1943 - val_loss: 0.2398 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2398\n",
      "Epoch 1358/1500\n",
      "204/204 [==============================] - 0s 540us/step - loss: 0.1958 - predict_loss: 0.0000e+00 - loss_loss: 0.1958 - val_loss: 0.2410 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2410\n",
      "Epoch 1359/1500\n",
      "204/204 [==============================] - 0s 618us/step - loss: 0.1945 - predict_loss: 0.0000e+00 - loss_loss: 0.1945 - val_loss: 0.2411 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2411\n",
      "Epoch 1360/1500\n",
      "204/204 [==============================] - 0s 564us/step - loss: 0.1940 - predict_loss: 0.0000e+00 - loss_loss: 0.1940 - val_loss: 0.2402 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2402\n",
      "Epoch 1361/1500\n",
      "204/204 [==============================] - 0s 616us/step - loss: 0.1952 - predict_loss: 0.0000e+00 - loss_loss: 0.1952 - val_loss: 0.2397 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2397\n",
      "Epoch 1362/1500\n",
      "204/204 [==============================] - 0s 573us/step - loss: 0.1935 - predict_loss: 0.0000e+00 - loss_loss: 0.1935 - val_loss: 0.2413 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2413\n",
      "Epoch 1363/1500\n",
      "204/204 [==============================] - 0s 606us/step - loss: 0.1930 - predict_loss: 0.0000e+00 - loss_loss: 0.1930 - val_loss: 0.2406 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2406\n",
      "Epoch 1364/1500\n",
      "204/204 [==============================] - 0s 556us/step - loss: 0.1943 - predict_loss: 0.0000e+00 - loss_loss: 0.1943 - val_loss: 0.2400 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2400\n",
      "Epoch 1365/1500\n",
      "204/204 [==============================] - 0s 602us/step - loss: 0.1935 - predict_loss: 0.0000e+00 - loss_loss: 0.1935 - val_loss: 0.2406 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2406\n",
      "Epoch 1366/1500\n",
      "204/204 [==============================] - 0s 557us/step - loss: 0.1909 - predict_loss: 0.0000e+00 - loss_loss: 0.1909 - val_loss: 0.2406 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2406\n",
      "Epoch 1367/1500\n",
      "204/204 [==============================] - 0s 608us/step - loss: 0.1935 - predict_loss: 0.0000e+00 - loss_loss: 0.1935 - val_loss: 0.2405 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2405\n",
      "Epoch 1368/1500\n",
      "204/204 [==============================] - 0s 621us/step - loss: 0.1939 - predict_loss: 0.0000e+00 - loss_loss: 0.1939 - val_loss: 0.2414 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2414\n",
      "Epoch 1369/1500\n",
      "204/204 [==============================] - 0s 606us/step - loss: 0.1920 - predict_loss: 0.0000e+00 - loss_loss: 0.1920 - val_loss: 0.2412 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2412\n",
      "Epoch 1370/1500\n",
      "204/204 [==============================] - 0s 598us/step - loss: 0.1931 - predict_loss: 0.0000e+00 - loss_loss: 0.1931 - val_loss: 0.2400 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2400\n",
      "Epoch 1371/1500\n",
      "204/204 [==============================] - 0s 566us/step - loss: 0.1952 - predict_loss: 0.0000e+00 - loss_loss: 0.1952 - val_loss: 0.2401 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2401\n",
      "Epoch 1372/1500\n",
      "204/204 [==============================] - 0s 576us/step - loss: 0.1898 - predict_loss: 0.0000e+00 - loss_loss: 0.1898 - val_loss: 0.2400 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2400\n",
      "Epoch 1373/1500\n",
      "204/204 [==============================] - 0s 537us/step - loss: 0.1938 - predict_loss: 0.0000e+00 - loss_loss: 0.1938 - val_loss: 0.2407 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2407\n",
      "Epoch 1374/1500\n",
      "204/204 [==============================] - 0s 546us/step - loss: 0.1927 - predict_loss: 0.0000e+00 - loss_loss: 0.1927 - val_loss: 0.2407 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2407\n",
      "Epoch 1375/1500\n",
      "204/204 [==============================] - 0s 614us/step - loss: 0.1972 - predict_loss: 0.0000e+00 - loss_loss: 0.1972 - val_loss: 0.2404 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2404\n",
      "Epoch 1376/1500\n",
      "204/204 [==============================] - 0s 564us/step - loss: 0.1985 - predict_loss: 0.0000e+00 - loss_loss: 0.1985 - val_loss: 0.2406 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2406\n",
      "Epoch 1377/1500\n",
      "204/204 [==============================] - 0s 576us/step - loss: 0.1964 - predict_loss: 0.0000e+00 - loss_loss: 0.1964 - val_loss: 0.2399 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2399\n",
      "Epoch 1378/1500\n",
      "204/204 [==============================] - 0s 637us/step - loss: 0.1949 - predict_loss: 0.0000e+00 - loss_loss: 0.1949 - val_loss: 0.2402 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2402\n",
      "Epoch 1379/1500\n",
      "204/204 [==============================] - 0s 573us/step - loss: 0.1940 - predict_loss: 0.0000e+00 - loss_loss: 0.1940 - val_loss: 0.2404 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2404\n",
      "Epoch 1380/1500\n",
      "204/204 [==============================] - 0s 603us/step - loss: 0.1934 - predict_loss: 0.0000e+00 - loss_loss: 0.1934 - val_loss: 0.2404 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2404\n",
      "Epoch 1381/1500\n",
      "204/204 [==============================] - 0s 646us/step - loss: 0.1928 - predict_loss: 0.0000e+00 - loss_loss: 0.1928 - val_loss: 0.2401 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2401\n",
      "Epoch 1382/1500\n",
      "204/204 [==============================] - 0s 589us/step - loss: 0.1947 - predict_loss: 0.0000e+00 - loss_loss: 0.1947 - val_loss: 0.2402 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2402\n",
      "Epoch 1383/1500\n",
      "204/204 [==============================] - 0s 570us/step - loss: 0.1968 - predict_loss: 0.0000e+00 - loss_loss: 0.1968 - val_loss: 0.2401 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2401\n",
      "Epoch 1384/1500\n",
      "204/204 [==============================] - 0s 561us/step - loss: 0.1985 - predict_loss: 0.0000e+00 - loss_loss: 0.1985 - val_loss: 0.2399 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2399\n",
      "Epoch 1385/1500\n",
      "204/204 [==============================] - 0s 645us/step - loss: 0.1884 - predict_loss: 0.0000e+00 - loss_loss: 0.1884 - val_loss: 0.2405 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2405\n",
      "Epoch 1386/1500\n",
      "204/204 [==============================] - 0s 601us/step - loss: 0.1859 - predict_loss: 0.0000e+00 - loss_loss: 0.1859 - val_loss: 0.2402 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2402\n",
      "Epoch 1387/1500\n",
      "204/204 [==============================] - 0s 650us/step - loss: 0.1968 - predict_loss: 0.0000e+00 - loss_loss: 0.1968 - val_loss: 0.2400 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2400\n",
      "Epoch 1388/1500\n",
      "204/204 [==============================] - 0s 644us/step - loss: 0.1933 - predict_loss: 0.0000e+00 - loss_loss: 0.1933 - val_loss: 0.2403 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2403\n",
      "Epoch 1389/1500\n",
      "204/204 [==============================] - 0s 568us/step - loss: 0.1949 - predict_loss: 0.0000e+00 - loss_loss: 0.1949 - val_loss: 0.2413 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2413\n",
      "Epoch 1390/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "204/204 [==============================] - 0s 609us/step - loss: 0.1973 - predict_loss: 0.0000e+00 - loss_loss: 0.1973 - val_loss: 0.2404 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2404\n",
      "Epoch 1391/1500\n",
      "204/204 [==============================] - 0s 651us/step - loss: 0.1966 - predict_loss: 0.0000e+00 - loss_loss: 0.1966 - val_loss: 0.2405 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2405\n",
      "Epoch 1392/1500\n",
      "204/204 [==============================] - 0s 584us/step - loss: 0.1920 - predict_loss: 0.0000e+00 - loss_loss: 0.1920 - val_loss: 0.2403 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2403\n",
      "Epoch 1393/1500\n",
      "204/204 [==============================] - 0s 590us/step - loss: 0.1925 - predict_loss: 0.0000e+00 - loss_loss: 0.1925 - val_loss: 0.2405 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2405\n",
      "Epoch 1394/1500\n",
      "204/204 [==============================] - 0s 564us/step - loss: 0.1937 - predict_loss: 0.0000e+00 - loss_loss: 0.1937 - val_loss: 0.2406 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2406\n",
      "Epoch 1395/1500\n",
      "204/204 [==============================] - 0s 614us/step - loss: 0.1932 - predict_loss: 0.0000e+00 - loss_loss: 0.1932 - val_loss: 0.2409 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2409\n",
      "Epoch 1396/1500\n",
      "204/204 [==============================] - 0s 567us/step - loss: 0.1950 - predict_loss: 0.0000e+00 - loss_loss: 0.1950 - val_loss: 0.2398 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2398\n",
      "Epoch 1397/1500\n",
      "204/204 [==============================] - 0s 587us/step - loss: 0.1927 - predict_loss: 0.0000e+00 - loss_loss: 0.1927 - val_loss: 0.2406 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2406\n",
      "Epoch 1398/1500\n",
      "204/204 [==============================] - 0s 555us/step - loss: 0.1931 - predict_loss: 0.0000e+00 - loss_loss: 0.1931 - val_loss: 0.2400 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2400\n",
      "Epoch 1399/1500\n",
      "204/204 [==============================] - 0s 630us/step - loss: 0.1960 - predict_loss: 0.0000e+00 - loss_loss: 0.1960 - val_loss: 0.2403 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2403\n",
      "Epoch 1400/1500\n",
      "204/204 [==============================] - 0s 558us/step - loss: 0.1971 - predict_loss: 0.0000e+00 - loss_loss: 0.1971 - val_loss: 0.2406 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2406\n",
      "Epoch 1401/1500\n",
      "204/204 [==============================] - 0s 589us/step - loss: 0.1903 - predict_loss: 0.0000e+00 - loss_loss: 0.1903 - val_loss: 0.2400 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2400\n",
      "Epoch 1402/1500\n",
      "204/204 [==============================] - 0s 538us/step - loss: 0.1956 - predict_loss: 0.0000e+00 - loss_loss: 0.1956 - val_loss: 0.2403 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2403\n",
      "Epoch 1403/1500\n",
      "204/204 [==============================] - 0s 583us/step - loss: 0.1937 - predict_loss: 0.0000e+00 - loss_loss: 0.1937 - val_loss: 0.2410 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2410\n",
      "Epoch 1404/1500\n",
      "204/204 [==============================] - 0s 638us/step - loss: 0.1953 - predict_loss: 0.0000e+00 - loss_loss: 0.1953 - val_loss: 0.2399 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2399\n",
      "Epoch 1405/1500\n",
      "204/204 [==============================] - 0s 628us/step - loss: 0.1966 - predict_loss: 0.0000e+00 - loss_loss: 0.1966 - val_loss: 0.2397 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2397\n",
      "Epoch 1406/1500\n",
      "204/204 [==============================] - 0s 599us/step - loss: 0.1924 - predict_loss: 0.0000e+00 - loss_loss: 0.1924 - val_loss: 0.2396 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2396\n",
      "Epoch 1407/1500\n",
      "204/204 [==============================] - 0s 620us/step - loss: 0.1890 - predict_loss: 0.0000e+00 - loss_loss: 0.1890 - val_loss: 0.2396 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2396\n",
      "Epoch 1408/1500\n",
      "204/204 [==============================] - 0s 605us/step - loss: 0.1897 - predict_loss: 0.0000e+00 - loss_loss: 0.1897 - val_loss: 0.2402 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2402\n",
      "Epoch 1409/1500\n",
      "204/204 [==============================] - 0s 584us/step - loss: 0.1940 - predict_loss: 0.0000e+00 - loss_loss: 0.1940 - val_loss: 0.2394 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2394\n",
      "Epoch 1410/1500\n",
      "204/204 [==============================] - 0s 615us/step - loss: 0.1886 - predict_loss: 0.0000e+00 - loss_loss: 0.1886 - val_loss: 0.2390 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2390\n",
      "Epoch 1411/1500\n",
      "204/204 [==============================] - 0s 566us/step - loss: 0.1937 - predict_loss: 0.0000e+00 - loss_loss: 0.1937 - val_loss: 0.2401 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2401\n",
      "Epoch 1412/1500\n",
      "204/204 [==============================] - 0s 575us/step - loss: 0.1976 - predict_loss: 0.0000e+00 - loss_loss: 0.1976 - val_loss: 0.2403 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2403\n",
      "Epoch 1413/1500\n",
      "204/204 [==============================] - 0s 638us/step - loss: 0.1946 - predict_loss: 0.0000e+00 - loss_loss: 0.1946 - val_loss: 0.2405 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2405\n",
      "Epoch 1414/1500\n",
      "204/204 [==============================] - 0s 587us/step - loss: 0.1935 - predict_loss: 0.0000e+00 - loss_loss: 0.1935 - val_loss: 0.2408 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2408\n",
      "Epoch 1415/1500\n",
      "204/204 [==============================] - 0s 575us/step - loss: 0.1970 - predict_loss: 0.0000e+00 - loss_loss: 0.1970 - val_loss: 0.2403 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2403\n",
      "Epoch 1416/1500\n",
      "204/204 [==============================] - 0s 543us/step - loss: 0.1937 - predict_loss: 0.0000e+00 - loss_loss: 0.1937 - val_loss: 0.2413 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2413\n",
      "Epoch 1417/1500\n",
      "204/204 [==============================] - 0s 637us/step - loss: 0.1954 - predict_loss: 0.0000e+00 - loss_loss: 0.1954 - val_loss: 0.2413 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2413\n",
      "Epoch 1418/1500\n",
      "204/204 [==============================] - 0s 644us/step - loss: 0.1961 - predict_loss: 0.0000e+00 - loss_loss: 0.1961 - val_loss: 0.2404 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2404\n",
      "Epoch 1419/1500\n",
      "204/204 [==============================] - 0s 639us/step - loss: 0.1943 - predict_loss: 0.0000e+00 - loss_loss: 0.1943 - val_loss: 0.2396 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2396\n",
      "Epoch 1420/1500\n",
      "204/204 [==============================] - 0s 566us/step - loss: 0.1917 - predict_loss: 0.0000e+00 - loss_loss: 0.1917 - val_loss: 0.2397 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2397\n",
      "Epoch 1421/1500\n",
      "204/204 [==============================] - 0s 574us/step - loss: 0.1918 - predict_loss: 0.0000e+00 - loss_loss: 0.1918 - val_loss: 0.2399 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2399\n",
      "Epoch 1422/1500\n",
      "204/204 [==============================] - 0s 597us/step - loss: 0.1896 - predict_loss: 0.0000e+00 - loss_loss: 0.1896 - val_loss: 0.2398 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2398\n",
      "Epoch 1423/1500\n",
      "204/204 [==============================] - 0s 667us/step - loss: 0.1940 - predict_loss: 0.0000e+00 - loss_loss: 0.1940 - val_loss: 0.2395 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2395\n",
      "Epoch 1424/1500\n",
      "204/204 [==============================] - 0s 674us/step - loss: 0.1781 - predict_loss: 0.0000e+00 - loss_loss: 0.1781 - val_loss: 0.2404 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2404\n",
      "Epoch 1425/1500\n",
      "204/204 [==============================] - 0s 632us/step - loss: 0.1945 - predict_loss: 0.0000e+00 - loss_loss: 0.1945 - val_loss: 0.2400 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2400\n",
      "Epoch 1426/1500\n",
      "204/204 [==============================] - 0s 606us/step - loss: 0.1946 - predict_loss: 0.0000e+00 - loss_loss: 0.1946 - val_loss: 0.2404 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2404\n",
      "Epoch 1427/1500\n",
      "204/204 [==============================] - 0s 625us/step - loss: 0.1911 - predict_loss: 0.0000e+00 - loss_loss: 0.1911 - val_loss: 0.2399 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2399\n",
      "Epoch 1428/1500\n",
      "204/204 [==============================] - 0s 624us/step - loss: 0.1896 - predict_loss: 0.0000e+00 - loss_loss: 0.1896 - val_loss: 0.2403 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2403\n",
      "Epoch 1429/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "204/204 [==============================] - 0s 586us/step - loss: 0.1941 - predict_loss: 0.0000e+00 - loss_loss: 0.1941 - val_loss: 0.2407 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2407\n",
      "Epoch 1430/1500\n",
      "204/204 [==============================] - 0s 594us/step - loss: 0.1883 - predict_loss: 0.0000e+00 - loss_loss: 0.1883 - val_loss: 0.2410 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2410\n",
      "Epoch 1431/1500\n",
      "204/204 [==============================] - 0s 561us/step - loss: 0.1932 - predict_loss: 0.0000e+00 - loss_loss: 0.1932 - val_loss: 0.2403 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2403\n",
      "Epoch 1432/1500\n",
      "204/204 [==============================] - 0s 548us/step - loss: 0.1926 - predict_loss: 0.0000e+00 - loss_loss: 0.1926 - val_loss: 0.2413 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2413\n",
      "Epoch 1433/1500\n",
      "204/204 [==============================] - 0s 589us/step - loss: 0.1962 - predict_loss: 0.0000e+00 - loss_loss: 0.1962 - val_loss: 0.2395 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2395\n",
      "Epoch 1434/1500\n",
      "204/204 [==============================] - 0s 523us/step - loss: 0.1923 - predict_loss: 0.0000e+00 - loss_loss: 0.1923 - val_loss: 0.2410 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2410\n",
      "Epoch 1435/1500\n",
      "204/204 [==============================] - 0s 583us/step - loss: 0.1936 - predict_loss: 0.0000e+00 - loss_loss: 0.1936 - val_loss: 0.2397 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2397\n",
      "Epoch 1436/1500\n",
      "204/204 [==============================] - 0s 610us/step - loss: 0.1849 - predict_loss: 0.0000e+00 - loss_loss: 0.1849 - val_loss: 0.2400 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2400\n",
      "Epoch 1437/1500\n",
      "204/204 [==============================] - 0s 610us/step - loss: 0.1911 - predict_loss: 0.0000e+00 - loss_loss: 0.1911 - val_loss: 0.2402 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2402\n",
      "Epoch 1438/1500\n",
      "204/204 [==============================] - 0s 613us/step - loss: 0.1950 - predict_loss: 0.0000e+00 - loss_loss: 0.1950 - val_loss: 0.2408 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2408\n",
      "Epoch 1439/1500\n",
      "204/204 [==============================] - 0s 631us/step - loss: 0.1902 - predict_loss: 0.0000e+00 - loss_loss: 0.1902 - val_loss: 0.2407 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2407\n",
      "Epoch 1440/1500\n",
      "204/204 [==============================] - 0s 631us/step - loss: 0.1887 - predict_loss: 0.0000e+00 - loss_loss: 0.1887 - val_loss: 0.2405 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2405\n",
      "Epoch 1441/1500\n",
      "204/204 [==============================] - 0s 585us/step - loss: 0.1977 - predict_loss: 0.0000e+00 - loss_loss: 0.1977 - val_loss: 0.2404 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2404\n",
      "Epoch 1442/1500\n",
      "204/204 [==============================] - 0s 573us/step - loss: 0.1925 - predict_loss: 0.0000e+00 - loss_loss: 0.1925 - val_loss: 0.2393 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2393\n",
      "Epoch 1443/1500\n",
      "204/204 [==============================] - 0s 633us/step - loss: 0.1907 - predict_loss: 0.0000e+00 - loss_loss: 0.1907 - val_loss: 0.2402 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2402\n",
      "Epoch 1444/1500\n",
      "204/204 [==============================] - 0s 570us/step - loss: 0.1929 - predict_loss: 0.0000e+00 - loss_loss: 0.1929 - val_loss: 0.2407 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2407\n",
      "Epoch 1445/1500\n",
      "204/204 [==============================] - 0s 565us/step - loss: 0.1924 - predict_loss: 0.0000e+00 - loss_loss: 0.1924 - val_loss: 0.2398 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2398\n",
      "Epoch 1446/1500\n",
      "204/204 [==============================] - 0s 634us/step - loss: 0.1967 - predict_loss: 0.0000e+00 - loss_loss: 0.1967 - val_loss: 0.2398 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2398\n",
      "Epoch 1447/1500\n",
      "204/204 [==============================] - 0s 556us/step - loss: 0.1943 - predict_loss: 0.0000e+00 - loss_loss: 0.1943 - val_loss: 0.2404 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2404\n",
      "Epoch 1448/1500\n",
      "204/204 [==============================] - 0s 585us/step - loss: 0.1828 - predict_loss: 0.0000e+00 - loss_loss: 0.1828 - val_loss: 0.2407 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2407\n",
      "Epoch 1449/1500\n",
      "204/204 [==============================] - 0s 586us/step - loss: 0.1881 - predict_loss: 0.0000e+00 - loss_loss: 0.1881 - val_loss: 0.2408 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2408\n",
      "Epoch 1450/1500\n",
      "204/204 [==============================] - 0s 574us/step - loss: 0.1922 - predict_loss: 0.0000e+00 - loss_loss: 0.1922 - val_loss: 0.2399 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2399\n",
      "Epoch 1451/1500\n",
      "204/204 [==============================] - 0s 598us/step - loss: 0.1938 - predict_loss: 0.0000e+00 - loss_loss: 0.1938 - val_loss: 0.2394 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2394\n",
      "Epoch 1452/1500\n",
      "204/204 [==============================] - 0s 582us/step - loss: 0.1749 - predict_loss: 0.0000e+00 - loss_loss: 0.1749 - val_loss: 0.2393 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2393\n",
      "Epoch 1453/1500\n",
      "204/204 [==============================] - 0s 610us/step - loss: 0.1923 - predict_loss: 0.0000e+00 - loss_loss: 0.1923 - val_loss: 0.2400 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2400\n",
      "Epoch 1454/1500\n",
      "204/204 [==============================] - 0s 578us/step - loss: 0.1953 - predict_loss: 0.0000e+00 - loss_loss: 0.1953 - val_loss: 0.2402 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2402\n",
      "Epoch 1455/1500\n",
      "204/204 [==============================] - 0s 582us/step - loss: 0.1979 - predict_loss: 0.0000e+00 - loss_loss: 0.1979 - val_loss: 0.2400 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2400\n",
      "Epoch 1456/1500\n",
      "204/204 [==============================] - 0s 615us/step - loss: 0.1960 - predict_loss: 0.0000e+00 - loss_loss: 0.1960 - val_loss: 0.2389 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2389\n",
      "Epoch 1457/1500\n",
      "204/204 [==============================] - 0s 632us/step - loss: 0.1932 - predict_loss: 0.0000e+00 - loss_loss: 0.1932 - val_loss: 0.2390 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2390\n",
      "Epoch 1458/1500\n",
      "204/204 [==============================] - 0s 606us/step - loss: 0.1954 - predict_loss: 0.0000e+00 - loss_loss: 0.1954 - val_loss: 0.2392 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2392\n",
      "Epoch 1459/1500\n",
      "204/204 [==============================] - 0s 590us/step - loss: 0.1962 - predict_loss: 0.0000e+00 - loss_loss: 0.1962 - val_loss: 0.2403 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2403\n",
      "Epoch 1460/1500\n",
      "204/204 [==============================] - 0s 635us/step - loss: 0.1917 - predict_loss: 0.0000e+00 - loss_loss: 0.1917 - val_loss: 0.2399 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2399\n",
      "Epoch 1461/1500\n",
      "204/204 [==============================] - 0s 609us/step - loss: 0.1931 - predict_loss: 0.0000e+00 - loss_loss: 0.1931 - val_loss: 0.2408 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2408\n",
      "Epoch 1462/1500\n",
      "204/204 [==============================] - 0s 612us/step - loss: 0.1955 - predict_loss: 0.0000e+00 - loss_loss: 0.1955 - val_loss: 0.2396 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2396\n",
      "Epoch 1463/1500\n",
      "204/204 [==============================] - 0s 578us/step - loss: 0.1949 - predict_loss: 0.0000e+00 - loss_loss: 0.1949 - val_loss: 0.2403 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2403\n",
      "Epoch 1464/1500\n",
      "204/204 [==============================] - 0s 663us/step - loss: 0.1954 - predict_loss: 0.0000e+00 - loss_loss: 0.1954 - val_loss: 0.2395 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2395\n",
      "Epoch 1465/1500\n",
      "204/204 [==============================] - 0s 653us/step - loss: 0.1936 - predict_loss: 0.0000e+00 - loss_loss: 0.1936 - val_loss: 0.2394 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2394\n",
      "Epoch 1466/1500\n",
      "204/204 [==============================] - 0s 688us/step - loss: 0.1922 - predict_loss: 0.0000e+00 - loss_loss: 0.1922 - val_loss: 0.2394 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2394\n",
      "Epoch 1467/1500\n",
      "204/204 [==============================] - 0s 582us/step - loss: 0.1932 - predict_loss: 0.0000e+00 - loss_loss: 0.1932 - val_loss: 0.2401 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2401\n",
      "Epoch 1468/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "204/204 [==============================] - 0s 666us/step - loss: 0.1923 - predict_loss: 0.0000e+00 - loss_loss: 0.1923 - val_loss: 0.2389 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2389\n",
      "Epoch 1469/1500\n",
      "204/204 [==============================] - 0s 623us/step - loss: 0.1977 - predict_loss: 0.0000e+00 - loss_loss: 0.1977 - val_loss: 0.2392 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2392\n",
      "Epoch 1470/1500\n",
      "204/204 [==============================] - 0s 578us/step - loss: 0.1927 - predict_loss: 0.0000e+00 - loss_loss: 0.1927 - val_loss: 0.2389 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2389\n",
      "Epoch 1471/1500\n",
      "204/204 [==============================] - 0s 594us/step - loss: 0.1933 - predict_loss: 0.0000e+00 - loss_loss: 0.1933 - val_loss: 0.2389 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2389\n",
      "Epoch 1472/1500\n",
      "204/204 [==============================] - 0s 600us/step - loss: 0.1793 - predict_loss: 0.0000e+00 - loss_loss: 0.1793 - val_loss: 0.2394 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2394\n",
      "Epoch 1473/1500\n",
      "204/204 [==============================] - 0s 561us/step - loss: 0.1951 - predict_loss: 0.0000e+00 - loss_loss: 0.1951 - val_loss: 0.2393 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2393\n",
      "Epoch 1474/1500\n",
      "204/204 [==============================] - 0s 590us/step - loss: 0.1906 - predict_loss: 0.0000e+00 - loss_loss: 0.1906 - val_loss: 0.2401 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2401\n",
      "Epoch 1475/1500\n",
      "204/204 [==============================] - 0s 552us/step - loss: 0.1916 - predict_loss: 0.0000e+00 - loss_loss: 0.1916 - val_loss: 0.2397 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2397\n",
      "Epoch 1476/1500\n",
      "204/204 [==============================] - 0s 580us/step - loss: 0.1896 - predict_loss: 0.0000e+00 - loss_loss: 0.1896 - val_loss: 0.2401 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2401\n",
      "Epoch 1477/1500\n",
      "204/204 [==============================] - 0s 560us/step - loss: 0.1914 - predict_loss: 0.0000e+00 - loss_loss: 0.1914 - val_loss: 0.2396 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2396\n",
      "Epoch 1478/1500\n",
      "204/204 [==============================] - 0s 570us/step - loss: 0.1956 - predict_loss: 0.0000e+00 - loss_loss: 0.1956 - val_loss: 0.2394 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2394\n",
      "Epoch 1479/1500\n",
      "204/204 [==============================] - 0s 631us/step - loss: 0.1910 - predict_loss: 0.0000e+00 - loss_loss: 0.1910 - val_loss: 0.2395 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2395\n",
      "Epoch 1480/1500\n",
      "204/204 [==============================] - 0s 546us/step - loss: 0.1881 - predict_loss: 0.0000e+00 - loss_loss: 0.1881 - val_loss: 0.2394 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2394\n",
      "Epoch 1481/1500\n",
      "204/204 [==============================] - 0s 511us/step - loss: 0.1864 - predict_loss: 0.0000e+00 - loss_loss: 0.1864 - val_loss: 0.2396 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2396\n",
      "Epoch 1482/1500\n",
      "204/204 [==============================] - 0s 633us/step - loss: 0.1950 - predict_loss: 0.0000e+00 - loss_loss: 0.1950 - val_loss: 0.2396 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2396\n",
      "Epoch 1483/1500\n",
      "204/204 [==============================] - 0s 589us/step - loss: 0.1926 - predict_loss: 0.0000e+00 - loss_loss: 0.1926 - val_loss: 0.2401 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2401\n",
      "Epoch 1484/1500\n",
      "204/204 [==============================] - 0s 569us/step - loss: 0.1970 - predict_loss: 0.0000e+00 - loss_loss: 0.1970 - val_loss: 0.2394 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2394\n",
      "Epoch 1485/1500\n",
      "204/204 [==============================] - 0s 581us/step - loss: 0.1950 - predict_loss: 0.0000e+00 - loss_loss: 0.1950 - val_loss: 0.2405 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2405\n",
      "Epoch 1486/1500\n",
      "204/204 [==============================] - 0s 559us/step - loss: 0.1957 - predict_loss: 0.0000e+00 - loss_loss: 0.1957 - val_loss: 0.2398 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2398\n",
      "Epoch 1487/1500\n",
      "204/204 [==============================] - 0s 567us/step - loss: 0.1920 - predict_loss: 0.0000e+00 - loss_loss: 0.1920 - val_loss: 0.2397 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2397\n",
      "Epoch 1488/1500\n",
      "204/204 [==============================] - 0s 576us/step - loss: 0.1857 - predict_loss: 0.0000e+00 - loss_loss: 0.1857 - val_loss: 0.2395 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2395\n",
      "Epoch 1489/1500\n",
      "204/204 [==============================] - 0s 630us/step - loss: 0.1850 - predict_loss: 0.0000e+00 - loss_loss: 0.1850 - val_loss: 0.2386 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2386\n",
      "Epoch 1490/1500\n",
      "204/204 [==============================] - 0s 592us/step - loss: 0.1817 - predict_loss: 0.0000e+00 - loss_loss: 0.1817 - val_loss: 0.2388 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2388\n",
      "Epoch 1491/1500\n",
      "204/204 [==============================] - 0s 559us/step - loss: 0.1935 - predict_loss: 0.0000e+00 - loss_loss: 0.1935 - val_loss: 0.2398 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2398\n",
      "Epoch 1492/1500\n",
      "204/204 [==============================] - 0s 562us/step - loss: 0.1826 - predict_loss: 0.0000e+00 - loss_loss: 0.1826 - val_loss: 0.2393 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2393\n",
      "Epoch 1493/1500\n",
      "204/204 [==============================] - 0s 620us/step - loss: 0.1921 - predict_loss: 0.0000e+00 - loss_loss: 0.1921 - val_loss: 0.2395 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2395\n",
      "Epoch 1494/1500\n",
      "204/204 [==============================] - 0s 619us/step - loss: 0.1937 - predict_loss: 0.0000e+00 - loss_loss: 0.1937 - val_loss: 0.2397 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2397\n",
      "Epoch 1495/1500\n",
      "204/204 [==============================] - 0s 615us/step - loss: 0.1944 - predict_loss: 0.0000e+00 - loss_loss: 0.1944 - val_loss: 0.2388 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2388\n",
      "Epoch 1496/1500\n",
      "204/204 [==============================] - 0s 652us/step - loss: 0.1925 - predict_loss: 0.0000e+00 - loss_loss: 0.1925 - val_loss: 0.2392 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2392\n",
      "Epoch 1497/1500\n",
      "204/204 [==============================] - 0s 603us/step - loss: 0.1849 - predict_loss: 0.0000e+00 - loss_loss: 0.1849 - val_loss: 0.2385 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2385\n",
      "Epoch 1498/1500\n",
      "204/204 [==============================] - 0s 587us/step - loss: 0.1852 - predict_loss: 0.0000e+00 - loss_loss: 0.1852 - val_loss: 0.2400 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2400\n",
      "Epoch 1499/1500\n",
      "204/204 [==============================] - 0s 586us/step - loss: 0.1847 - predict_loss: 0.0000e+00 - loss_loss: 0.1847 - val_loss: 0.2388 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2388\n",
      "Epoch 1500/1500\n",
      "204/204 [==============================] - 0s 578us/step - loss: 0.1810 - predict_loss: 0.0000e+00 - loss_loss: 0.1810 - val_loss: 0.2394 - val_predict_loss: 0.0000e+00 - val_loss_loss: 0.2394\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f1c7ac51c18>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now run the training!\n",
    "INPUT_LENGTH = 8\n",
    "OUTPUT_LENGTH = 8\n",
    "TOTAL_LENGTH = INPUT_LENGTH + OUTPUT_LENGTH\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "inp_ph,target_ph, model, params, loss = vanilla_lstm_model(128,\n",
    "                                                           INPUT_LENGTH,\n",
    "                                                           OUTPUT_LENGTH,\n",
    "                                                          1e-3)\n",
    "model.summary()\n",
    "indices, input_batch, target_batch = get_batch(normalized_data,BATCH_SIZE,INPUT_LENGTH,OUTPUT_LENGTH)\n",
    "\n",
    "# prepare data\n",
    "input_batch_padded = np.hstack([input_batch,np.zeros((BATCH_SIZE,OUTPUT_LENGTH,2))])\n",
    "target_batch_padded = np.hstack([np.zeros((BATCH_SIZE,INPUT_LENGTH,2)),target_batch])\n",
    "\n",
    "\n",
    "# and train\n",
    "model.fit(\n",
    "    [input_batch_padded,target_batch_padded],\n",
    "    [np.zeros((BATCH_SIZE,TOTAL_LENGTH,2)),np.zeros(BATCH_SIZE)],\n",
    "    epochs = 1500,\n",
    "    validation_split = 0.2,\n",
    "    callbacks = get_callbacks(input_batch_padded,target_batch_padded)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finetuning\n",
    "# model.compile(optimizer=optimizers.RMSprop(lr = 1e-5, clipvalue = 10.),\n",
    "#               loss= {\n",
    "#                     'predict': lambda _, loss: loss - loss,\n",
    "#                       'loss': lambda _, loss: loss\n",
    "#                 })\n",
    "# # and train\n",
    "# model.fit(\n",
    "#     [input_batch_padded,target_batch_padded],\n",
    "#     [np.zeros((BATCH_SIZE,TOTAL_LENGTH,2)),np.zeros(BATCH_SIZE)],\n",
    "#     epochs = 500,\n",
    "    \n",
    "#     callbacks = get_callbacks(input_batch_padded,target_batch_padded, True)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Function that takes a \"close\" look of each tragectory\n",
    "    Input:\n",
    "        model: trained Keras model\n",
    "        input: the batch of input of shape (N, input_length + target_length, 2)\n",
    "        target: the batch of input of the same shape with input\n",
    "        input_length: self-explanatory\n",
    "    Output:\n",
    "        Nothing, but generate N pictures of the plot with:\n",
    "            target tragectories (should be input_length + target_length long) in black,\n",
    "            predicted tragectories (shoudl be target_length long) in blue\n",
    "'''\n",
    "def close_visualize(model, input_batch,target_batch,input_length):\n",
    "    if not input_batch.shape == target_batch.shape:\n",
    "        raise ValueError(\"input batch and target batch should have the same size\")\n",
    "    batch_size,_,__ = input_batch.shape\n",
    "    \n",
    "    prediction, loss = model.predict([input_batch,target_batch])\n",
    "    for batch_id in range(batch_size):\n",
    "        # first clear the plot...\n",
    "        plt.gcf().clear()\n",
    "        \n",
    "        # then retrieve the tragectories\n",
    "        target_tragectories = target_batch[batch_id][input_length:]\n",
    "        predicted_tragectories = prediction[batch_id]\n",
    "        # evaluate the boundary to plot\n",
    "        x_min = np.min(target_tragectories[:,0] - .1)\n",
    "        x_max = np.max(target_tragectories[:,0] + .1)\n",
    "        \n",
    "        y_min = np.min(target_tragectories[:,1] - .1)\n",
    "        y_max = np.max(target_tragectories[:,1] + .1)\n",
    "        \n",
    "        x_min_predicted = np.min(predicted_tragectories[:,0] - .1)\n",
    "        x_max_predicted = np.max(predicted_tragectories[:,0] + .1)\n",
    "        \n",
    "        y_min_predicted = np.min(predicted_tragectories[:,1] - .1)\n",
    "        y_max_predicted = np.max(predicted_tragectories[:,1] + .1)\n",
    "        \n",
    "        # set the boundary\n",
    "        x_min = min(x_min,x_min_predicted)\n",
    "        x_max = max(x_max,x_max_predicted)\n",
    "        y_min = min(y_min,y_min_predicted)\n",
    "        y_max = max(y_max,y_max_predicted)\n",
    "        \n",
    "        plt.xlim(x_min,x_max)\n",
    "        plt.ylim(y_min,y_max)\n",
    "        # plot line...\n",
    "        # first the target tragectories\n",
    "#         plt.plot(target_tragectories[:,0],target_tragectories[:,1],c = 'black')\n",
    "        # then the prediction\n",
    "        for i in range(len(target_tragectories) - 1):\n",
    "            cur_point = target_tragectories[i,:]\n",
    "            next_point = target_tragectories[i + 1,:]\n",
    "            plt.plot([cur_point[0],next_point[0]],[cur_point[1],next_point[1]],c = 'black')\n",
    "            # and predicted...\n",
    "            cur_point = predicted_tragectories[i,:]\n",
    "            next_point = predicted_tragectories[i + 1,:]\n",
    "            plt.plot([cur_point[0],next_point[0]],[cur_point[1],next_point[1]],\n",
    "                     c = 'blue',\n",
    "                     linestyle = ':'\n",
    "            )\n",
    "#         plt.plot(predicted_tragectories[:,0],\n",
    "#                   predicted_tragectories[:,1],\n",
    "#                   linestyle = ':',\n",
    "#                   c = 'blue')\n",
    "        # then save the plot...\n",
    "        plt.savefig('close-{}.png'.format(batch_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "close_visualize(model,input_batch_padded,target_batch_padded,INPUT_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get an overview of the prediction given the whole scene...\n",
    "def visualize_batch_overview(input_batch_padded, target_batch_padded, filename = 'overview.png'):\n",
    "\n",
    "    params, loss = model.predict([input_batch_padded,target_batch_padded])\n",
    "    # visualize the trace, as well as the distributions generated by the params...\n",
    "    # first clear the previous drawing...\n",
    "    #     try:\n",
    "    plt.gcf().clear()\n",
    "    visualize_trace(input_batch,target_batch)\n",
    "    #     params = params[:,INPUT_LENGTH + 1, :] # (B,5), and it should be the params immediately after the input\n",
    "    #     draw_heatmap(params)\n",
    "    draw_mean(params)\n",
    "    plt.savefig(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzs3Xd4VUX+x/H33J7eOy2ELr1Kk64oCNgACxZWURTr2ntZ26JrXVdR7AUVEWyACkgHKQFCD+kJCek9ue3M74/wU1llVRISE76v5/ExXM45M3MSPncyd86M0lojhBCiZTE1dQWEEEI0PAl3IYRogSTchRCiBZJwF0KIFkjCXQghWiAJdyGEaIF+N9yVUm8qpfKVUrt/8VqoUuo7pVTy0f+HnNxqCiGE+DP+SM/9bWD8f712N7BCa90RWHH0z0IIIf4i1B95iEkp1Q74Smvd/eifDwAjtda5SqkY4AetdeeTWVEhhBB/nOUEz4vSWuce/ToPiDregUqpWcAsAD8/v35dunQ5wSLF/1JZo8kp86KUxqoM3CZFK4cNX/+mrpkQor62bdtWqLWO+DPnnGi4/0RrrZVSx+3+a63nAfMA+vfvr7du3VrfIsV/mf5+DrPsz9JrzTZ+rLZxXdQIims18T2ncldgAmecX+9vsxCiCSmlMv7sOSf6r/6IUirmF8My+Sd4nfpLTobly8HfH6ZMgeDgJqtKU7j2/U28+9R5WA/kk9rmOt7wfM8A0wf0/WY53so4PucIPXNiCI5TTV1VIUQjOtGpkF8AVxz9+gpgScNU50+6+27o2RN9263oWVejI8Pw3jsDXVt5csorLYV77kK3jcPTIYbCe2aQllyC4T05xf2eRT9s4b7Ft2E9kM+iiDP4pJ+NsokzyGw9A/ujeRgVbmYtvY+XPitvmgoKIZrM7/bclVIfASOBcKVUNvAQ8BTwiVLqb0AGMPVkVvI3rV4NL70EtbX8sk9qeuZDXObDJN/7Ct19GvAz3qoqdJ/ekJWJ8mosQNgzHxL63Uoeee9FWu+ewNUXORquvN+jNaPvuZjAfTnUekzsi5tAcYeziRuXw/gsB22D7WRnJqGfLaLy8z3AkMarmxCiyf1uz11rfbHWOkZrbdVat9Jaz9daF2mtx2itO2qtx2qtixujssd45x2oqfnVywqFdddh/Da8zO2H91LjMRqmvHffhZxslPfnjxeUx0DtyOX2T+fx6en7+GiZu2HK+gO8336OT3U1yu1l3xUDcEyJwlJQS9zngbhtB8jfXkZ7z1LenngrHWoKGq1eQoi/hub7hKrTCb+YxvlI1yf4JPZiys3+KK9BRHYBwcEH6b8xj0pXAwT8smUo98/jL16s5ASeR5lPb/y2ZDAyKJV/5FbUv5w/SO/eiaWwktqEMCqGRhAfl0FgeRVRqSH47u7M3w5OYtgTr+I8UzM6QXrtQpxqmm+4X3IJOOqGQWpNdoLdJWwMH86THR/mY5/zKFJBBFGLNbKSq1+qpmBvPctr2xZ9zGeSHlYMmMyLl14I/jb8tJvyCA81zsbZ/MQ84hxMRyqxlNUw4Ik1dP7gecZunU/bzN1ckXMVNX2jCTlSSNfsFBK6/KkZVEKIFuAPPcTUUBp0KqTWMH06+vOF4DZQJkWKbwJf9r+cREsvdJAN8zmwqW8crqRYZl4XxnnvQLcLf+M6S5bAc/9CHzkMo4aiHnkKImOOPe7AAXSP047pvcftvoby4GoyP6/kzhm3sGpxT/ZfFoLF3DgzU7y94lH70sFugxAftNWM4ePAklmEqnTi7hmD2pyBxW5tlPoIIU4OpdQ2rXX/P3NO8+25KwULFqAXvIUxrB16YBztLwhg9pnruW32RqKH15Jc5iR/6XeU5XzM3oR9fHE1eF3/dZ377sMz4xKMdetQB1Lg9ffRXdqTsfJ5jnnj69wZ/e58tI8VbTWhzSa0uRSU4p7J3fkhvQ2TlF+jBXtJKhy46wDZg67BCLSjciswpRVj3XMYql1ou4XyZ/8lwS7EKar59tx/weOpZlv2B9hrcqj082V3aCzbTG1YtSeEkg2boKpuKqC91sadvc/glvN71Z2Ylwft2nHwjHYYZhOttmTiX1QFFhN6eFuef/UfhK29kIsvsWH1qTvFqCmn4ONH8ZRnsbZfDK/GR5OTfB6DDrThjSsdOGwnN9w9eQVkzrmc1ok/YoT6su3qCTzvvJ/5CRn4PjsH0rPx9kzA+sQrqG59T2pdhBCN40R67i0i3P/fkowqHizPxWNSFJU68GgT7Zf6031BMSsvWE5FcAUoiA8O4v0Lp9Bl/Tr0pRdTHOVL+tB4DJOJ2F05xOw8jGoXzOFnLmBi1zs5d3YC939txuZ3bHkFZZrUXIPWESZiw05+j10fOYLRqR2mKifKq/GE+4PTQ9W03lw6YiFfXRZ30usghGh8JxLuLeq59Mlt/Rhb256nPnaRuErTdrWNiHQTigBmPjuL2gF5bJz1HYeKi8mrrKJLZCQYBmFpxQQeLiNzYFvyesRR3DaUhMOlOGqddI0q5Kuzojj7tSAG33ZseRFBioggc+M0zuOheurZ+FY6UYamdMognBMGQHkNEfe+wwXXfUZO9hziWjXfkTYhRMNpcUng5zDx6OUObo7wITbPjD1A4e9fztiAx3i0eAyrXp7LXksNI1rHwuDBEBmJVmB1eklYm0qbjWlgNpHSvw0pFgc1hpnidh72LvyDFdD6mCmaDcLrxTWoB47dB1CGpiYigqox4wAw5RajDINRq9ewPbOB5vQLIZq9FhfuUPdZ67i5cOMhmPxKNTeGDGRw9WNYkvdCYhL+d96HHpyAO2sH6oc16JhgtMWEtpkJzyyho9uFivDjyfxY1j2/GnP6YXxCf6fQ4mL0heehbVa02YyzUyzrbvyEzPX1b49esghr0kGw1f2WsPzCKaTW+GD6fg8Rz3wOhqYiNIgu8S3y2ymEOAEtOg0C46Bb5bvYCjNQ7p+fHlVuA5WYjXp6NstDq1BZheS8ewu1Nw2jZu4E8m8dy7abppHRvgeekjKKsxbwXv+3MIzj9IwNAz3kdPh8CcrjRWmN7VAuQ+dfyqb8J3jlSWe92uH9cD7KbYDTwzNXj+axThYe5TBJOw/VfQejA5jXaQYdY1r0t1MI8Se0/DRYvhxqan/1sjKZSEs7Qsye9xixJhfVay5HHv2Gq0Y/xpXtruGx7DEUVY0goepCbMrCpoJDtLvjH6w5kPLrMlauhPQ0lPGLpQl03ZvIBa+9xTvnprBtx4mvLqbCwtAmMJXU4BkSj2+XUHru2s+oQznocD+WPHMbj/Uafsw5xr69FL90P4kfvUZuYf3eXIQQzU+L+kD1N7VujTapY4IXYHewiY+oInLxSiKvHMr5i/z5R0UwHz/dk0M5Bon7IdJQ9J8bgT2kB5NefJONKRlMfvFNrhw2kH9Nn4RSR2fIHDgAnmPDO7tHKBEHS7HlVjDdbxv3rmzF8t6BJ9QE020PwpsLUIbBrLs/YTYG5u5xZM4YQeL5o2jd/V6C/Y5+sOty4TpzMNZ1OwgxK0IMjY67m3+98CLXjrgUv+CW/34uhDgVeu6zZ4P1vx7kUdDZZWJwWDCpRWWYXn0Sp2M+616tJWsDdIgzcdE0EyMuVvhFgMVi4ZvbZvH+rEuxW8y8ve5HHv3i25+v160bmH++lZUhdvaMb8Pim3oyO9DLwQ++IN3hOeEmqM6dcc+9H201EVJaTWBpLX4bU3HE2dgSOJt+7X6eseO9fQ7W9Ykor4FyeUk+05+sNh5uuf92rl6RfMJ1EEI0Ly1qnvtxfbYQPeNi8BgoDTrYAed2oWJMFxYP6MGnn+3Gu3svcTk9mTrlHsbdcvylgmtdLl78fh1n9+hCj9axdS9qjbdDHKb0vJ9+QzjSPoBPBkXw/Z4CNKDNvrz77U5Cwo+7I+Hv0mXFFM5/hOLaMjadcSlju40mLvTYqZhGoC+mirrVMp0OxfZrwgk7VEvHbyv5asUjnN7mfiLiZeMOIZqTU36e+3FdcCHeM4dTPG82QflFGHHBVHaJYXuvLmwO7MXuc6bjb9vGBX1TCL78Q7YV22nvdxEh9o6/upTDZuPOc0Yf+6JSmLYk4ZwyCPvGNNCaSGXhmvZtSR04mOT3luOw2vAPCqlXM1RQKBG3vUAEcLy3H+X8eX2F7Kg2uEscBBzMBkMTVVFCXipExNerGkKIZuDUCHfAEhBF6K2f8lrBZlz2LIrsfhyojWDHwUgqSx102zCRoo1ezBe/hdeRz6Gq9/GpiaFrwFWYzfbfvb4KDcOx5hBbMldRmbWewrAgPvbvz54t7Tnv9jd5bKoFs/nkj4J5+nXBsmkPSkOtCiFsHUSkV0GEHyvj+3NjzO9fQwjR/J0y4Q5gMZm5IWoIn+1w8WZiNWX+HnwKLAz/ype+a630uFLRJ+YGCp2JZFR/SY2RS2LZ03QOuJIAa5s/VMaANqPQrUdyIEsTUQSdRyhiQhtvGMTyxgLo1wfX/3++qzVmq4nia4eSvW48ftfKkIwQp4JTY8z9N2ityTkEqUtA1yg6TYSYPj//vdfrJbV6IeXuQ8T6nEGMz/DjX+wvxpt2kMKbLqNyTwUBQV4O3XYWy3we5JELIlCS7UI0O6f8wmEng1c7UZgxqVPqlxwhxF+IfKB6EpjV74+3CyHEX03Ln+cuhBCnIAl3IYRogSTchRCiBZJwF0KIFkjCXQghWiAJdyGEaIEk3IUQogWScG9ohgHz50PvntC2FVw1FXLSm7pWQohTjIR7Q5s5Ez3netiZBJk56PcWont2w/Xd601dMyHEKUTCvSEdPAgfL0DVuvj/RR2UV6MqnFif+SfLDq3E24jLPQDg9cLaNfDNF1BV2bhlCyGajIR7Q9q48adQ39ZvKN+OnUxZQBC4Dcgs5bSdH9BzXQ5FzhPfT/VP2bABHRaMHjsafd756LBgyu++AMNz4rtCCSGaBwn3hhQbC0dXXVSGl7T4jnw87Wq29h2MEeTAz+UiPuEI535afvLrUlGBHjcGVVaJcnnr/nN6CXh2MSn/uZIdh9wnvw5CiCYj4d6QRo+G4GC0gn6Jm5j6yXzaZqRwoGsvvjp9EgfDWhPo5yK5tZPK0pM8PLNoEbiPDXBtAaU1CQvXc1/NTvJTjZNbByFEk5Fwb0hmM2r9RrytQ9FmE8FVZYxb/zX9TGkUBYezdnsIAxd/RVh1Abkne6/q4mLw/hze1QNtlNwYiNcHVJWLkaGHeOht50muhBCiqciSvw0tPh7S8tj75d9pt/MAKtof/85xBHeMIH+nl2nvL+A8PsYxcArevk9jNpt//5onYvRoMCs4umG3JceDYVekndWTeKtBsa8/+12NNPYvhGh09Qp3pdStwNWABpKAq7TWtQ1RsebMYrLS4dzneGZAIrXBuTiVlXRPCLtaRZB6ui/3bnoG14+fc/ia5YReOxe/QeMbvhK9euE5ewSWpatRbgNbjsHGH69nc8Z1dB68ix+r2tD2sLy3C9FSnfCwjFIqDrgJ6K+17g6YgekNVbHmzm4yc19sfwZ9M55NLwwi6d1OBH8USY+MmcT+ZxvW+O7grKb4xRvIf/JyDLerwetgXbyCrMdm4u0Yhm4TRBtvIdUVEWz/dgxZ2yKZPUY2IhGipTrhbfaOhvsmoBdQDiwGXtRaf3u8c5rjNnsNxV0DFjuoX7ydVu9aS/GLN6BrqvAbexmhVz1yUsr+NruIr1w7qQ1S2Ab3x1ZmJX6kiTkLbLKnqhDNQKPvoaqUuhl4HKgBvtVaX/obx8wCZgG0adOmX0ZGxgmX1xIZhkHNpq+xte+BNbrdSS3ryEFNQYpBaY5myEwzJpMkuxDNQaOGu1IqBPgMmAaUAp8CC7XW7x/vnFO55y6EECfqRMK9PlMhxwJpWusCrbUbWAQMqcf1hBBCNJD6hHsmcLpSylcppYAxwL6GqZYQQoj6OOFw11pvBhYC26mbBmkC5jVQvYQQQtRDvSY6a60fAh5qoLoIIYRoILL8gBBCtEAS7i1Bbi5cfTU6IgwdG45xw8VQVtjUtRJCNCF5/ry5KymBPn3QBfmoo+vIMO8T9NpVOD/7GEfHEU1bPyFEk5Cee3M3bx66uAhlaNzhJjSgPAbszcf8yr3MS8+lPg+qCSGaJwn35m7lSpTbg9dXceCjaNKfCsMVbUaZTVhSigg0FjFmSYkEvBCnGAn35q5TJ7QJTE5N5JvluFpZSPtnOAVTfDACrUTpKg53KGXeB7+z85LbDV99BS8+C1s3gLwZCNGsyZh7c3fjjfD6ayinm8iPKglcV8Phm4I58rdgSmyQH6jw8Xp475CTWfo4C4Xt2YMecQZUlNWt/641Rp9WeD9ZiC1+YKM3SQhRf9Jzb+46dYLFSzCCHGizCXuOQbuXvUSkhFERZ8OaGMAVSSn4VTpxVfzG+VrDxIlQVFy3z6rHQHk1ph05WG6+goVpOxq9SUKI+pOeewugxp9NRf5h0pY/RKuCfJyxwRzsnsD6gARi95vpXllGt9O+J+NgVzr173XsyUlJ6Lzc/9/Xm/SuE2h98DvMHhfsOEzP/W8wZvdDfDchXFaRFKIZkXBvIQJtIbSf+Cz3Ju/BG1dCqeEgrSCItKBgbk9Kp1dsJpl79pF7KJW+Z44gIDS07sSaGv4/2YtienCg32VkdD2HLpveINKdTGhJKZYzsnjg8UAef0A29xCiuZBhmRYkQFl5sWNvxj4/gsrrexPxUDwzLo2ia84QhkyZiN3PF7fTyeYvvyU/I7vupL59wVL3Hh+Wm0T3dS/jNZnZMeYuEsfdQVFUHIG+br6x1lKV34SNE0L8KfXarOPPkvXcG4+nFsqywD8a7AE/v561P5mM3fto16MbrTp3qHtxyWL0RReAx0BpcPv4snf4LHIHjqcmPJtdkTaWpE3io+5BdJ7UNO0R4lR2Iuu5y7BMC2VxQFjHX7/euktH4joloH45bebcSejzzkMtXIS2mrC4auiR/SHV1wfj3GlicLaL/t7n8XqnA50brQ1CiBMnwzKnIJPJdGy4z52LWvwFytB1M2a8GtOBAk5/+DUyh4aRTygWk4fsDe+z8YVXMTyepqu8EOIPkXAX8NxzKJebct+O7Gz/EC5LEHg17D7C1Vu/5N2Lx+E3ZDLKbKIiO4dtb7zb1DUWQvwOGZYRUFoKQH7wGeSFjqEwaCCdsl+lVeGXWIuqmNB+DzOyR7Ljrgco3LoWv4jwJq6wEOL3SM9dwOmD0ECHw/MZsP9GfJ15HGx1A1u7v0C5bztMJgNru2r++Q+DhHGjiO7do6lrLIT4HRLuAl54EexWtILQyp0M2j+L1oWfUxh1BmsP3kd2ZgXa7WVPuoHxR4fb9+2DSRPQYUHo+EjcD1+NUVlyUpshhPiZDMsI6NULduykdvYUHHtyUKEO4kfuwD7yI74K6I71UDIXkkJ1yHi8rlGYfu+nZu9e9ID+UFOD0kAxWB5/C715Lfvmz6NrrKwxL8TJJvPcxU+01nxcsRKfqv24rBZ2+LYixRuKZ3MynSuSUECAI4CZQ2cRERh9/Auddx568WIU4AUMwApou4XKZyZw3bAneKNrF3zs8oujEH/Eicxzl39d4idKKaYHjiG4YCavrB/PF7tOY/PSDpR9eBHqgVvxUQFUOCt4YeWzfL932fEvtGHDT2vVLA305f1Qf3ItZpTXwHdvLkPbbmfMa+W4qxulWUKckiTcxa+M6OnD573b8s8P2nHvMxHMwYebP4nlvikPMqbLWZiVmR8OriC3NOe3LxAX+9OXHZwuak2Kj0P82GS34vW3E6idlA0v4YO7jUZqkRCnHhlzF78puC2c/dKvXx/VZSzDOo4gteAQMcFxv33ygw+hp16Icnvp4vQQ46rkmxBf1oQHkHLITUqlP16LZssazZUntRVCnLqk5y7+NKvZSuforr/9l2vXwsMPoq0mtN2MtpoINMHUKB96D21L+rpYLF2W0PPh7/EplCWEhThZpOcuGs6mTegzx6FqnZgAr8XEj6/NIirIS2nHaFLadiZ/QSWRN/9Ar69XYXHsw137FFaHLCUsREOTnrtoOPffj6p1AvB1/4l4MDP4b6/y3Sfh3Fh9OfPSB/F18HlsH/MwYMVbm8dzMddQllV4/GvW1MD8N9CTxmNcOQVj3deyv6sQf4CEu2g4u3YCsK9VV+ZecC9X3/Qu2xL6ce2ip1n2yg1cuqCCM14I5Np+nbgp4y1aD++Kx+khZdlxtvIrKYHu3dCzr0N9uRz1/heo0ZOovWEoR6pSG7FhQjQ/Eu6i4XSoW2O4a/Y+7vrkUYoDQrn16pd58vz78MRBu7//yPa3MllY4SIgysyM7x7k+j3P0+fqMb99vSeeQGdkotxeAJRXo9wG9g+3Y/7sfp46mNZYLROi2ZFwFw3nscfQVjMAZycu48O5FzFqzwo+HzWNG/veh62sELOCJJuH6qMjMcHxkccuP/xLH3+M8v48XdIVHorHx4GqdBHyYypBkRuY+USVjNII8Rsk3EXDGTMG3nsXI9gHrSDQU8XDh17ln7zNrODPmbjjG77ffQODqvZTnv0Hrme3/fSlBgrHDCdv0ngqO8SDSRHpqWDHiAJWvyrpLsR/k3AXDUpNuwRdUMra7f8i5Zvr2fH6VWRN78qigWeSYY8i0Kjm2ZE3EVZ0I4bX+78vdsMctLXuR1QBISvXYXK5KD7jdHZ2GcI+RxSGv5elCyTchfhvEu6iwZktNob3uoVNcY9wu3k6r5UP4qvkbpyV/SYLlt+I1ibMpdvhm3PRlcd5yhVgzhwYPRJtMaEtJnxKS4hetQp7iIsHzpjKg5ZxJIeEooobrWlCNBsyz12cFEopLusSwrikED6bo8nYrYkyK7rcPh3TlMmw8W4o2gkVGeB/nCddLRbUshW4N3xL8XtPEGDzUj6iE4mDelAd6gNATZSNF9fUMMfroJXZ3IgtFOKvTVaFFE1GO8tQ9qA/dKxhGFy1PpfctoVUYaYwJRjH+hD2XwvuQGgFpNt8MR3vw1khmrETWRWyXj13pVQw8AbQnbrPvGZqrTfW55ri1PFHgx3qNvV+Z3gcmYmxfP6KpjYT+g9QnK4V75ldhKAk2IX4hfoOy7wALNNaX6iUsgG+DVAnIY6rTR/Fza8fG+LXYTvO0UKcuk443JVSQcAZULewn9baBbgaplpCCCHqoz6zZeKBAuAtpVSiUuoNpZTffx+klJqllNqqlNpaUFBQj+KEEEL8UfUJdwvQF/iP1roPUAXc/d8Haa3naa37a637R0RE1KM4IYQQf1R9wj0byNZabz7654XUhb0QQogmdsLhrrXOA7KUUp2PvjQG2NsgtRJCCFEv9Z0tcyPwwdGZMqnAVfWvkhBCiPqqV7hrrXcAf2pivRBCiJNP1pYRQogWSMJdCCFaIAl3IYRogSTchRCiBZJwF6I+Fi1Cd+2E9vfB6BpHzWt343XVNHWthJBwF+KEvfIK+pKLUfuTUVW1mPYfxjFnLt5bx7C9fFNT106c4iTchfgtbjds3Qr79vGbO3C73XDPPShn3Vp5Kf0jKYvwQXkMrEv3E71qPg9u2IurqpHrLcRREu5C/LcvvkCHh6GHDUb37I6O8Mf53j+OPSY3F1xOANw2M7mdQtgxvi2pfSLQRyoJPZRLaM8k7rqkFreM0ogmIOEuxC8dPAjTpqLKK1BOD8pjoIqqsV3/GOkLb8DpddcdFx5etz0NYHV56bc4hYCCajJ6R7B9YjuqLOCnXWyaVsLOd2UDb9H4JNyF+KV589CuX29LoJwe2ny8mheLFrF6jxt8fWHmVWhL3T8hn2oPfZZl0n57PlWtAvih1pedS+04TV52f93YjRBCwl2IY2Vno4yfe9rabMMb0R6NQlU4GVqWws3VmWxapuGFF+HSqWiLCW1W4LDQJjKYTj07sLZmLCXfOOm0bDXuiLwmbJA4VdV34TAhWpazz0YvWohyewEwwlpjhLXBCAiHCIXbYSU0rorHX3byRf9q1PkX4506je+cibT1K6GgfSw/RrWnwBNH1cNH8C2t4gfv9zj/0445s4c3cePEqUTCXYhfmj4dHn8EnZaO8mhM+Slow42O78I+WxTzMqtwRWumZDwHsQ+hlYFJa0b72PnbC2tJCgrFnWeGrUGcdnAgIwYfYE1qIhs3p1NYWMndd4zF4bA2dSvFKUDCXYhfsttRiUnU/P1iHEtWgVlh6utL+dggng8PI6ugHFP+O3RxL0W53T+dZnPX8O5NQ9j/+rWURD9DaG8rHa5TWBzdmV6WwDPPraKsvBbU/yhbiAak9G/N4T1J+vfvr7du3dpo5QlRH9urU0gp+hYfi5f9AVHsIYKDq1Zg9lQCcMu8lZy/bNfPJ1hNeC/qwb9euI9uByYzYajtmOu5XB5sNulPiT9PKbVNa/2nlleXnzQhjqOvbwIxxjXM+Caf/Dg31Tm+hCYO5KKUmXw3MRj/qrp57h5lJydgMOHuPfhVOulXm8lt5hwCP2nH8Kk/d9Ul2EVjkp82If6HGH8L30+NJXWj5mA6RJ8G3WNHccOsOVicHgDcJh+8ykKeYyBBZi+GxURgqyqen+tm6AU2TOambYM4NclUSCH+gPaDFePvVPS+TGG55gos8W3h/+e4e0tpXb0Ba5RmX24H/vnP1lSXQVG4lwLZVVg0EQl3If4shwOVuJuaOy7D6BCG7haBuqwrfo9E8tawAWSkOii/Lx1rWiG20b0wHr4ZXLVNXWtxipFhGSFOhMOB7xPvsPOBHLaUfo+vxc0Bv0hcE8Kx3ViNu6iWTJ+dbLAGcM7jL+PZ+i3OD7/DL7BV3fl798JnC8FZAdOmQ49+Tdse0eJIz12IeujlE8f5IZfy4bLJfLa6H5s+6sJtecO4M783EW4bi9uMRXkMzCsOUbn4bt5Kz4f770f36Y1++CH0U8+i+wzANW0ArorDdRfdtQvGjUEH+KFjQvDefhm6qqRpGyqaHem5C1FPoQ4LX82IoDRLk3nhVfj7xmOKGsvw6hD8j+SgAeX2EpaUSdbkDVyaPYX3XU8cnfJeNxXZ+tk2arpezs4JtzNg5AXo6uq6v6+sxvTCR+it69n79jOc1u6CJmunaF6k5y5EAwlurejZ7kfaFz5MbPIl+FftwRl5LUe6Y8E5AAAgAElEQVTi/4Pb1grDz0YrTwmBXZPYHnw+2H6eRqOUwmddGm0eugtdUxfshoKKIFW3MuWGTNp/Mp8Hk3+gNFNWmRS/T8JdiIb0xJNoqwmbK5OolJkE572E2yeBzF4vs6Yyhgt7/J2XH5pF34pFaENjtAvmSFRn8nVrtFKEbk9DHc3uZZf58syLoWw40w6Aff8RTg9M4qJPCkn5tgnbKJoFCXchGtLkyehnHkf7WVEmCCr7gMjAh9nScSVf7AzmsU4XkuITifLquh55djlVtnBSWvclLb89ZXFRP12q6xYXfmUGS2YF8OpDARRGm3F43TgnFfDGDQaGB9i/H96cD0s+A/evlyoWpy5ZfkCIk6C8Koes1XOJqCqltF0kB3xCSbtyM4eC4wDokZ/KVUlLsQBGkIMDl1zAts3R7Ag8Qo0nmX+v2wyA1wzfzPBj/QWBqFYhxNpG8pH9errM6MjLfpfhs2Yx6LoVLPGzUfb8bPxnPIXFZDtOzURzdCLLD0i4C3GS1BpertuTQmVMCeaMMj4YPIFtoe35qNtojJJQzCU2rjnyNt2sh6n85wQe6jqZl95fi9vkwcftJumzJbSvroFuEaRe05UPR4Blcxpu5SD2g3H8bd1rxyxehgIdE0Da6zPwjHyUTr5hTdd40aBOJNxlWEaIk8RhMvN2j07cnTSAsH+PpNwWw6AjB3n6h9cJqazEW+PHa4GzWRlxFjXhQTgGRhA14nbMXis1Visdp1/IknlXcnDeDHZNn0pJwl3kh/TGYtRScPGXvH1HKF4zVGLjAwZSpW2ovEpar9jFKtdSnrnGRW1pU98F0VSk5y5EY/nxR/QZQ8HjRXk1icFdWGE5C7N/KEX9h3B4djTfVUZyzsNhFFz5Kt8m7+KyybPZFu2HthuUZfgTuDqYM/d9Q9DY29EWha3aS+8H7HyiBxLiquYy5xa6XRPLqkcu5sHd53PRnDbcckD6cM2drAopxF/ZwIHoA/uouXMqPsl59IrTdBqXxpLKBHauHYF+ysQwm0FEoOK1m27A6XZjt1pxuTVrftAc2QU9eym63n8RzPmaT/yXkt3RTmj3fA4PNshbHsa/skczLt2Jv7ag2lVzwKP54REY+VBTN140Num5C9HIPIaXeUXfEaYzKbL7cdAWRfKPrQl9KgGzt66XPfVqOGfq/+hxFxWhW8dQ7dD4lnjID/PjsUsGcyg7noC9FlwvTeNIaDynXRVL+2wz9xYplGwU0mzJmLsQzYDFZOb6iPG4sy7hrdUj+GbFaaRvi6HMYRB+dAe+z96GirL/0fEKC0N99CmWtpEYPaKJ6BTKM/4eptzs4MDz4ezw30+4KYDSuyup8HpxVTRK08RfiAzLCNFELuvnzyW9/Nm7TFMdCPFPKCK6QnmJQXY6BAT9Tld78mRsrWPZPvdhPn76ekKDasm1BhNQ4IvxYgZ6mhdXNwt7lpVziABOQ/ZuPZXIsIwQLcDTh3axdaciLa0VgQftxP9oYmfFa4TOHY8tPhJlaB5K8KV/sKOpqypOgAzLCNGSGQa89hp0SECHBqPPGQl76/ZwvatDT24c2JnAaE1+71oODdYMs89h4oJciud+ietIKfdf8wGF+TI+c6qo97CMUsoMbAVytNYT618lIcRvuuEG9JvzUS43CtDLVqNX96fys+cIGH8DZ7S2sfKyULQGz8qNWJZfAp9nca02uGXlNPb7t5MPVU8hDdFzvxnY1wDXEUIcz+HD8OabKJcbfXSxYKVBOT34P/I0L6eswOU16l7PzsI6aSwqLR3l8WL3al4pWMCy4pfIP/gqhmE0ZUtEI6lXuCulWgETgDcapjpCiN+0Ywf66ArBxW1HUBw3GK/JCl4NOeWMS13CwDVZ7NpSN3Sjncdu66cMsFfW0PG1j7j38Hfs3ettgkaIxlTfnvvzwJ3AcbsCSqlZSqmtSqmtBQUF9SxOiFNUmzbgNdCAtaYYt18ERfFjqPWPgWAHvh43XXocZtaiGty7DqKO9uLXT3me5D7T0UfHY8xHKhhr7ObSQ3msekXWhW/JTjjclVITgXyt9bb/dZzWep7Wur/Wun9ERMSJFifEqa17d+iUACZFYP4ugrM3AZqyuIGUxvUjp01rzDYoaeNmp2so2mrCbfXFa7Gz//RrWH3RPIrDu6JjArCaND69SnljgYfy7KZumDhZ6tNzHwpMUkqlAwuA0Uqp9xukVkKIX1E/rMd7WizarLA5CwjNX4s1xk2V6kLElK4kvO3FUmtiveVKCArE4q1m+MLZdF3/CjWB0ay95DW2d5pJpiUQrzZR2Eqzb1FTt0qcLCcc7lrre7TWrbTW7YDpwEqt9WUNVjMhxLHCwjDtSGf3igeomDuRqv9MIff53iTN6IUjz8S19xYx719b6NTBF5L24R7RFXytJGR8xYjUBwiPyqBoZS5tunzJlBs/IaDIhCFD7y2WPKEqRDNiMlnoMeIR3uqdzo+k4bWYSIkMJuAZC/+6M4X4kkp4eynOSadjX7GHlYeXE5q9Fk+gnbToWvLCIhmwWTNs1Y+czkXEXHQXcHpTN0ucBPKEqhDNVFmRwX1zXewvMgg/aKVPreIy73rU/lKwQNjSYdgHh7G5pJY79hyhwt9DRbofcd8b3PnuAwQaeQDEzJpIwtxZKLP5d0oUTUWW/BXiFBIUZuLlpxx43WC4weoLMIKa745Q9WIKlo7+AAwKcbBmWFvSEzU7t0NQJHT/cT6e7StIu/t1Sn/YgTY0SrK9RZGeuxCnMK01nqJyrOFBTV0V8T/I2jJCiD9FKSXB3kJJuAshRAsk4S6EEC2QhLsQQrRAEu5CCNECSbgLIUQLJPPchRAn7vBh+ORjKC6AKedB3wFNXSNxlIS7EOLEfPABeuZVYHjB0PDEU7jHdse0aA0W3+Cmrt0pT4ZlhBB/XmEh+m8z67b88xgoQ6O8GuuKPdTeP5k7EzMoq/rFNg+1tTDvNfSIIeizhmF8Oh8a8QHKU5GEuxDiZ243LFgAF54PV18BOxN/+7ivvwb96z16lMfAb3UyA8K+ZNjCYjZ9a0BNDZw+CD3nBtSajfDtetQls3BOPo3Pczbh8cq2fyeDhLsQok5tLQwZgr5iBnz2OfrNd9H9++O9b9avjzWMX/W8NVDtqBvp7VCYh//wfO7/yE3Js4vQ+/ayJmEI33Yfi9NsQ3kMbEsPMPrTJ7kyZyOP3e2murAR2ngKkXAXQtR56y30zh0ol4fKoKC6Dbg9Bqa58/nqmxdJyv3F4u8TJ9al+S8cah/KD8Pbsb9bJKXhgfjZ3RTGe1g93w/l8lAYEMbaTkN5+czr2dquL5gUAetSONd/DwtG5vPIOIOa4sZtcksm4S6EqPP+eyi3h4qQELZOPJvdw4bgttlQSjF28QfMKj/Apa/VUFWjYdEidIANbVZokwIFUUXV+PlY2RcdROJ71ZiSNJZqE0WuBDRwwZbFzFzzNkE15SzrdRZvDL2STHskoa5q/DqXs6uzhx9fbuqb0HJIuAsh6vjYAfArLaXN7j2URkWyc8woSiIisHg8DG+Vweq4Ch57rQp9002YSmpw9onDdW5XvMPa4juuA31igvGGxFOVNZBhS0o5s3AbrUb5grVuPeH4wgxm/vAWZyZ9h9Nu54MevXnOtBO3kU1loMGhZf9Vp5ISePYZ9PnnYNw5C2/Gvka+Kc2XhLsQos6NN6MtJkxa035nEt3XrEcrRdLwYRxs0wW79qBia1mTZqHWYgPAvjWbTzK8JFfZMH2xj4D5m7jw4fc4a8uboDU+Vg+7InOpveOOul6+zYzZAgNyErkqfTHWsO2kevOp9n2e8pl/x9yx7Of67N8P8e3Qd9+F+nwp6vn5mE7rzeF/T+XubSv5YWE+ntqmuVXNgYS7EKLOpMm4zx1YF8JWE8GlRfReu5rQADdZrmAOrdmMqzQNbUCt2ReA1JBAtkdH8mZ4JJsjwlEejXIbDEz8jLuXXgjhHsyBduyPPo6RnETJ9WMw/O1gaPz2H+aO61bz+K07sbs0znbpPP3UpdxV/Cxaa5g5E11WTtX4s6kcMxqNGVXtIualldz45bNEvXIOh/sOIvGO6/lyaxbOiia+f38xslmHEOInhtdD7qc3ELl4E8qscA6JJ31IF760teHljYswPLX4lvZl34P/xuqu+4A1LSSQ93p2ptDXh275RUxKziOmvAhMisKXL2DK2IcIW5jAR7c68HvwLvS/niWx6wRCS7Jok7sTZTGhDc1Dz/Xk0/NjwQTdTQm8H/8fHNUejrz4PK62bTAXFuG7dh2+K77HmpeLchtoBZhN6GFtWfbqbF7fez5/y2rLWQlrsOzbiDchBs/4ydgcIZhU8+3LnshmHRLuQohjaK35pmojpTV7Kbf7cNAayY6SKA5sL4fMh8HkJAALC5/cQvfDNaA1bg2LuiawrVVbupo70MZVxOC1SzHfM4QH77yNBcv7cenuEJ6cF483J5eVQ6+lNDCa4LLDdD+wgpiCZDBBaadQrv/mXPbkJ7Bj6CMorxetFLX9+lI1ZjTO3r3Rfr6Yt2+l/MtXiE8qweYCbVa47xrB4isnM2TUc8Tl5wC6bkZPq0DSnruIVwZdSO3bpzO7ewDdJiiaU9ZLuAshGkzxYS8PPlTF7hgDT5WV2N02+msP3189h93FBwG4NtPJAx8lozLLUIbmUGwEmVdOJz/bgkkbdEwoZ+EdF/D2np4kzI9i9XddUampaKVIa9WXg+2H4LL6EVqaTdfkVYRWHqbmoXE8eOutTBz2HCOSvkX94hknd0Q41ePGkRETQOKWA4QOdeIIchKSVEH3Ej9qy6sI2nDomHOwmND9Ysm8ZDAFuyvhsA+uTr3ZNzuM9abO+K9rx6U9oxjY2w+lVOPe5D9Iwl0I0aAML6R+D6VpEN0b4gaBUvBq4kf8Y/N/MNBMCOvMq9d9hKmiFuU2cIf6svuRmWQcsFFTrtk0eTSfRfSl3YfhrGo/D8u9N6MMDYbGbbJwsP0wUtsOIKtDGEmtLVxg7CLr5tEsX+zDB1dfisnrYUOnaDrklRFZ6QQFJcM7saZfa+whJdjCzJgdGltINcVpfVj94810OZDIZVuvwX60HdpiAgW4DbCYQGt022C8LgN3fDipl4/h0zMmsmV5b9q8GM55N9kZc40Ji6Mp7/7PJNyFEI2mKDeDh+4fzxU/5NCmKAaf80LxT0yHYAeusZ1IG9iLfYeCeLrfLAwU1iwTayb4YZowArV2CxjGTz3sCt8g3plxHTu7dqXaYeVIlD/3PPkEo3/cjFaQMG8GIbsLmXikkBmrD9ChoBJ9Rju+nn8jHycFcXHqKwTFFpDuuZx0NZy0JauoSs1FKxPBnmraFGTgX+Pm0Mu3kNyjG16zGbPbjcXrJbioiJHLljPDu52ld03l9eohBF5+Gu22W7h9u5mwjk16mwEJdyFEY3rjDfQN16FcXrTZhPfKC0nuGsbuad0pCQhgtzWWzHQHAzPzWBx8BtpkwgI83dGfAV+/hOO6e6HWg/JqUGCYTXw7+2I+7HE23QvK6eaqIdNh4+xSF58mpjA/bytl0T7Y2/kyxlnBzMRUBueVURsZxIYpE1medCtD5yThdQWStT+L4owCyitrKSnOwU+l4VvjR2dHCBXhg7EpA6u5nL3VXvztisM923H64fWUWspY1WMsOe16kbAxkF5JVv7+70iCbE3bhZdwF0I0nueeQ9/+d5Sh0b4OjDFD0bGRUFjClzWjmB95HZTZiDxkZdAZblZfW0tyjRcFvNDJn9PdZVTNmYTfxv0QaMczugOHz+/H130HUjaviF4VXuw2B/62TqQmKfYc3smuks3sI4dKq8bbyo+uofDi1n2cdiAfNbY9H71yC+/sHUn7JF/al5UxOv4ZsjsWER2ZSmjZAP71cgSF1SOwGD4oRxZ5+3KxUUugvRgnCrfZwVn+wYRVpxBy+tm4rDZAUWwCV6dQbrhsaJPcagl3IUTj2bcP3bM7ylM3tqIBb+9uMKQvBFjYeU5H7o2YxL7d0YzLCGL+7VY2lTp5P6+WRxICCLPWTVcpdR5hRcl60i1e9lsj2ZobTft3c/jouQl8Pedu7L4BOKzh+KYqykp6csBzkE2137PfvZvSAAfv92/FhtgEjnQtpYerhv2DhvPJzmH0eDSaqGw3c8s7o0ZUUN0niCJtIzn3NPLSzoGQQrIKyzHjolYVU+ytpbQqlFG+nQm2l+Dbpd9PTVVKkeII4tb7zmiKOy3hLoRoXN6LxmFashLlPjp4bjHh7dkK9yVnUe4IY0NIWx6vHU/U/li++qftd6+3/0eDDYsg2Ks55z/R2J3FeDFTmNCByIPJ7On/EiURvSmKcJPXbg/p5iwUEFWbgC1tAPkTDhMV+Q2RFdkcOBhNdVkgsellnPtDGiZvAdpThsmoxWx4MJQJA4N17fqyP7Y7PrEWgtpqfHrnUVRdTlG+L8U7WuPMCSbE0YZpV45n4HlBJ/eGHoeEuxCiUWnDS+XjU/FbsA7l9qJPi6R6UnfWnT2EIxkh2D6DgUuLSYuLZ9TX52I2m//4xTMy8J47FFNmIdrphnB/VFE1lfZ4smOmsLf3JLaY3ST7l2NYTJRV++G2mfHxszK+69f0i9iCYZhIrw0n+s0LCMnPxuUswBPqxhvsxe6qIDrvC+yGFd+YQNJmn8WqUUP4tKwbPm+2In6tBarA4lXMXWAjrNPJu4+/R8JdCNHotNYklayiomgzht3M/uAYEq2t2JgVh22VhXde/go/rwvsJsJ6FeNbkQSjR8LV14Pd/nuXx3lgI6YzzsJSUIGrbSip04fS5dlv8HitPDV9Huu6x1Boc0K1Cz+XQWF0FZ7Tkjg9vZrxCwp5bsJkvG47IZV7sTnL8C0vJTOoHaG5JYTsT8bnvJ6Y8vLo4tebiCPh+BQEU1Nm4K1IJsDzA1XudtQYHblg3nCihvQ46ffzt0i4CyGajMfQ/PulUr631ZBl2LFnOWiTaOPmKxUd3n8Nd2o1APaKPYTmv4c51A/3uhXYOvyBTbXz8jAmjkIlHqibox7qgypzUuQXyOZ2vdhy/nlsSejOO6MMjjiLWLVlDdFLk9iZXkxRVGeqAqMp6DWcMr9yPOZa8nxCMefXYNlVTJmfnaD2FZgsFYTaNzCyYgi7LH3xUf+g9MO+OPx8YZBB9iQ/xn2bx+AOozjvwmtP8t08loS7EKLJ5SbCoWVgD4DTpoJfqAciI6h2xVAUdxmYbbhCK4nKmI9PgpU5H7xOfnIfbo4JZHgn8/98StSTs4+8L57gSM84UqJjiF61n1a6ksoecWzp1JUv19q4zqoY2D2Ysspyps15ld6to6kurmJq7FOYTVbeN33Bfv+VdPQz2JPXigJXCF0vqQCrwtfhxEYg3ppgrEnvseKxtGPK73Dmadw15lqmXXH9yb6Nx5BwF0L89Wzfjh46BFXrxIuZ4rgZbPlkFPbKHLrMX0LJ9UO5p9slbN/WmvNTg3nuWisW8/9eBkA7KzmyeR4rs4I5mN2L4jYW1nVwYPs4ho5rHYSfn4q3zW5mL/+ehAE9yf12KR/nFfJZ9x6UhwYRepqTjv2i2f5pLNV51fj6mbEFuekc1xZTYD64YfeSNyhJycRWVk212SAoOoobZ9/CNRddi9nyJz47aAAS7kKIv559+9C9e6Jcnp9eKurRid2PTKakf3tCXJV8E9udBXu6EbIqhIeH+DJl6J8Lz6pKzepPNZa0AoYeuQNn3iGSo0fAyGASz47l4D4Tdz//Ilnje7K8+xmsq+5J6dpohi7M429J99HF+BosJmq7tObw1ZewNnQwubnBWHKtkOch2ieGi19vi9lsaei784dIuAsh/nq0RreNg6xcftkf9/pY2f/S5aRcNJQc32CWZMeTvKoto4v9ef3vVkhNheJi6N4dHH/+CVGtNYVeN2uSDHJXW+hkNzHc91PM/74LU60TVeVEh/qiSmpQhsY7sDV5M05n+eChfFrWg4L32zPgXT9GjTYx7VUTTbmm2ImE+wm/DSmlWgPvAlHUPb8wT2v9woleTwjRQimF+mY5enB/tNOD0oDWmDqEEF9TwEHtwouJSdEZfJHcDm11Qf/T0UlJ1EWLxrhhOua5b8Of6DkrpYiw2LigD9Dn/1+dBldMA48H71cfUJX8A4U+Hg5EtifJOYj06mj2Lo6g1YoIru9iZ/x2hT2goW9I4zjhnrtSKgaI0VpvV0oFANuAKVrrvcc7R3ruQpy6vCXZ1Dx5JY6DuXjbh1I7pD3bBvdgdWhnNle0wrU1iA7/jkNpLxMOPsqklKd/OldbTFTPHsIzD/ybjtUdmRJtx9fejBZkr6cmHZZRSi0BXtZaf3e8YyTchTi1aa3ZV7WDxMq9ZAU6SCOMpIJI0g+GEPtDEL0ywFIMGhNRlQeYtWE0c3UX7jESie7kR+KHs3i8zWQ2/9iW212h3DTZgsn011yDvSE16rDMfxXcjrpffDb/xt/NAmYBtGnTpiGKE0I0U0opuvn3oZt/H/Z8DAue8JAQaNDVY2bkGMVlV6wif9ps5vZezpGALlzbZylH9r7JlbXBTM3P4dwDWfTqlsuetuG8utZO6Lf+XD7+aIxt3QofvA9VZTD1Ihh7TtM2tonVu+eulPIHVgOPa60X/a9jpecuhPglrcFdBVZf6ra9Ky1FR0WiXG7e6f4K61tfQVlVEXmJT5HmqaRLDzunPTOALfFDOPR/7d15fFTV+cfxzzOTmSSTnYSEkEASZA0BZJVNkUUFRUEUcUFtRas/l4rWFgpWS7WAdatFXBAFFRRRVhWVRbYWZZGdhEDCEgLZyDbZZpJMzu+PxFesrcqaoeF5/5OZec2997nnxXy5c+6Zc1IiSFoRxuqX7TBpErz8Esblrt2xjwVP/0soXvgJ4U07e/Ucz4UzuXI/q04rEbEBi4D5vxTsSin1YyJgD6R+PdPQUJjwB4yPhbv3Psgz65Lo6UpmYoyDV67KxRXsYNHEdHJf3UFQeRVl1WD27oWXXwKXG4OdbP/bqPKEYt2YRtBT9zF/6RSm/30xGVkVXj3XhnY2N1QFeBcoMMaMP5Vt9MpdKXUqPG9MRZ57HimtxCSEUTm4LfnDO/NF647MmFHJkKxYrs2pIcM3gru7/wt5ajLiqaHc2oqTftcAFoIrtxJstiHW2mX1AI7efgdxb81GfGzePcHT1KA3VEWkP7AR2AN8vxztJGPMip/aRsNdKXWqqt1FpB+aT0V1DicjwtgTHMMeiWbz4WhCvrUyZfEu/IwHxNDywAfEpS0BoFLCSIu8kVJ7KwIqD9Emdwl2UwiAsQgVd3Rj4bTXublZdwKt/xsjbvRHTEqpRqfGGP55pISZSypIM3YsRXaikm2MGyi0zd9KweYsMAabq4BOW54moDybbZc62JzUn0uSkwguqaFlwRqa5+9Bvl8oG9jy8I3MbP8Ob4wNwuG4sEfcaLgrpRotYyBrO1SWQkwvsPnXvl6RU8qehz7HXWZFaqpI3P0iTXK+oyTAwgcjWxGSfTWf3vUdCQkdGPTxEQb+Yx1C7dj54slX8WDI68y9LwF7oFdP72c1+A1VpZRqKCLQvDvED6gPdgD/qEB6fTKGtvdE47Dn4dMziMp7ehDohvvnpdE54208vk6OOaP4fNAYXl76LEXRTRBjCFmTwuOD5nDdgX18nVLlvZM7DzTclVKNQtTofnRb9jDlvduz+IEx7B/ZB2OzkHjAzftjU+j47XrAhUva8fpbM5g943Fc1RB/OJOBiQe5KzuPFwaswxw+5O1TOSc03JVSjYfFQrNJrzMqsA/NOwaxe+jt1MSF4uMRxs8u53dfdiLpRBPsFsHTN4ZbXpzEtISrCHMWcv2BLzgYX8KWpOdxTR9HQ3ZZnw/a566UapSMMWxKX8PaoCLu+u0LtFiylcqwNjgHTSC/dU++SkxhZUAKVX6+WELb8OhLMxi6eCOIYDo1paRrC4peW0GcLdzbp6J97kop9T0RoV/rIUyevp7U3e1ZNHIytvBCmmx9gpjjc7jpUAL/t38QkdlW8lelM22SkzcntKa6phrZnQPx4Vhm3suH0/9CdZXH26dz2vTKXSnV6FWvWgAv/5kTEc1JeXQoUQEVOCWKyI+asW/3EQ7cXo3n0yJy9x0jcng1w774mG7Bcez8262sknbEbdxFv5PNiP3TeLA07CpMoEMhlVLqJ1VVlZH6ykN0eHIeRUEh+LoqsbndPNk/jPToZtxGb1anFSJASEeIaZvJDR3iWHbNjby3uD+jv1jEdf5fEz9zNkEN3FWj3TJKKfUTbGVVJP1pAVa3h/CTBQSWluJbVcUDO/NpdTyDOTUr6H2pm4hgyFrlQ+rq5izYVY117Q4svYpZ43MvUwpnUnHrHbyVupGaC/yGq4a7UurisGFD7cJOgMFKoW0Ma2P7EOQM4fl/Ofnz2iy+yv6WjKhtXD0YqnN8SH7NTsYnKdz29lP4+C8kfH8EU/e9z8ivZ/HU7hUcSq35+WN6kYa7Uuri4HCAp3aR7kpaU3VnKZ3+kkfKiC5sThpA17Bw3t9fzQ0uWFy1hTaXZtGuezVF+2yU9CxnSGAKkR0mcKhtGRmfd2Rg0wM8xna+HD8b3G4vn9x/8s5S3kop1dAGDACbHYwLX08qIUtKKGnSjNDBqZT1tbM6bRiR18TSLaaSSIcfew5V0X1vR4JDvyHNLxVXbgAhzgQG+b7P8uJUri9tQ/eWJ3iw3w282PUfjCh5HsvfpsGt9+DV1bTr6JW7UuriYLPBihW1S24L+BaeoMkLu6jYGIXVp4L4Kz4jbdUadm0J46OIAazrP5JZvVvTKeMy2rgnktMvi11d95JrCcYSMIQlU8sY1+w29oxrRQxbmPn4NFz3PETFk9dSUpnr7bPV0TJKqYuL2bENc/1gJKuk9oWYYNKHtqfi8qNY7VVkHfPjWPow0qJvZ2V5T5qk27i5aA8JfQ6ztIebnUtzaJMaQbAdojtaaGvKQIEAAAtGSURBVJs8ixs2fovEBbNg6ng6PLOJ6Gdj+Hjgr7gvoC9+52BaYR0KqZRSp6DGU03+/PsJzM2hJsCOu1VTjoU4KN65kujIHHJz7MxbO4zLW15H3let+S6gFUG5dm7s9hnJwVYWWE/QPLUJ2a0LCLzESuyW3gz/bhrXFy2mYEA7isst1Ixqx8PXTmRCRRKDk85ucRANd6WUOg3rczaTYdmLj6+h0B7AgeomxK/+FPu2dbwZcQ/NXG76pF7C1YePEm5dSOuNO6gRG5vaP8KGxB6sumkurnIbnrR2NNtzA4muMsZ++yydKzezbfLdtPRkMbbfdJ7K6sjld5x5P7yGu1JKnaY8t5vR+9MpC7TgLLdTUOBP541ZDMibyraotqT7xtL5mIMhKc1o5/qA3ps+wMfjodL48tqQh1h+QyaV7TKJ+iaIK7dYOcYE4g9+w7VNv8T9XDd2Vcbyz+iujHP0oEvcmU0ar+GulFJn6NXFpSz7XLCUWYk6UkO73YVEd5vFydjjfNSzE+U2B2VfB9Mn3cPkkml0ProXgB1Ne/DimB60jV1LzI4qHMV+ZMZ3IS1tHFf23EDJ/bF8Wn4pViB0d1vm3BKMnOZoGg13pZQ6C1WeGn4/KY2WBTt4qOoFrA4rh00MC0LicMX1YmlVOaPWLeVIenfaOtw8sf0l/KrcVFotvLX8UoIL8nBusBNU4MHhsJDXMZEXC8bh29RGeURHujoF363hvP9GIDbHqdel4a6UUudAeV4p2295Cedfg4h0n+CSO9/ms7sewNY8AZa9RVjeMXb692alJ4kHem3mptdX4/a1MPexdjTtX45rRw0JmwylVijo5M/KwJtZWdwX/4pggqJb0WpXKAvnObBaTu0KXsNdKaXOlcxMCgf1YumwESQm76Xnuk0sGz2GgD6X0SItg+Obl+OqtHPU5wqOdIY7kpfQZWseaYkhrP59IiPL93Nioy+luXbyW/hyJKkvC/IGcrwmnOCypvTY24k3F/rhiPjlUjTclVLqXKqqouCeobh3Z3HctyXd96zhZNMwVt57F7G+rfHduImy/Cp8pJrSpk4OdgyjRWwH+kceZUm4i84Z3xF9NJ/SsuasrPg1/4yMZUTkG0w+Mp7wrCbcsaM3E3b+8hTCGu5KKXUelBXsJmfjGzjKIWTeeuwr97OxZ1fyx4wjLvU4BbvT8a0xZHVqzty+v8LXWkKPwXkUBHoImfce/TfZcFjdZEs8f+s/kmt2vkNNWwvOqXOZ+LmV6G4/f3wNd6WUOk88xvBk3jcYvwJunTCTLu+swo0w5akHcQe0J3r9LroXGk4kRLD+sis4HNSCYoRcWzh9Nx3g6kNLCIrN44/XP07rj1fS8kgxw8YupDT/Q8b+/udzW8NdKaXOsy+OuZn4bR5xAYd4K/MNArNOsjkrmoPdB3IgJ5vem9NxuAJZOPYKCm0JuLJ8aPNuCyKDi2nfdD6LTBckZwf+ltrsDW+bzrVTb2dw9FU/eUwNd6WUagDGwJxny1i93tB+1F56xS2l+5RlfHjf0wT7HCTvRDrFEWmcDLycLL+bcZX5Yw8rZWtkNCPezmTotkNsKuvOyfDlGEslgUE1TP94PIGBwf/1eBruSinVwLbM+IB3SuI52g0mH/8IW5EPaa0CyXNs4cTxQKTayZHQe3Gb9vRcK+wvjiJhYShJg2eQc3QIabYysBYxvPunNHv2Njo1v/w/jqHL7CmlVAPr9cjtvPp4BzpyHOfbFuYm348nLpZmObcSFhzMMVdHWpZ8SLuySZwYsIHhhbu5LD6ZbdsfpiIJBndexTByGbh/D+5nXuZcXXBruCul1Fny8QvjhaGj8RkVwTP+D9D/64MkBwbzecJj2I51x5XVmgCXIbhqE+sedeIXUMK1kduJ2BTKotcGMek3A9kwdAA1ffuf9tQEP1nTOdmLUkoprn5iMmUZo3BOfoKmV1/C0OYWDlYMwj+lBS0Sy7D7uPH/5hP2DgCp7kWX9b2JHgM7JlYzZd6TDN/tQ687z00teuWulFLnUEDLDiS89ynDX/yKPqNfYfuNlSx+6BLedkwmo6gT4g8dCpzEl61hxbB3sGZXM+PhHFpLHqkpwsEvz00dekNVKaXOE+cXC/B/7o+86vcPtjqH4u8UYivSaTHqTWLKMrAUujnp70tJ3rV0SO/ImqggYpyJ3P/dv+9HR8sopdQFxlNVytENUzDWag4fuRRnYSglPSs5+eVR2pYmY6so5uGuubTNjOVgjwymLHuc2+fc9G/7OJNw1z53pZQ6j6y2QFoNfp79Fbl83W0zOdXBeGxW8hNbseTglYyetQT75mJWXbceY+CZPnOo2O5kXLdfn9Vx9cpdKaUa0CPf5rM234VlXxAeHwg67ENSxktkJp+kov1+wrIcOLIN+b+tYMFjCwmzBTd8t4yIDAVeAazAbGPM9J97v4a7UkpBcXYNfxpdQ1G5IeikhZBcC816HWSNZxG2tFQC3CUAODu4eeCD8QxNuKrhwl1ErMAB4CogE9gK3GaMSf6pbTTclVKqXk01uJ3gFwpSN3Zxxl/fY/n8/URlp+FjPLia1PDRoaUN+gvVXkCaMeaQMaYSWACMOIv9KaXURcXiA/5N6oMd4JHJd7F069PUXHYZuWGxWEr8z2jfZ3NDNQY49oPnmcBlP36TiPwG+E3dU7eI7D2LYzYmEcBJbxdxgdC2qKdtUU/bol67093gvI+WMcbMAmYBiMi20/1q0VhpW9TTtqinbVFP26KeiJx2f/bZdMscB1r84Hls3WtKKaW87GzCfSvQRkQSRMQO3AosPzdlKaWUOhtn3C1jjKkWkYeBr6gdCvmOMWbfL2w260yP1whpW9TTtqinbVFP26LeabdFg/6ISSmlVMPQWSGVUqoR0nBXSqlGqEHCXUSGikiqiKSJyMSGOOaFSERaiMhaEUkWkX0i8qi3a/I2EbGKyA4R+czbtXiTiISKyCcisl9EUkSkj7dr8hYReazu87FXRD4UET9v19RQROQdEcn94e+BRKSJiKwSkYN1f8NOZV/nPdzrpimYCQwDEoHbRCTxfB/3AlUN/M4Ykwj0Bh66iNvie48CKd4u4gLwCvClMaY90IWLtE1EJAb4LdDDGJNE7WCNW71bVYOaCwz90WsTgTXGmDbAmrrnv6ghrtx1moI6xpgsY8z2uscl1H6AY7xblfeISCxwHTDb27V4k4iEAFcAbwMYYyqNMUXercqrfAB/EfEBHMAJL9fTYIwxG4CCH708Ani37vG7wMhT2VdDhPt/m6bgog2074lIPNAV2OzdSrzq78AfgBpvF+JlCUAeMKeui2q2iAR4uyhvMMYcB14AMoAsoNgYs9K7VXldlDEmq+5xNhB1KhvpDVUvEJFAYBEw3hjj9HY93iAiw4FcY8x3v/jmxs8H6Aa8bozpCpRxil+9G5u6/uQR1P6H1xwIEJGx3q3qwmFqx66f0vj1hgh3nabgB0TERm2wzzfGLPZ2PV7UD7hBRI5Q21U3SETmebckr8kEMo0x33+L+4TasL8YDQEOG2PyjDFVwGKgr5dr8rYcEYkGqPubeyobNUS46zQFdUREqO1XTTHGvOTterzJGPNHY0ysMSae2n8TXxtjLsorNGNMNnBMRL6f+W8w8JPrIjRyGUBvEXHUfV4Gc5HeXP6B5cDddY/vBpadykYNMSvkmUxT0Fj1A+4E9ojIzrrXJhljVnixJnVheASYX3cBdAg4uwU0/0cZYzaLyCfAdmpHl+3gIpqGQEQ+BK4EIkQkE3gamA4sFJFxwFHgllPal04/oJRSjY/eUFVKqUZIw10ppRohDXellGqENNyVUqoR0nBXSqlGSMNdKaUaIQ13pZRqhP4faNzkyEagMJAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualize_batch_overview(input_batch_padded, target_batch_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
